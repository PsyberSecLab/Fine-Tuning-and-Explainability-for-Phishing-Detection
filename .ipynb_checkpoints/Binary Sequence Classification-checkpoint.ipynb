{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c004269b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n",
      "GPU Memory: 23.68 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5deec9068ba44d65b8d991f416bcec07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 1.8862\n",
      "Epoch 1, Step 10: Loss = 1.7530\n",
      "Epoch 1, Step 20: Loss = 1.3627\n",
      "Epoch 1, Step 30: Loss = 1.5234\n",
      "Epoch 1, Step 40: Loss = 0.6137\n",
      "Epoch 1, Step 50: Loss = 1.3821\n",
      "Epoch 1, Step 60: Loss = 0.7046\n",
      "Epoch 1, Step 70: Loss = 2.1004\n",
      "Epoch 1, Step 80: Loss = 2.2951\n",
      "Epoch 1, Step 90: Loss = 1.0264\n",
      "Epoch 1, Step 100: Loss = 0.9422\n",
      "Epoch 1, Step 110: Loss = 0.4088\n",
      "Epoch 1, Step 120: Loss = 0.7887\n",
      "Epoch 1, Step 130: Loss = 2.2298\n",
      "Epoch 1, Step 140: Loss = 1.0574\n",
      "Epoch 1, Step 150: Loss = 1.8968\n",
      "Epoch 1, Step 160: Loss = 1.3741\n",
      "Epoch 1, Step 170: Loss = 1.4226\n",
      "Epoch 1, Step 180: Loss = 1.8554\n",
      "Epoch 1, Step 190: Loss = 0.6717\n",
      "Epoch 1, Step 200: Loss = 1.0808\n",
      "Epoch 1, Step 210: Loss = 1.3347\n",
      "Epoch 1, Step 220: Loss = 0.4885\n",
      "Epoch 1, Step 230: Loss = 1.3909\n",
      "Epoch 1, Step 240: Loss = 0.9509\n",
      "Epoch 1, Step 250: Loss = 0.8483\n",
      "Epoch 1, Step 260: Loss = 0.8913\n",
      "Epoch 1, Step 270: Loss = 0.8954\n",
      "Epoch 1, Step 280: Loss = 0.9386\n",
      "Epoch 1, Step 290: Loss = 0.7468\n",
      "Epoch 1, Step 300: Loss = 0.4875\n",
      "Epoch 1, Step 310: Loss = 1.6441\n",
      "Epoch 1, Step 320: Loss = 0.7231\n",
      "Epoch 1, Step 330: Loss = 0.9066\n",
      "Epoch 1, Step 340: Loss = 0.6675\n",
      "Epoch 1, Step 350: Loss = 0.9580\n",
      "Epoch 1, Step 360: Loss = 0.9569\n",
      "Epoch 1, Step 370: Loss = 0.9463\n",
      "Epoch 1, Step 380: Loss = 0.8563\n",
      "Epoch 1, Step 390: Loss = 1.4920\n",
      "Epoch 1, Step 400: Loss = 1.8105\n",
      "Epoch 1, Step 410: Loss = 0.5710\n",
      "Epoch 1, Step 420: Loss = 1.0147\n",
      "Epoch 1, Step 430: Loss = 0.5017\n",
      "Epoch 1, Step 440: Loss = 0.9170\n",
      "Epoch 1, Step 450: Loss = 0.6494\n",
      "Epoch 1, Step 460: Loss = 1.2348\n",
      "Epoch 1, Step 470: Loss = 0.5841\n",
      "Epoch 1, Step 480: Loss = 0.6716\n",
      "Epoch 1, Step 490: Loss = 1.0240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.4750\n",
      "precision: 0.4501\n",
      "recall: 0.2255\n",
      "f1: 0.3005\n",
      "loss: 1.1589\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.6790\n",
      "precision: 0.6908\n",
      "recall: 0.6480\n",
      "f1: 0.6687\n",
      "loss: 0.6683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 0: Loss = 0.7116\n",
      "Epoch 2, Step 10: Loss = 1.2045\n",
      "Epoch 2, Step 20: Loss = 0.7348\n",
      "Epoch 2, Step 30: Loss = 0.6645\n",
      "Epoch 2, Step 40: Loss = 0.5472\n",
      "Epoch 2, Step 50: Loss = 0.7364\n",
      "Epoch 2, Step 60: Loss = 0.8564\n",
      "Epoch 2, Step 70: Loss = 0.2349\n",
      "Epoch 2, Step 80: Loss = 0.4479\n",
      "Epoch 2, Step 90: Loss = 0.6749\n",
      "Epoch 2, Step 100: Loss = 0.2712\n",
      "Epoch 2, Step 110: Loss = 0.6367\n",
      "Epoch 2, Step 120: Loss = 1.0097\n",
      "Epoch 2, Step 130: Loss = 0.1786\n",
      "Epoch 2, Step 140: Loss = 0.2758\n",
      "Epoch 2, Step 150: Loss = 0.1934\n",
      "Epoch 2, Step 160: Loss = 0.5079\n",
      "Epoch 2, Step 170: Loss = 0.4402\n",
      "Epoch 2, Step 180: Loss = 0.8845\n",
      "Epoch 2, Step 190: Loss = 0.4442\n",
      "Epoch 2, Step 200: Loss = 0.6309\n",
      "Epoch 2, Step 210: Loss = 0.4963\n",
      "Epoch 2, Step 220: Loss = 0.9293\n",
      "Epoch 2, Step 230: Loss = 0.3344\n",
      "Epoch 2, Step 240: Loss = 0.5259\n",
      "Epoch 2, Step 250: Loss = 0.3631\n",
      "Epoch 2, Step 260: Loss = 0.3413\n",
      "Epoch 2, Step 270: Loss = 0.2123\n",
      "Epoch 2, Step 280: Loss = 0.3799\n",
      "Epoch 2, Step 290: Loss = 0.8429\n",
      "Epoch 2, Step 300: Loss = 0.4118\n",
      "Epoch 2, Step 310: Loss = 0.7680\n",
      "Epoch 2, Step 320: Loss = 0.5049\n",
      "Epoch 2, Step 330: Loss = 0.4688\n",
      "Epoch 2, Step 340: Loss = 0.2063\n",
      "Epoch 2, Step 350: Loss = 0.3546\n",
      "Epoch 2, Step 360: Loss = 0.2086\n",
      "Epoch 2, Step 370: Loss = 0.1380\n",
      "Epoch 2, Step 380: Loss = 0.2458\n",
      "Epoch 2, Step 390: Loss = 0.2519\n",
      "Epoch 2, Step 400: Loss = 0.6025\n",
      "Epoch 2, Step 410: Loss = 0.3619\n",
      "Epoch 2, Step 420: Loss = 0.2497\n",
      "Epoch 2, Step 430: Loss = 0.4703\n",
      "Epoch 2, Step 440: Loss = 0.1507\n",
      "Epoch 2, Step 450: Loss = 0.1421\n",
      "Epoch 2, Step 460: Loss = 0.2307\n",
      "Epoch 2, Step 470: Loss = 0.0934\n",
      "Epoch 2, Step 480: Loss = 0.1360\n",
      "Epoch 2, Step 490: Loss = 0.1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8075\n",
      "precision: 0.8251\n",
      "recall: 0.7805\n",
      "f1: 0.8022\n",
      "loss: 0.4246\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8670\n",
      "precision: 0.8536\n",
      "recall: 0.8860\n",
      "f1: 0.8695\n",
      "loss: 0.3116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 0: Loss = 0.4618\n",
      "Epoch 3, Step 10: Loss = 0.9066\n",
      "Epoch 3, Step 20: Loss = 0.3491\n",
      "Epoch 3, Step 30: Loss = 0.3149\n",
      "Epoch 3, Step 40: Loss = 0.5431\n",
      "Epoch 3, Step 50: Loss = 0.1569\n",
      "Epoch 3, Step 60: Loss = 0.5555\n",
      "Epoch 3, Step 70: Loss = 0.4737\n",
      "Epoch 3, Step 80: Loss = 0.2687\n",
      "Epoch 3, Step 90: Loss = 0.1112\n",
      "Epoch 3, Step 100: Loss = 0.1680\n",
      "Epoch 3, Step 110: Loss = 0.0765\n",
      "Epoch 3, Step 120: Loss = 0.8623\n",
      "Epoch 3, Step 130: Loss = 0.3594\n",
      "Epoch 3, Step 140: Loss = 0.5808\n",
      "Epoch 3, Step 150: Loss = 0.4015\n",
      "Epoch 3, Step 160: Loss = 0.0359\n",
      "Epoch 3, Step 170: Loss = 0.0941\n",
      "Epoch 3, Step 180: Loss = 0.7391\n",
      "Epoch 3, Step 190: Loss = 0.3301\n",
      "Epoch 3, Step 200: Loss = 0.2222\n",
      "Epoch 3, Step 210: Loss = 0.2294\n",
      "Epoch 3, Step 220: Loss = 0.2410\n",
      "Epoch 3, Step 230: Loss = 0.1176\n",
      "Epoch 3, Step 240: Loss = 0.0437\n",
      "Epoch 3, Step 250: Loss = 0.2377\n",
      "Epoch 3, Step 260: Loss = 0.4675\n",
      "Epoch 3, Step 270: Loss = 0.2277\n",
      "Epoch 3, Step 280: Loss = 0.4567\n",
      "Epoch 3, Step 290: Loss = 0.1460\n",
      "Epoch 3, Step 300: Loss = 0.1738\n",
      "Epoch 3, Step 310: Loss = 0.1856\n",
      "Epoch 3, Step 320: Loss = 1.0170\n",
      "Epoch 3, Step 330: Loss = 0.3166\n",
      "Epoch 3, Step 340: Loss = 0.0664\n",
      "Epoch 3, Step 350: Loss = 0.3479\n",
      "Epoch 3, Step 360: Loss = 0.0369\n",
      "Epoch 3, Step 370: Loss = 0.1469\n",
      "Epoch 3, Step 380: Loss = 0.6542\n",
      "Epoch 3, Step 390: Loss = 0.1933\n",
      "Epoch 3, Step 400: Loss = 0.2736\n",
      "Epoch 3, Step 410: Loss = 0.2408\n",
      "Epoch 3, Step 420: Loss = 0.1089\n",
      "Epoch 3, Step 430: Loss = 0.5485\n",
      "Epoch 3, Step 440: Loss = 0.0573\n",
      "Epoch 3, Step 450: Loss = 0.1043\n",
      "Epoch 3, Step 460: Loss = 0.0388\n",
      "Epoch 3, Step 470: Loss = 0.2068\n",
      "Epoch 3, Step 480: Loss = 0.0638\n",
      "Epoch 3, Step 490: Loss = 0.2242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8858\n",
      "precision: 0.8958\n",
      "recall: 0.8730\n",
      "f1: 0.8843\n",
      "loss: 0.2671\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8920\n",
      "precision: 0.8784\n",
      "recall: 0.9100\n",
      "f1: 0.8939\n",
      "loss: 0.2459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 0: Loss = 0.4992\n",
      "Epoch 4, Step 10: Loss = 0.1418\n",
      "Epoch 4, Step 20: Loss = 0.4480\n",
      "Epoch 4, Step 30: Loss = 0.3826\n",
      "Epoch 4, Step 40: Loss = 0.0542\n",
      "Epoch 4, Step 50: Loss = 0.3463\n",
      "Epoch 4, Step 60: Loss = 0.9047\n",
      "Epoch 4, Step 70: Loss = 0.2004\n",
      "Epoch 4, Step 80: Loss = 0.3021\n",
      "Epoch 4, Step 90: Loss = 0.4775\n",
      "Epoch 4, Step 100: Loss = 0.1293\n",
      "Epoch 4, Step 110: Loss = 0.3278\n",
      "Epoch 4, Step 120: Loss = 0.5180\n",
      "Epoch 4, Step 130: Loss = 0.0804\n",
      "Epoch 4, Step 140: Loss = 0.1579\n",
      "Epoch 4, Step 150: Loss = 0.1266\n",
      "Epoch 4, Step 160: Loss = 0.1312\n",
      "Epoch 4, Step 170: Loss = 0.5710\n",
      "Epoch 4, Step 180: Loss = 0.5844\n",
      "Epoch 4, Step 190: Loss = 0.3246\n",
      "Epoch 4, Step 200: Loss = 0.4837\n",
      "Epoch 4, Step 210: Loss = 0.2505\n",
      "Epoch 4, Step 220: Loss = 0.3108\n",
      "Epoch 4, Step 230: Loss = 0.2367\n",
      "Epoch 4, Step 240: Loss = 0.0424\n",
      "Epoch 4, Step 250: Loss = 0.1142\n",
      "Epoch 4, Step 260: Loss = 0.5294\n",
      "Epoch 4, Step 270: Loss = 0.1120\n",
      "Epoch 4, Step 280: Loss = 0.0589\n",
      "Epoch 4, Step 290: Loss = 0.2893\n",
      "Epoch 4, Step 300: Loss = 0.2973\n",
      "Epoch 4, Step 310: Loss = 0.0964\n",
      "Epoch 4, Step 320: Loss = 0.0725\n",
      "Epoch 4, Step 330: Loss = 0.0457\n",
      "Epoch 4, Step 340: Loss = 0.3338\n",
      "Epoch 4, Step 350: Loss = 0.1340\n",
      "Epoch 4, Step 360: Loss = 0.3206\n",
      "Epoch 4, Step 370: Loss = 0.1787\n",
      "Epoch 4, Step 380: Loss = 0.0867\n",
      "Epoch 4, Step 390: Loss = 0.0811\n",
      "Epoch 4, Step 400: Loss = 0.1612\n",
      "Epoch 4, Step 410: Loss = 0.0251\n",
      "Epoch 4, Step 420: Loss = 0.1159\n",
      "Epoch 4, Step 430: Loss = 0.0873\n",
      "Epoch 4, Step 440: Loss = 0.1229\n",
      "Epoch 4, Step 450: Loss = 0.0993\n",
      "Epoch 4, Step 460: Loss = 0.7025\n",
      "Epoch 4, Step 470: Loss = 0.0710\n",
      "Epoch 4, Step 480: Loss = 0.2213\n",
      "Epoch 4, Step 490: Loss = 0.1060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9048\n",
      "precision: 0.9120\n",
      "recall: 0.8960\n",
      "f1: 0.9039\n",
      "loss: 0.2250\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9000\n",
      "precision: 0.8876\n",
      "recall: 0.9160\n",
      "f1: 0.9016\n",
      "loss: 0.2185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.2840\n",
      "Epoch 5, Step 10: Loss = 0.1175\n",
      "Epoch 5, Step 20: Loss = 0.1477\n",
      "Epoch 5, Step 30: Loss = 0.0236\n",
      "Epoch 5, Step 40: Loss = 0.2171\n",
      "Epoch 5, Step 50: Loss = 0.0347\n",
      "Epoch 5, Step 60: Loss = 0.2353\n",
      "Epoch 5, Step 70: Loss = 0.1558\n",
      "Epoch 5, Step 80: Loss = 0.1263\n",
      "Epoch 5, Step 90: Loss = 0.1659\n",
      "Epoch 5, Step 100: Loss = 0.0431\n",
      "Epoch 5, Step 110: Loss = 0.2071\n",
      "Epoch 5, Step 120: Loss = 0.2685\n",
      "Epoch 5, Step 130: Loss = 0.1310\n",
      "Epoch 5, Step 140: Loss = 0.1153\n",
      "Epoch 5, Step 150: Loss = 0.1445\n",
      "Epoch 5, Step 160: Loss = 0.0619\n",
      "Epoch 5, Step 170: Loss = 0.3447\n",
      "Epoch 5, Step 180: Loss = 0.4257\n",
      "Epoch 5, Step 190: Loss = 0.1868\n",
      "Epoch 5, Step 200: Loss = 0.4359\n",
      "Epoch 5, Step 210: Loss = 0.8335\n",
      "Epoch 5, Step 220: Loss = 0.4176\n",
      "Epoch 5, Step 230: Loss = 0.1357\n",
      "Epoch 5, Step 240: Loss = 0.0614\n",
      "Epoch 5, Step 250: Loss = 0.1743\n",
      "Epoch 5, Step 260: Loss = 0.0349\n",
      "Epoch 5, Step 270: Loss = 0.4068\n",
      "Epoch 5, Step 280: Loss = 0.3461\n",
      "Epoch 5, Step 290: Loss = 0.0990\n",
      "Epoch 5, Step 300: Loss = 0.3188\n",
      "Epoch 5, Step 310: Loss = 0.3492\n",
      "Epoch 5, Step 320: Loss = 0.3488\n",
      "Epoch 5, Step 330: Loss = 0.1140\n",
      "Epoch 5, Step 340: Loss = 0.2032\n",
      "Epoch 5, Step 350: Loss = 0.1781\n",
      "Epoch 5, Step 360: Loss = 0.4295\n",
      "Epoch 5, Step 370: Loss = 0.2176\n",
      "Epoch 5, Step 380: Loss = 0.3726\n",
      "Epoch 5, Step 390: Loss = 0.1084\n",
      "Epoch 5, Step 400: Loss = 0.2158\n",
      "Epoch 5, Step 410: Loss = 0.0557\n",
      "Epoch 5, Step 420: Loss = 0.3030\n",
      "Epoch 5, Step 430: Loss = 0.0473\n",
      "Epoch 5, Step 440: Loss = 0.0712\n",
      "Epoch 5, Step 450: Loss = 0.1410\n",
      "Epoch 5, Step 460: Loss = 0.2696\n",
      "Epoch 5, Step 470: Loss = 0.1208\n",
      "Epoch 5, Step 480: Loss = 0.0642\n",
      "Epoch 5, Step 490: Loss = 0.0847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9125\n",
      "precision: 0.9213\n",
      "recall: 0.9020\n",
      "f1: 0.9116\n",
      "loss: 0.2041\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9060\n",
      "precision: 0.8980\n",
      "recall: 0.9160\n",
      "f1: 0.9069\n",
      "loss: 0.2061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Step 0: Loss = 0.0177\n",
      "Epoch 6, Step 10: Loss = 0.2638\n",
      "Epoch 6, Step 20: Loss = 0.0607\n",
      "Epoch 6, Step 30: Loss = 0.0715\n",
      "Epoch 6, Step 40: Loss = 0.0275\n",
      "Epoch 6, Step 50: Loss = 0.1655\n",
      "Epoch 6, Step 60: Loss = 0.0284\n",
      "Epoch 6, Step 70: Loss = 0.1170\n",
      "Epoch 6, Step 80: Loss = 0.2197\n",
      "Epoch 6, Step 90: Loss = 0.0507\n",
      "Epoch 6, Step 100: Loss = 0.3275\n",
      "Epoch 6, Step 110: Loss = 0.5148\n",
      "Epoch 6, Step 120: Loss = 0.0695\n",
      "Epoch 6, Step 130: Loss = 0.4821\n",
      "Epoch 6, Step 140: Loss = 0.0750\n",
      "Epoch 6, Step 150: Loss = 0.0805\n",
      "Epoch 6, Step 160: Loss = 0.0391\n",
      "Epoch 6, Step 170: Loss = 0.2336\n",
      "Epoch 6, Step 180: Loss = 0.3582\n",
      "Epoch 6, Step 190: Loss = 0.1672\n",
      "Epoch 6, Step 200: Loss = 0.1362\n",
      "Epoch 6, Step 210: Loss = 0.1321\n",
      "Epoch 6, Step 220: Loss = 0.5296\n",
      "Epoch 6, Step 230: Loss = 0.0876\n",
      "Epoch 6, Step 240: Loss = 0.2686\n",
      "Epoch 6, Step 250: Loss = 0.2292\n",
      "Epoch 6, Step 260: Loss = 0.0997\n",
      "Epoch 6, Step 270: Loss = 0.3179\n",
      "Epoch 6, Step 280: Loss = 0.0407\n",
      "Epoch 6, Step 290: Loss = 0.1414\n",
      "Epoch 6, Step 300: Loss = 0.3250\n",
      "Epoch 6, Step 310: Loss = 0.0533\n",
      "Epoch 6, Step 320: Loss = 0.1065\n",
      "Epoch 6, Step 330: Loss = 0.2154\n",
      "Epoch 6, Step 340: Loss = 0.3318\n",
      "Epoch 6, Step 350: Loss = 0.0264\n",
      "Epoch 6, Step 360: Loss = 0.0250\n",
      "Epoch 6, Step 370: Loss = 0.1152\n",
      "Epoch 6, Step 380: Loss = 0.0691\n",
      "Epoch 6, Step 390: Loss = 0.5330\n",
      "Epoch 6, Step 400: Loss = 0.2839\n",
      "Epoch 6, Step 410: Loss = 0.1261\n",
      "Epoch 6, Step 420: Loss = 0.0748\n",
      "Epoch 6, Step 430: Loss = 0.0188\n",
      "Epoch 6, Step 440: Loss = 0.1433\n",
      "Epoch 6, Step 450: Loss = 0.3961\n",
      "Epoch 6, Step 460: Loss = 0.0238\n",
      "Epoch 6, Step 470: Loss = 0.1084\n",
      "Epoch 6, Step 480: Loss = 0.1156\n",
      "Epoch 6, Step 490: Loss = 0.0363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9185\n",
      "precision: 0.9262\n",
      "recall: 0.9095\n",
      "f1: 0.9178\n",
      "loss: 0.1957\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9070\n",
      "precision: 0.8967\n",
      "recall: 0.9200\n",
      "f1: 0.9082\n",
      "loss: 0.2006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Step 0: Loss = 0.1646\n",
      "Epoch 7, Step 10: Loss = 0.0346\n",
      "Epoch 7, Step 20: Loss = 0.1163\n",
      "Epoch 7, Step 30: Loss = 0.4327\n",
      "Epoch 7, Step 40: Loss = 0.0602\n",
      "Epoch 7, Step 50: Loss = 0.2500\n",
      "Epoch 7, Step 60: Loss = 0.1210\n",
      "Epoch 7, Step 70: Loss = 0.1555\n",
      "Epoch 7, Step 80: Loss = 0.0701\n",
      "Epoch 7, Step 90: Loss = 0.1577\n",
      "Epoch 7, Step 100: Loss = 0.1935\n",
      "Epoch 7, Step 110: Loss = 0.2684\n",
      "Epoch 7, Step 120: Loss = 0.0288\n",
      "Epoch 7, Step 130: Loss = 0.0467\n",
      "Epoch 7, Step 140: Loss = 0.0763\n",
      "Epoch 7, Step 150: Loss = 0.3544\n",
      "Epoch 7, Step 160: Loss = 0.3598\n",
      "Epoch 7, Step 170: Loss = 0.0865\n",
      "Epoch 7, Step 180: Loss = 0.1754\n",
      "Epoch 7, Step 190: Loss = 0.0677\n",
      "Epoch 7, Step 200: Loss = 0.0425\n",
      "Epoch 7, Step 210: Loss = 0.1569\n",
      "Epoch 7, Step 220: Loss = 0.3503\n",
      "Epoch 7, Step 230: Loss = 0.0912\n",
      "Epoch 7, Step 240: Loss = 0.1234\n",
      "Epoch 7, Step 250: Loss = 0.1022\n",
      "Epoch 7, Step 260: Loss = 0.2016\n",
      "Epoch 7, Step 270: Loss = 0.1400\n",
      "Epoch 7, Step 280: Loss = 0.2066\n",
      "Epoch 7, Step 290: Loss = 0.1196\n",
      "Epoch 7, Step 300: Loss = 0.1431\n",
      "Epoch 7, Step 310: Loss = 0.1847\n",
      "Epoch 7, Step 320: Loss = 0.5784\n",
      "Epoch 7, Step 330: Loss = 0.4875\n",
      "Epoch 7, Step 340: Loss = 0.0546\n",
      "Epoch 7, Step 350: Loss = 0.0865\n",
      "Epoch 7, Step 360: Loss = 0.0471\n",
      "Epoch 7, Step 370: Loss = 0.1219\n",
      "Epoch 7, Step 380: Loss = 0.3402\n",
      "Epoch 7, Step 390: Loss = 0.0574\n",
      "Epoch 7, Step 400: Loss = 0.2152\n",
      "Epoch 7, Step 410: Loss = 0.3063\n",
      "Epoch 7, Step 420: Loss = 0.4323\n",
      "Epoch 7, Step 430: Loss = 0.0427\n",
      "Epoch 7, Step 440: Loss = 0.5018\n",
      "Epoch 7, Step 450: Loss = 0.1319\n",
      "Epoch 7, Step 460: Loss = 0.0429\n",
      "Epoch 7, Step 470: Loss = 0.4983\n",
      "Epoch 7, Step 480: Loss = 0.0150\n",
      "Epoch 7, Step 490: Loss = 0.0520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9207\n",
      "precision: 0.9300\n",
      "recall: 0.9100\n",
      "f1: 0.9199\n",
      "loss: 0.1914\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9080\n",
      "precision: 0.8984\n",
      "recall: 0.9200\n",
      "f1: 0.9091\n",
      "loss: 0.1976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Step 0: Loss = 0.0652\n",
      "Epoch 8, Step 10: Loss = 0.2255\n",
      "Epoch 8, Step 20: Loss = 0.5462\n",
      "Epoch 8, Step 30: Loss = 0.0218\n",
      "Epoch 8, Step 40: Loss = 0.1834\n",
      "Epoch 8, Step 50: Loss = 0.1525\n",
      "Epoch 8, Step 60: Loss = 0.1369\n",
      "Epoch 8, Step 70: Loss = 0.1811\n",
      "Epoch 8, Step 80: Loss = 0.0855\n",
      "Epoch 8, Step 90: Loss = 0.2218\n",
      "Epoch 8, Step 100: Loss = 0.0649\n",
      "Epoch 8, Step 110: Loss = 0.0747\n",
      "Epoch 8, Step 120: Loss = 0.3312\n",
      "Epoch 8, Step 130: Loss = 0.1783\n",
      "Epoch 8, Step 140: Loss = 0.2651\n",
      "Epoch 8, Step 150: Loss = 0.0732\n",
      "Epoch 8, Step 160: Loss = 0.2659\n",
      "Epoch 8, Step 170: Loss = 0.0296\n",
      "Epoch 8, Step 180: Loss = 0.1694\n",
      "Epoch 8, Step 190: Loss = 0.0925\n",
      "Epoch 8, Step 200: Loss = 0.0815\n",
      "Epoch 8, Step 210: Loss = 0.1976\n",
      "Epoch 8, Step 220: Loss = 0.1838\n",
      "Epoch 8, Step 230: Loss = 0.2269\n",
      "Epoch 8, Step 240: Loss = 0.5805\n",
      "Epoch 8, Step 250: Loss = 0.1232\n",
      "Epoch 8, Step 260: Loss = 0.0215\n",
      "Epoch 8, Step 270: Loss = 0.2170\n",
      "Epoch 8, Step 280: Loss = 0.1435\n",
      "Epoch 8, Step 290: Loss = 0.4980\n",
      "Epoch 8, Step 300: Loss = 0.0605\n",
      "Epoch 8, Step 310: Loss = 0.0958\n",
      "Epoch 8, Step 320: Loss = 0.5257\n",
      "Epoch 8, Step 330: Loss = 0.1039\n",
      "Epoch 8, Step 340: Loss = 0.2013\n",
      "Epoch 8, Step 350: Loss = 0.2552\n",
      "Epoch 8, Step 360: Loss = 0.1568\n",
      "Epoch 8, Step 370: Loss = 0.1660\n",
      "Epoch 8, Step 380: Loss = 0.1843\n",
      "Epoch 8, Step 390: Loss = 0.1044\n",
      "Epoch 8, Step 400: Loss = 0.1689\n",
      "Epoch 8, Step 410: Loss = 0.0673\n",
      "Epoch 8, Step 420: Loss = 0.1725\n",
      "Epoch 8, Step 430: Loss = 0.0662\n",
      "Epoch 8, Step 440: Loss = 0.2113\n",
      "Epoch 8, Step 450: Loss = 0.1613\n",
      "Epoch 8, Step 460: Loss = 0.1807\n",
      "Epoch 8, Step 470: Loss = 0.1131\n",
      "Epoch 8, Step 480: Loss = 0.3090\n",
      "Epoch 8, Step 490: Loss = 0.1516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9205\n",
      "precision: 0.9278\n",
      "recall: 0.9120\n",
      "f1: 0.9198\n",
      "loss: 0.1902\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9090\n",
      "precision: 0.8986\n",
      "recall: 0.9220\n",
      "f1: 0.9102\n",
      "loss: 0.1962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 0: Loss = 0.5244\n",
      "Epoch 9, Step 10: Loss = 0.3127\n",
      "Epoch 9, Step 20: Loss = 0.2639\n",
      "Epoch 9, Step 30: Loss = 0.2064\n",
      "Epoch 9, Step 40: Loss = 0.0112\n",
      "Epoch 9, Step 50: Loss = 0.1878\n",
      "Epoch 9, Step 60: Loss = 0.0116\n",
      "Epoch 9, Step 70: Loss = 0.2564\n",
      "Epoch 9, Step 80: Loss = 0.2378\n",
      "Epoch 9, Step 90: Loss = 0.3180\n",
      "Epoch 9, Step 100: Loss = 0.2747\n",
      "Epoch 9, Step 110: Loss = 0.2552\n",
      "Epoch 9, Step 120: Loss = 0.3418\n",
      "Epoch 9, Step 130: Loss = 0.0859\n",
      "Epoch 9, Step 140: Loss = 0.1040\n",
      "Epoch 9, Step 150: Loss = 0.2279\n",
      "Epoch 9, Step 160: Loss = 0.3075\n",
      "Epoch 9, Step 170: Loss = 0.2711\n",
      "Epoch 9, Step 180: Loss = 0.0137\n",
      "Epoch 9, Step 190: Loss = 0.0370\n",
      "Epoch 9, Step 200: Loss = 0.0512\n",
      "Epoch 9, Step 210: Loss = 0.2675\n",
      "Epoch 9, Step 220: Loss = 0.0800\n",
      "Epoch 9, Step 230: Loss = 0.1220\n",
      "Epoch 9, Step 240: Loss = 0.2889\n",
      "Epoch 9, Step 250: Loss = 0.0189\n",
      "Epoch 9, Step 260: Loss = 0.0091\n",
      "Epoch 9, Step 270: Loss = 0.0385\n",
      "Epoch 9, Step 280: Loss = 0.2621\n",
      "Epoch 9, Step 290: Loss = 0.1430\n",
      "Epoch 9, Step 300: Loss = 0.5027\n",
      "Epoch 9, Step 310: Loss = 0.2489\n",
      "Epoch 9, Step 320: Loss = 0.2000\n",
      "Epoch 9, Step 330: Loss = 0.1791\n",
      "Epoch 9, Step 340: Loss = 0.0951\n",
      "Epoch 9, Step 350: Loss = 0.1422\n",
      "Epoch 9, Step 360: Loss = 0.0469\n",
      "Epoch 9, Step 370: Loss = 0.1300\n",
      "Epoch 9, Step 380: Loss = 0.3110\n",
      "Epoch 9, Step 390: Loss = 0.1101\n",
      "Epoch 9, Step 400: Loss = 0.1153\n",
      "Epoch 9, Step 410: Loss = 0.6010\n",
      "Epoch 9, Step 420: Loss = 0.0760\n",
      "Epoch 9, Step 430: Loss = 0.7274\n",
      "Epoch 9, Step 440: Loss = 0.0643\n",
      "Epoch 9, Step 450: Loss = 0.2573\n",
      "Epoch 9, Step 460: Loss = 0.1729\n",
      "Epoch 9, Step 470: Loss = 0.0890\n",
      "Epoch 9, Step 480: Loss = 0.3296\n",
      "Epoch 9, Step 490: Loss = 0.3063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9215\n",
      "precision: 0.9297\n",
      "recall: 0.9120\n",
      "f1: 0.9207\n",
      "loss: 0.1886\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9090\n",
      "precision: 0.8986\n",
      "recall: 0.9220\n",
      "f1: 0.9102\n",
      "loss: 0.1953\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        self.emails_df = emails_df.copy()\n",
    "        self.emails_df['sender'] = self.emails_df['sender'].apply(clean_text)\n",
    "        self.emails_df['subject'] = self.emails_df['subject'].apply(clean_text)\n",
    "        self.emails_df['body'] = self.emails_df['body'].apply(clean_text)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        email = self.emails_df.iloc[idx]\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(email['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model_config.use_cache = False\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, output_dir):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "    \n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    return metrics\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9):\n",
    "    best_val_metrics = {'f1': 0}\n",
    "    best_model_state = None\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % 2 == 0:  # Gradient accumulation steps = 2\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(batch['label'].cpu().numpy())\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "  \n",
    "        train_metrics = compute_metrics(train_preds, train_labels)\n",
    "        train_metrics['loss'] = total_loss / len(train_loader)\n",
    "      \n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "      \n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(\"Training Metrics:\")\n",
    "        for metric, value in train_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        for metric, value in val_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_val_metrics['f1']:\n",
    "            best_val_metrics = val_metrics\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return best_model_state, best_val_metrics\n",
    "\n",
    "def main():\n",
    "    login(token=\n",
    "    device = setup_environment()\n",
    "    model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/final_data.csv\")\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "    \n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    \n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "    \n",
    "    train_dataset = EmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = EmailDataset(val_df, tokenizer, max_length=512)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    best_model_state, best_metrics = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/lama7b_binary_classification_model\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    plot_confusion_matrix(best_metrics['confusion_matrix'], output_dir)\n",
    "    \n",
    "  \n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"best_metrics\": {k: float(v) if k != 'confusion_matrix' else v.tolist() \n",
    "                        for k, v in best_metrics.items()}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d41ae454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n",
      "GPU Memory: 23.68 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ebba8c09d74073902e5920b5d2584d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 1.7160\n",
      "Epoch 1, Step 10: Loss = 2.9442\n",
      "Epoch 1, Step 20: Loss = 1.6117\n",
      "Epoch 1, Step 30: Loss = 1.9146\n",
      "Epoch 1, Step 40: Loss = 1.2296\n",
      "Epoch 1, Step 50: Loss = 1.8982\n",
      "Epoch 1, Step 60: Loss = 1.7840\n",
      "Epoch 1, Step 70: Loss = 1.1333\n",
      "Epoch 1, Step 80: Loss = 1.9108\n",
      "Epoch 1, Step 90: Loss = 1.0153\n",
      "Epoch 1, Step 100: Loss = 1.2603\n",
      "Epoch 1, Step 110: Loss = 2.0296\n",
      "Epoch 1, Step 120: Loss = 0.8160\n",
      "Epoch 1, Step 130: Loss = 1.6099\n",
      "Epoch 1, Step 140: Loss = 1.7204\n",
      "Epoch 1, Step 150: Loss = 1.5328\n",
      "Epoch 1, Step 160: Loss = 0.8729\n",
      "Epoch 1, Step 170: Loss = 1.7121\n",
      "Epoch 1, Step 180: Loss = 1.2583\n",
      "Epoch 1, Step 190: Loss = 0.6022\n",
      "Epoch 1, Step 200: Loss = 0.3489\n",
      "Epoch 1, Step 210: Loss = 3.0040\n",
      "Epoch 1, Step 220: Loss = 1.1964\n",
      "Epoch 1, Step 230: Loss = 1.3043\n",
      "Epoch 1, Step 240: Loss = 1.1198\n",
      "Epoch 1, Step 250: Loss = 0.8775\n",
      "Epoch 1, Step 260: Loss = 0.9502\n",
      "Epoch 1, Step 270: Loss = 0.6216\n",
      "Epoch 1, Step 280: Loss = 1.0937\n",
      "Epoch 1, Step 290: Loss = 0.7937\n",
      "Epoch 1, Step 300: Loss = 0.7512\n",
      "Epoch 1, Step 310: Loss = 1.4299\n",
      "Epoch 1, Step 320: Loss = 1.1965\n",
      "Epoch 1, Step 330: Loss = 1.6797\n",
      "Epoch 1, Step 340: Loss = 2.3358\n",
      "Epoch 1, Step 350: Loss = 0.2642\n",
      "Epoch 1, Step 360: Loss = 1.0886\n",
      "Epoch 1, Step 370: Loss = 0.6437\n",
      "Epoch 1, Step 380: Loss = 0.8265\n",
      "Epoch 1, Step 390: Loss = 1.0179\n",
      "Epoch 1, Step 400: Loss = 0.9239\n",
      "Epoch 1, Step 410: Loss = 0.6427\n",
      "Epoch 1, Step 420: Loss = 0.7271\n",
      "Epoch 1, Step 430: Loss = 1.0021\n",
      "Epoch 1, Step 440: Loss = 0.2917\n",
      "Epoch 1, Step 450: Loss = 0.1674\n",
      "Epoch 1, Step 460: Loss = 0.9028\n",
      "Epoch 1, Step 470: Loss = 0.7517\n",
      "Epoch 1, Step 480: Loss = 1.1230\n",
      "Epoch 1, Step 490: Loss = 0.5032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.5235\n",
      "precision: 0.5497\n",
      "recall: 0.2600\n",
      "f1: 0.3530\n",
      "loss: 1.3460\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.6640\n",
      "precision: 0.6934\n",
      "recall: 0.5880\n",
      "f1: 0.6364\n",
      "loss: 0.7628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 0: Loss = 0.7560\n",
      "Epoch 2, Step 10: Loss = 0.7693\n",
      "Epoch 2, Step 20: Loss = 0.4761\n",
      "Epoch 2, Step 30: Loss = 1.1525\n",
      "Epoch 2, Step 40: Loss = 0.6480\n",
      "Epoch 2, Step 50: Loss = 0.5539\n",
      "Epoch 2, Step 60: Loss = 0.3797\n",
      "Epoch 2, Step 70: Loss = 0.3815\n",
      "Epoch 2, Step 80: Loss = 0.4731\n",
      "Epoch 2, Step 90: Loss = 0.1660\n",
      "Epoch 2, Step 100: Loss = 0.4897\n",
      "Epoch 2, Step 110: Loss = 0.4512\n",
      "Epoch 2, Step 120: Loss = 0.2502\n",
      "Epoch 2, Step 130: Loss = 0.3446\n",
      "Epoch 2, Step 140: Loss = 0.5971\n",
      "Epoch 2, Step 150: Loss = 0.4476\n",
      "Epoch 2, Step 160: Loss = 0.3314\n",
      "Epoch 2, Step 170: Loss = 0.5730\n",
      "Epoch 2, Step 180: Loss = 0.2981\n",
      "Epoch 2, Step 190: Loss = 0.2552\n",
      "Epoch 2, Step 200: Loss = 0.4009\n",
      "Epoch 2, Step 210: Loss = 0.4329\n",
      "Epoch 2, Step 220: Loss = 1.4023\n",
      "Epoch 2, Step 230: Loss = 0.1422\n",
      "Epoch 2, Step 240: Loss = 0.1375\n",
      "Epoch 2, Step 250: Loss = 0.2196\n",
      "Epoch 2, Step 260: Loss = 0.6730\n",
      "Epoch 2, Step 270: Loss = 0.1488\n",
      "Epoch 2, Step 280: Loss = 0.1732\n",
      "Epoch 2, Step 290: Loss = 0.3078\n",
      "Epoch 2, Step 300: Loss = 0.2991\n",
      "Epoch 2, Step 310: Loss = 0.1516\n",
      "Epoch 2, Step 320: Loss = 0.0954\n",
      "Epoch 2, Step 330: Loss = 0.1672\n",
      "Epoch 2, Step 340: Loss = 0.6487\n",
      "Epoch 2, Step 350: Loss = 0.6533\n",
      "Epoch 2, Step 360: Loss = 0.1285\n",
      "Epoch 2, Step 370: Loss = 0.4120\n",
      "Epoch 2, Step 380: Loss = 0.8254\n",
      "Epoch 2, Step 390: Loss = 0.4149\n",
      "Epoch 2, Step 400: Loss = 0.0808\n",
      "Epoch 2, Step 410: Loss = 0.3773\n",
      "Epoch 2, Step 420: Loss = 0.1762\n",
      "Epoch 2, Step 430: Loss = 1.1609\n",
      "Epoch 2, Step 440: Loss = 0.4154\n",
      "Epoch 2, Step 450: Loss = 0.0312\n",
      "Epoch 2, Step 460: Loss = 0.7877\n",
      "Epoch 2, Step 470: Loss = 0.5971\n",
      "Epoch 2, Step 480: Loss = 0.1405\n",
      "Epoch 2, Step 490: Loss = 0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8227\n",
      "precision: 0.8236\n",
      "recall: 0.8215\n",
      "f1: 0.8225\n",
      "loss: 0.4243\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8850\n",
      "precision: 0.8723\n",
      "recall: 0.9020\n",
      "f1: 0.8869\n",
      "loss: 0.3260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 0: Loss = 0.0296\n",
      "Epoch 3, Step 10: Loss = 0.0589\n",
      "Epoch 3, Step 20: Loss = 0.0784\n",
      "Epoch 3, Step 30: Loss = 0.2739\n",
      "Epoch 3, Step 40: Loss = 0.0501\n",
      "Epoch 3, Step 50: Loss = 0.2014\n",
      "Epoch 3, Step 60: Loss = 0.4901\n",
      "Epoch 3, Step 70: Loss = 1.2105\n",
      "Epoch 3, Step 80: Loss = 0.1604\n",
      "Epoch 3, Step 90: Loss = 0.0785\n",
      "Epoch 3, Step 100: Loss = 0.1149\n",
      "Epoch 3, Step 110: Loss = 0.2818\n",
      "Epoch 3, Step 120: Loss = 0.0878\n",
      "Epoch 3, Step 130: Loss = 0.1841\n",
      "Epoch 3, Step 140: Loss = 0.1401\n",
      "Epoch 3, Step 150: Loss = 0.0999\n",
      "Epoch 3, Step 160: Loss = 0.0363\n",
      "Epoch 3, Step 170: Loss = 0.8047\n",
      "Epoch 3, Step 180: Loss = 0.1576\n",
      "Epoch 3, Step 190: Loss = 0.2243\n",
      "Epoch 3, Step 200: Loss = 0.3306\n",
      "Epoch 3, Step 210: Loss = 0.2267\n",
      "Epoch 3, Step 220: Loss = 0.1665\n",
      "Epoch 3, Step 230: Loss = 0.0937\n",
      "Epoch 3, Step 240: Loss = 0.0504\n",
      "Epoch 3, Step 250: Loss = 0.1032\n",
      "Epoch 3, Step 260: Loss = 0.0239\n",
      "Epoch 3, Step 270: Loss = 0.2993\n",
      "Epoch 3, Step 280: Loss = 0.4538\n",
      "Epoch 3, Step 290: Loss = 0.0481\n",
      "Epoch 3, Step 300: Loss = 0.0837\n",
      "Epoch 3, Step 310: Loss = 0.2632\n",
      "Epoch 3, Step 320: Loss = 0.0373\n",
      "Epoch 3, Step 330: Loss = 0.0533\n",
      "Epoch 3, Step 340: Loss = 0.0692\n",
      "Epoch 3, Step 350: Loss = 0.3707\n",
      "Epoch 3, Step 360: Loss = 0.4942\n",
      "Epoch 3, Step 370: Loss = 0.0419\n",
      "Epoch 3, Step 380: Loss = 0.8868\n",
      "Epoch 3, Step 390: Loss = 0.1167\n",
      "Epoch 3, Step 400: Loss = 0.0077\n",
      "Epoch 3, Step 410: Loss = 1.0314\n",
      "Epoch 3, Step 420: Loss = 0.2190\n",
      "Epoch 3, Step 430: Loss = 1.3298\n",
      "Epoch 3, Step 440: Loss = 0.0169\n",
      "Epoch 3, Step 450: Loss = 0.3231\n",
      "Epoch 3, Step 460: Loss = 0.2832\n",
      "Epoch 3, Step 470: Loss = 0.0773\n",
      "Epoch 3, Step 480: Loss = 0.0319\n",
      "Epoch 3, Step 490: Loss = 0.1396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9097\n",
      "precision: 0.9047\n",
      "recall: 0.9160\n",
      "f1: 0.9103\n",
      "loss: 0.2375\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9210\n",
      "precision: 0.9087\n",
      "recall: 0.9360\n",
      "f1: 0.9222\n",
      "loss: 0.2456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 0: Loss = 0.0875\n",
      "Epoch 4, Step 10: Loss = 0.0210\n",
      "Epoch 4, Step 20: Loss = 0.2401\n",
      "Epoch 4, Step 30: Loss = 0.0605\n",
      "Epoch 4, Step 40: Loss = 0.2529\n",
      "Epoch 4, Step 50: Loss = 0.1607\n",
      "Epoch 4, Step 60: Loss = 0.2264\n",
      "Epoch 4, Step 70: Loss = 0.2493\n",
      "Epoch 4, Step 80: Loss = 0.1734\n",
      "Epoch 4, Step 90: Loss = 0.0605\n",
      "Epoch 4, Step 100: Loss = 0.1698\n",
      "Epoch 4, Step 110: Loss = 0.1759\n",
      "Epoch 4, Step 120: Loss = 0.3050\n",
      "Epoch 4, Step 130: Loss = 0.2141\n",
      "Epoch 4, Step 140: Loss = 0.3136\n",
      "Epoch 4, Step 150: Loss = 0.0253\n",
      "Epoch 4, Step 160: Loss = 0.3635\n",
      "Epoch 4, Step 170: Loss = 0.4912\n",
      "Epoch 4, Step 180: Loss = 0.2475\n",
      "Epoch 4, Step 190: Loss = 0.0374\n",
      "Epoch 4, Step 200: Loss = 0.2764\n",
      "Epoch 4, Step 210: Loss = 0.0247\n",
      "Epoch 4, Step 220: Loss = 0.3401\n",
      "Epoch 4, Step 230: Loss = 0.0887\n",
      "Epoch 4, Step 240: Loss = 0.0229\n",
      "Epoch 4, Step 250: Loss = 0.0481\n",
      "Epoch 4, Step 260: Loss = 0.1089\n",
      "Epoch 4, Step 270: Loss = 0.0139\n",
      "Epoch 4, Step 280: Loss = 0.0949\n",
      "Epoch 4, Step 290: Loss = 0.1055\n",
      "Epoch 4, Step 300: Loss = 0.0593\n",
      "Epoch 4, Step 310: Loss = 0.5605\n",
      "Epoch 4, Step 320: Loss = 0.0361\n",
      "Epoch 4, Step 330: Loss = 0.0261\n",
      "Epoch 4, Step 340: Loss = 0.0250\n",
      "Epoch 4, Step 350: Loss = 0.7151\n",
      "Epoch 4, Step 360: Loss = 0.3059\n",
      "Epoch 4, Step 370: Loss = 0.3378\n",
      "Epoch 4, Step 380: Loss = 0.0988\n",
      "Epoch 4, Step 390: Loss = 0.2254\n",
      "Epoch 4, Step 400: Loss = 0.0702\n",
      "Epoch 4, Step 410: Loss = 0.0273\n",
      "Epoch 4, Step 420: Loss = 0.0104\n",
      "Epoch 4, Step 430: Loss = 0.0510\n",
      "Epoch 4, Step 440: Loss = 0.2824\n",
      "Epoch 4, Step 450: Loss = 0.3389\n",
      "Epoch 4, Step 460: Loss = 0.0151\n",
      "Epoch 4, Step 470: Loss = 0.1941\n",
      "Epoch 4, Step 480: Loss = 0.0710\n",
      "Epoch 4, Step 490: Loss = 0.0367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9325\n",
      "precision: 0.9240\n",
      "recall: 0.9425\n",
      "f1: 0.9332\n",
      "loss: 0.1894\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9290\n",
      "precision: 0.9133\n",
      "recall: 0.9480\n",
      "f1: 0.9303\n",
      "loss: 0.2145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.4616\n",
      "Epoch 5, Step 10: Loss = 0.0521\n",
      "Epoch 5, Step 20: Loss = 0.0421\n",
      "Epoch 5, Step 30: Loss = 0.8226\n",
      "Epoch 5, Step 40: Loss = 0.6059\n",
      "Epoch 5, Step 50: Loss = 0.0190\n",
      "Epoch 5, Step 60: Loss = 0.1420\n",
      "Epoch 5, Step 70: Loss = 0.0461\n",
      "Epoch 5, Step 80: Loss = 0.0615\n",
      "Epoch 5, Step 90: Loss = 0.0140\n",
      "Epoch 5, Step 100: Loss = 0.0299\n",
      "Epoch 5, Step 110: Loss = 0.0342\n",
      "Epoch 5, Step 120: Loss = 0.0391\n",
      "Epoch 5, Step 130: Loss = 0.5346\n",
      "Epoch 5, Step 140: Loss = 0.3167\n",
      "Epoch 5, Step 150: Loss = 0.0245\n",
      "Epoch 5, Step 160: Loss = 0.0417\n",
      "Epoch 5, Step 170: Loss = 0.0164\n",
      "Epoch 5, Step 180: Loss = 0.0846\n",
      "Epoch 5, Step 190: Loss = 1.1824\n",
      "Epoch 5, Step 200: Loss = 0.0761\n",
      "Epoch 5, Step 210: Loss = 0.0715\n",
      "Epoch 5, Step 220: Loss = 0.0133\n",
      "Epoch 5, Step 230: Loss = 0.0256\n",
      "Epoch 5, Step 240: Loss = 0.1237\n",
      "Epoch 5, Step 250: Loss = 0.1202\n",
      "Epoch 5, Step 260: Loss = 0.4167\n",
      "Epoch 5, Step 270: Loss = 0.0409\n",
      "Epoch 5, Step 280: Loss = 0.2201\n",
      "Epoch 5, Step 290: Loss = 0.0160\n",
      "Epoch 5, Step 300: Loss = 0.0125\n",
      "Epoch 5, Step 310: Loss = 0.2462\n",
      "Epoch 5, Step 320: Loss = 0.2737\n",
      "Epoch 5, Step 330: Loss = 0.0637\n",
      "Epoch 5, Step 340: Loss = 0.0232\n",
      "Epoch 5, Step 350: Loss = 0.0576\n",
      "Epoch 5, Step 360: Loss = 0.0243\n",
      "Epoch 5, Step 370: Loss = 0.0441\n",
      "Epoch 5, Step 380: Loss = 0.0603\n",
      "Epoch 5, Step 390: Loss = 0.0391\n",
      "Epoch 5, Step 400: Loss = 0.0065\n",
      "Epoch 5, Step 410: Loss = 0.8922\n",
      "Epoch 5, Step 420: Loss = 0.0227\n",
      "Epoch 5, Step 430: Loss = 0.2298\n",
      "Epoch 5, Step 440: Loss = 0.2297\n",
      "Epoch 5, Step 450: Loss = 0.0271\n",
      "Epoch 5, Step 460: Loss = 0.0344\n",
      "Epoch 5, Step 470: Loss = 0.0891\n",
      "Epoch 5, Step 480: Loss = 0.1155\n",
      "Epoch 5, Step 490: Loss = 0.1750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9385\n",
      "precision: 0.9312\n",
      "recall: 0.9470\n",
      "f1: 0.9390\n",
      "loss: 0.1720\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9300\n",
      "precision: 0.9151\n",
      "recall: 0.9480\n",
      "f1: 0.9312\n",
      "loss: 0.2035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Step 0: Loss = 0.0603\n",
      "Epoch 6, Step 10: Loss = 0.0830\n",
      "Epoch 6, Step 20: Loss = 0.0285\n",
      "Epoch 6, Step 30: Loss = 0.0256\n",
      "Epoch 6, Step 40: Loss = 0.0521\n",
      "Epoch 6, Step 50: Loss = 0.2630\n",
      "Epoch 6, Step 60: Loss = 0.1048\n",
      "Epoch 6, Step 70: Loss = 0.1136\n",
      "Epoch 6, Step 80: Loss = 0.0752\n",
      "Epoch 6, Step 90: Loss = 0.0093\n",
      "Epoch 6, Step 100: Loss = 0.3118\n",
      "Epoch 6, Step 110: Loss = 0.1549\n",
      "Epoch 6, Step 120: Loss = 0.3602\n",
      "Epoch 6, Step 130: Loss = 0.0214\n",
      "Epoch 6, Step 140: Loss = 0.0355\n",
      "Epoch 6, Step 150: Loss = 0.0323\n",
      "Epoch 6, Step 160: Loss = 0.1068\n",
      "Epoch 6, Step 170: Loss = 0.0415\n",
      "Epoch 6, Step 180: Loss = 0.0162\n",
      "Epoch 6, Step 190: Loss = 0.0177\n",
      "Epoch 6, Step 200: Loss = 0.2701\n",
      "Epoch 6, Step 210: Loss = 0.3086\n",
      "Epoch 6, Step 220: Loss = 0.3561\n",
      "Epoch 6, Step 230: Loss = 0.5186\n",
      "Epoch 6, Step 240: Loss = 0.0574\n",
      "Epoch 6, Step 250: Loss = 0.0512\n",
      "Epoch 6, Step 260: Loss = 0.0367\n",
      "Epoch 6, Step 270: Loss = 0.1612\n",
      "Epoch 6, Step 280: Loss = 0.6101\n",
      "Epoch 6, Step 290: Loss = 0.6731\n",
      "Epoch 6, Step 300: Loss = 0.0088\n",
      "Epoch 6, Step 310: Loss = 0.0204\n",
      "Epoch 6, Step 320: Loss = 0.4795\n",
      "Epoch 6, Step 330: Loss = 0.0103\n",
      "Epoch 6, Step 340: Loss = 0.0724\n",
      "Epoch 6, Step 350: Loss = 0.2301\n",
      "Epoch 6, Step 360: Loss = 0.1542\n",
      "Epoch 6, Step 370: Loss = 0.0307\n",
      "Epoch 6, Step 380: Loss = 0.0154\n",
      "Epoch 6, Step 390: Loss = 0.5177\n",
      "Epoch 6, Step 400: Loss = 0.5146\n",
      "Epoch 6, Step 410: Loss = 0.0254\n",
      "Epoch 6, Step 420: Loss = 0.0171\n",
      "Epoch 6, Step 430: Loss = 0.3401\n",
      "Epoch 6, Step 440: Loss = 0.0242\n",
      "Epoch 6, Step 450: Loss = 0.0417\n",
      "Epoch 6, Step 460: Loss = 0.2063\n",
      "Epoch 6, Step 470: Loss = 0.1916\n",
      "Epoch 6, Step 480: Loss = 0.1134\n",
      "Epoch 6, Step 490: Loss = 0.4038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9417\n",
      "precision: 0.9372\n",
      "recall: 0.9470\n",
      "f1: 0.9421\n",
      "loss: 0.1644\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9330\n",
      "precision: 0.9188\n",
      "recall: 0.9500\n",
      "f1: 0.9341\n",
      "loss: 0.1986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Step 0: Loss = 0.3467\n",
      "Epoch 7, Step 10: Loss = 0.6382\n",
      "Epoch 7, Step 20: Loss = 0.0456\n",
      "Epoch 7, Step 30: Loss = 0.0727\n",
      "Epoch 7, Step 40: Loss = 0.2743\n",
      "Epoch 7, Step 50: Loss = 0.2281\n",
      "Epoch 7, Step 60: Loss = 0.0490\n",
      "Epoch 7, Step 70: Loss = 0.0299\n",
      "Epoch 7, Step 80: Loss = 0.0896\n",
      "Epoch 7, Step 90: Loss = 0.0379\n",
      "Epoch 7, Step 100: Loss = 0.1629\n",
      "Epoch 7, Step 110: Loss = 0.1489\n",
      "Epoch 7, Step 120: Loss = 0.0159\n",
      "Epoch 7, Step 130: Loss = 0.3015\n",
      "Epoch 7, Step 140: Loss = 0.0601\n",
      "Epoch 7, Step 150: Loss = 0.0585\n",
      "Epoch 7, Step 160: Loss = 0.4882\n",
      "Epoch 7, Step 170: Loss = 0.0245\n",
      "Epoch 7, Step 180: Loss = 0.4731\n",
      "Epoch 7, Step 190: Loss = 0.0462\n",
      "Epoch 7, Step 200: Loss = 0.5094\n",
      "Epoch 7, Step 210: Loss = 0.0592\n",
      "Epoch 7, Step 220: Loss = 0.0113\n",
      "Epoch 7, Step 230: Loss = 0.8364\n",
      "Epoch 7, Step 240: Loss = 0.0385\n",
      "Epoch 7, Step 250: Loss = 0.5789\n",
      "Epoch 7, Step 260: Loss = 0.0605\n",
      "Epoch 7, Step 270: Loss = 0.0926\n",
      "Epoch 7, Step 280: Loss = 0.0730\n",
      "Epoch 7, Step 290: Loss = 0.0913\n",
      "Epoch 7, Step 300: Loss = 0.0764\n",
      "Epoch 7, Step 310: Loss = 0.0842\n",
      "Epoch 7, Step 320: Loss = 0.0686\n",
      "Epoch 7, Step 330: Loss = 0.3307\n",
      "Epoch 7, Step 340: Loss = 0.0870\n",
      "Epoch 7, Step 350: Loss = 0.0127\n",
      "Epoch 7, Step 360: Loss = 0.1504\n",
      "Epoch 7, Step 370: Loss = 0.0418\n",
      "Epoch 7, Step 380: Loss = 0.5295\n",
      "Epoch 7, Step 390: Loss = 0.1111\n",
      "Epoch 7, Step 400: Loss = 0.0161\n",
      "Epoch 7, Step 410: Loss = 0.1557\n",
      "Epoch 7, Step 420: Loss = 0.0685\n",
      "Epoch 7, Step 430: Loss = 0.0435\n",
      "Epoch 7, Step 440: Loss = 0.1998\n",
      "Epoch 7, Step 450: Loss = 0.0454\n",
      "Epoch 7, Step 460: Loss = 0.7803\n",
      "Epoch 7, Step 470: Loss = 0.2357\n",
      "Epoch 7, Step 480: Loss = 0.0736\n",
      "Epoch 7, Step 490: Loss = 0.0665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9437\n",
      "precision: 0.9404\n",
      "recall: 0.9475\n",
      "f1: 0.9440\n",
      "loss: 0.1594\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9340\n",
      "precision: 0.9205\n",
      "recall: 0.9500\n",
      "f1: 0.9350\n",
      "loss: 0.1958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Step 0: Loss = 0.2702\n",
      "Epoch 8, Step 10: Loss = 0.0108\n",
      "Epoch 8, Step 20: Loss = 0.0666\n",
      "Epoch 8, Step 30: Loss = 0.0223\n",
      "Epoch 8, Step 40: Loss = 0.0295\n",
      "Epoch 8, Step 50: Loss = 0.1812\n",
      "Epoch 8, Step 60: Loss = 0.3145\n",
      "Epoch 8, Step 70: Loss = 0.0597\n",
      "Epoch 8, Step 80: Loss = 0.0207\n",
      "Epoch 8, Step 90: Loss = 0.0042\n",
      "Epoch 8, Step 100: Loss = 0.1102\n",
      "Epoch 8, Step 110: Loss = 0.0725\n",
      "Epoch 8, Step 120: Loss = 0.5240\n",
      "Epoch 8, Step 130: Loss = 0.0180\n",
      "Epoch 8, Step 140: Loss = 0.1036\n",
      "Epoch 8, Step 150: Loss = 0.1882\n",
      "Epoch 8, Step 160: Loss = 0.3888\n",
      "Epoch 8, Step 170: Loss = 0.0287\n",
      "Epoch 8, Step 180: Loss = 0.2654\n",
      "Epoch 8, Step 190: Loss = 0.0583\n",
      "Epoch 8, Step 200: Loss = 0.0428\n",
      "Epoch 8, Step 210: Loss = 0.0184\n",
      "Epoch 8, Step 220: Loss = 0.0221\n",
      "Epoch 8, Step 230: Loss = 0.0024\n",
      "Epoch 8, Step 240: Loss = 0.0189\n",
      "Epoch 8, Step 250: Loss = 0.0878\n",
      "Epoch 8, Step 260: Loss = 0.3123\n",
      "Epoch 8, Step 270: Loss = 0.2578\n",
      "Epoch 8, Step 280: Loss = 0.0055\n",
      "Epoch 8, Step 290: Loss = 0.0778\n",
      "Epoch 8, Step 300: Loss = 0.1005\n",
      "Epoch 8, Step 310: Loss = 0.2679\n",
      "Epoch 8, Step 320: Loss = 0.0460\n",
      "Epoch 8, Step 330: Loss = 0.1084\n",
      "Epoch 8, Step 340: Loss = 0.1410\n",
      "Epoch 8, Step 350: Loss = 0.1071\n",
      "Epoch 8, Step 360: Loss = 0.0275\n",
      "Epoch 8, Step 370: Loss = 0.3732\n",
      "Epoch 8, Step 380: Loss = 0.0195\n",
      "Epoch 8, Step 390: Loss = 0.0468\n",
      "Epoch 8, Step 400: Loss = 0.4379\n",
      "Epoch 8, Step 410: Loss = 0.0270\n",
      "Epoch 8, Step 420: Loss = 0.1984\n",
      "Epoch 8, Step 430: Loss = 0.8359\n",
      "Epoch 8, Step 440: Loss = 0.2635\n",
      "Epoch 8, Step 450: Loss = 0.0816\n",
      "Epoch 8, Step 460: Loss = 0.1484\n",
      "Epoch 8, Step 470: Loss = 0.0493\n",
      "Epoch 8, Step 480: Loss = 0.0253\n",
      "Epoch 8, Step 490: Loss = 0.0101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9450\n",
      "precision: 0.9419\n",
      "recall: 0.9485\n",
      "f1: 0.9452\n",
      "loss: 0.1583\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9320\n",
      "precision: 0.9170\n",
      "recall: 0.9500\n",
      "f1: 0.9332\n",
      "loss: 0.1945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 0: Loss = 0.1577\n",
      "Epoch 9, Step 10: Loss = 0.4113\n",
      "Epoch 9, Step 20: Loss = 0.8369\n",
      "Epoch 9, Step 30: Loss = 0.0295\n",
      "Epoch 9, Step 40: Loss = 0.3583\n",
      "Epoch 9, Step 50: Loss = 0.0446\n",
      "Epoch 9, Step 60: Loss = 0.2088\n",
      "Epoch 9, Step 70: Loss = 0.0130\n",
      "Epoch 9, Step 80: Loss = 0.1815\n",
      "Epoch 9, Step 90: Loss = 0.0147\n",
      "Epoch 9, Step 100: Loss = 0.3995\n",
      "Epoch 9, Step 110: Loss = 0.0241\n",
      "Epoch 9, Step 120: Loss = 0.0483\n",
      "Epoch 9, Step 130: Loss = 0.0666\n",
      "Epoch 9, Step 140: Loss = 0.4397\n",
      "Epoch 9, Step 150: Loss = 0.0398\n",
      "Epoch 9, Step 160: Loss = 0.7025\n",
      "Epoch 9, Step 170: Loss = 0.4561\n",
      "Epoch 9, Step 180: Loss = 0.2175\n",
      "Epoch 9, Step 190: Loss = 0.0099\n",
      "Epoch 9, Step 200: Loss = 0.0454\n",
      "Epoch 9, Step 210: Loss = 0.0854\n",
      "Epoch 9, Step 220: Loss = 0.2663\n",
      "Epoch 9, Step 230: Loss = 0.1689\n",
      "Epoch 9, Step 240: Loss = 0.0602\n",
      "Epoch 9, Step 250: Loss = 0.0813\n",
      "Epoch 9, Step 260: Loss = 0.0792\n",
      "Epoch 9, Step 270: Loss = 0.0653\n",
      "Epoch 9, Step 280: Loss = 0.0273\n",
      "Epoch 9, Step 290: Loss = 0.1192\n",
      "Epoch 9, Step 300: Loss = 0.1015\n",
      "Epoch 9, Step 310: Loss = 0.1428\n",
      "Epoch 9, Step 320: Loss = 0.5469\n",
      "Epoch 9, Step 330: Loss = 0.4041\n",
      "Epoch 9, Step 340: Loss = 0.2730\n",
      "Epoch 9, Step 350: Loss = 0.3753\n",
      "Epoch 9, Step 360: Loss = 0.0158\n",
      "Epoch 9, Step 370: Loss = 0.0280\n",
      "Epoch 9, Step 380: Loss = 0.1208\n",
      "Epoch 9, Step 390: Loss = 0.0288\n",
      "Epoch 9, Step 400: Loss = 0.2079\n",
      "Epoch 9, Step 410: Loss = 0.1583\n",
      "Epoch 9, Step 420: Loss = 0.0087\n",
      "Epoch 9, Step 430: Loss = 0.1604\n",
      "Epoch 9, Step 440: Loss = 0.0248\n",
      "Epoch 9, Step 450: Loss = 0.0111\n",
      "Epoch 9, Step 460: Loss = 0.0087\n",
      "Epoch 9, Step 470: Loss = 0.9037\n",
      "Epoch 9, Step 480: Loss = 0.0099\n",
      "Epoch 9, Step 490: Loss = 0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9437\n",
      "precision: 0.9413\n",
      "recall: 0.9465\n",
      "f1: 0.9439\n",
      "loss: 0.1564\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9330\n",
      "precision: 0.9188\n",
      "recall: 0.9500\n",
      "f1: 0.9341\n",
      "loss: 0.1939\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (AutoTokenizer,\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        self.emails_df = emails_df.copy()\n",
    "        self.emails_df['sender'] = self.emails_df['sender'].apply(clean_text)\n",
    "        self.emails_df['subject'] = self.emails_df['subject'].apply(clean_text)\n",
    "        self.emails_df['body'] = self.emails_df['body'].apply(clean_text)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        email = self.emails_df.iloc[idx]\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(email['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model_config.use_cache = False\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, output_dir):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "    \n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    return metrics\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9):\n",
    "    best_val_metrics = {'f1': 0}\n",
    "    best_model_state = None\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % 2 == 0:  # Gradient accumulation steps = 2\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(batch['label'].cpu().numpy())\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "  \n",
    "        train_metrics = compute_metrics(train_preds, train_labels)\n",
    "        train_metrics['loss'] = total_loss / len(train_loader)\n",
    "      \n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "      \n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(\"Training Metrics:\")\n",
    "        for metric, value in train_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        for metric, value in val_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_val_metrics['f1']:\n",
    "            best_val_metrics = val_metrics\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return best_model_state, best_val_metrics\n",
    "\n",
    "def main():\n",
    "    login(token=\n",
    "    device = setup_environment()\n",
    "    model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/final_data.csv\")\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "    \n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    \n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "    \n",
    "    train_dataset = EmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = EmailDataset(val_df, tokenizer, max_length=512)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    best_model_state, best_metrics = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/llama8b_binary_classification_model\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    plot_confusion_matrix(best_metrics['confusion_matrix'], output_dir)\n",
    "    \n",
    "  \n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"best_metrics\": {k: float(v) if k != 'confusion_matrix' else v.tolist() \n",
    "                        for k, v in best_metrics.items()}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df6c520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n",
      "GPU Memory: 23.68 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057e8be757424de0892a1cab69277181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at dreamgen/WizardLM-2-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 3.1648\n",
      "Epoch 1, Step 10: Loss = 1.1832\n",
      "Epoch 1, Step 20: Loss = 0.0461\n",
      "Epoch 1, Step 30: Loss = 0.6317\n",
      "Epoch 1, Step 40: Loss = 0.6844\n",
      "Epoch 1, Step 50: Loss = 4.5570\n",
      "Epoch 1, Step 60: Loss = 0.5165\n",
      "Epoch 1, Step 70: Loss = 3.0345\n",
      "Epoch 1, Step 80: Loss = 0.6065\n",
      "Epoch 1, Step 90: Loss = 1.2399\n",
      "Epoch 1, Step 100: Loss = 1.1913\n",
      "Epoch 1, Step 110: Loss = 0.7395\n",
      "Epoch 1, Step 120: Loss = 0.0087\n",
      "Epoch 1, Step 130: Loss = 2.8056\n",
      "Epoch 1, Step 140: Loss = 0.3464\n",
      "Epoch 1, Step 150: Loss = 0.2242\n",
      "Epoch 1, Step 160: Loss = 0.6680\n",
      "Epoch 1, Step 170: Loss = 1.2313\n",
      "Epoch 1, Step 180: Loss = 1.0059\n",
      "Epoch 1, Step 190: Loss = 0.5259\n",
      "Epoch 1, Step 200: Loss = 1.1587\n",
      "Epoch 1, Step 210: Loss = 2.7515\n",
      "Epoch 1, Step 220: Loss = 4.0594\n",
      "Epoch 1, Step 230: Loss = 2.3415\n",
      "Epoch 1, Step 240: Loss = 1.2338\n",
      "Epoch 1, Step 250: Loss = 2.5145\n",
      "Epoch 1, Step 260: Loss = 0.7798\n",
      "Epoch 1, Step 270: Loss = 0.1291\n",
      "Epoch 1, Step 280: Loss = 0.3329\n",
      "Epoch 1, Step 290: Loss = 5.1360\n",
      "Epoch 1, Step 300: Loss = 2.1502\n",
      "Epoch 1, Step 310: Loss = 0.0982\n",
      "Epoch 1, Step 320: Loss = 1.4705\n",
      "Epoch 1, Step 330: Loss = 0.3039\n",
      "Epoch 1, Step 340: Loss = 1.6855\n",
      "Epoch 1, Step 350: Loss = 0.1830\n",
      "Epoch 1, Step 360: Loss = 1.6974\n",
      "Epoch 1, Step 370: Loss = 2.1872\n",
      "Epoch 1, Step 380: Loss = 1.2150\n",
      "Epoch 1, Step 390: Loss = 0.6456\n",
      "Epoch 1, Step 400: Loss = 1.4889\n",
      "Epoch 1, Step 410: Loss = 1.0331\n",
      "Epoch 1, Step 420: Loss = 0.9796\n",
      "Epoch 1, Step 430: Loss = 1.8442\n",
      "Epoch 1, Step 440: Loss = 0.1295\n",
      "Epoch 1, Step 450: Loss = 1.1271\n",
      "Epoch 1, Step 460: Loss = 3.1933\n",
      "Epoch 1, Step 470: Loss = 0.0908\n",
      "Epoch 1, Step 480: Loss = 0.0092\n",
      "Epoch 1, Step 490: Loss = 0.9432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7282\n",
      "precision: 0.7359\n",
      "recall: 0.7120\n",
      "f1: 0.7238\n",
      "loss: 1.6450\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7520\n",
      "precision: 0.7614\n",
      "recall: 0.7340\n",
      "f1: 0.7475\n",
      "loss: 1.5600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 0: Loss = 0.6680\n",
      "Epoch 2, Step 10: Loss = 5.3790\n",
      "Epoch 2, Step 20: Loss = 2.0410\n",
      "Epoch 2, Step 30: Loss = 0.3160\n",
      "Epoch 2, Step 40: Loss = 0.7476\n",
      "Epoch 2, Step 50: Loss = 1.2208\n",
      "Epoch 2, Step 60: Loss = 2.3000\n",
      "Epoch 2, Step 70: Loss = 0.1875\n",
      "Epoch 2, Step 80: Loss = 0.0956\n",
      "Epoch 2, Step 90: Loss = 1.4404\n",
      "Epoch 2, Step 100: Loss = 0.5797\n",
      "Epoch 2, Step 110: Loss = 0.9452\n",
      "Epoch 2, Step 120: Loss = 0.0022\n",
      "Epoch 2, Step 130: Loss = 1.6293\n",
      "Epoch 2, Step 140: Loss = 1.2620\n",
      "Epoch 2, Step 150: Loss = 0.0062\n",
      "Epoch 2, Step 160: Loss = 0.7824\n",
      "Epoch 2, Step 170: Loss = 2.5513\n",
      "Epoch 2, Step 180: Loss = 1.5934\n",
      "Epoch 2, Step 190: Loss = 0.2229\n",
      "Epoch 2, Step 200: Loss = 1.0765\n",
      "Epoch 2, Step 210: Loss = 0.9237\n",
      "Epoch 2, Step 220: Loss = 0.3269\n",
      "Epoch 2, Step 230: Loss = 0.3030\n",
      "Epoch 2, Step 240: Loss = 0.7473\n",
      "Epoch 2, Step 250: Loss = 1.0516\n",
      "Epoch 2, Step 260: Loss = 0.0087\n",
      "Epoch 2, Step 270: Loss = 0.9324\n",
      "Epoch 2, Step 280: Loss = 2.4138\n",
      "Epoch 2, Step 290: Loss = 1.5012\n",
      "Epoch 2, Step 300: Loss = 1.1911\n",
      "Epoch 2, Step 310: Loss = 0.8329\n",
      "Epoch 2, Step 320: Loss = 1.1064\n",
      "Epoch 2, Step 330: Loss = 0.0670\n",
      "Epoch 2, Step 340: Loss = 1.2272\n",
      "Epoch 2, Step 350: Loss = 1.7605\n",
      "Epoch 2, Step 360: Loss = 0.3988\n",
      "Epoch 2, Step 370: Loss = 1.7402\n",
      "Epoch 2, Step 380: Loss = 1.2879\n",
      "Epoch 2, Step 390: Loss = 0.7198\n",
      "Epoch 2, Step 400: Loss = 0.4637\n",
      "Epoch 2, Step 410: Loss = 0.0158\n",
      "Epoch 2, Step 420: Loss = 4.7654\n",
      "Epoch 2, Step 430: Loss = 1.6045\n",
      "Epoch 2, Step 440: Loss = 3.2588\n",
      "Epoch 2, Step 450: Loss = 0.7160\n",
      "Epoch 2, Step 460: Loss = 2.2364\n",
      "Epoch 2, Step 470: Loss = 0.3882\n",
      "Epoch 2, Step 480: Loss = 0.4319\n",
      "Epoch 2, Step 490: Loss = 0.8271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7725\n",
      "precision: 0.7806\n",
      "recall: 0.7580\n",
      "f1: 0.7692\n",
      "loss: 1.3109\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7950\n",
      "precision: 0.8004\n",
      "recall: 0.7860\n",
      "f1: 0.7931\n",
      "loss: 1.2643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 0: Loss = 1.9813\n",
      "Epoch 3, Step 10: Loss = 1.4896\n",
      "Epoch 3, Step 20: Loss = 1.3373\n",
      "Epoch 3, Step 30: Loss = 0.4250\n",
      "Epoch 3, Step 40: Loss = 3.3058\n",
      "Epoch 3, Step 50: Loss = 0.5395\n",
      "Epoch 3, Step 60: Loss = 0.0563\n",
      "Epoch 3, Step 70: Loss = 0.2794\n",
      "Epoch 3, Step 80: Loss = 1.1111\n",
      "Epoch 3, Step 90: Loss = 0.6751\n",
      "Epoch 3, Step 100: Loss = 0.4981\n",
      "Epoch 3, Step 110: Loss = 3.0836\n",
      "Epoch 3, Step 120: Loss = 0.0008\n",
      "Epoch 3, Step 130: Loss = 0.0072\n",
      "Epoch 3, Step 140: Loss = 1.8848\n",
      "Epoch 3, Step 150: Loss = 0.2924\n",
      "Epoch 3, Step 160: Loss = 1.2520\n",
      "Epoch 3, Step 170: Loss = 0.6967\n",
      "Epoch 3, Step 180: Loss = 0.0687\n",
      "Epoch 3, Step 190: Loss = 1.2162\n",
      "Epoch 3, Step 200: Loss = 0.9944\n",
      "Epoch 3, Step 210: Loss = 1.4511\n",
      "Epoch 3, Step 220: Loss = 0.4858\n",
      "Epoch 3, Step 230: Loss = 0.7678\n",
      "Epoch 3, Step 240: Loss = 3.5716\n",
      "Epoch 3, Step 250: Loss = 0.0038\n",
      "Epoch 3, Step 260: Loss = 0.7577\n",
      "Epoch 3, Step 270: Loss = 0.3862\n",
      "Epoch 3, Step 280: Loss = 0.3920\n",
      "Epoch 3, Step 290: Loss = 0.2247\n",
      "Epoch 3, Step 300: Loss = 1.9727\n",
      "Epoch 3, Step 310: Loss = 0.1415\n",
      "Epoch 3, Step 320: Loss = 1.3894\n",
      "Epoch 3, Step 330: Loss = 0.0004\n",
      "Epoch 3, Step 340: Loss = 1.0653\n",
      "Epoch 3, Step 350: Loss = 0.5205\n",
      "Epoch 3, Step 360: Loss = 1.6339\n",
      "Epoch 3, Step 370: Loss = 1.5725\n",
      "Epoch 3, Step 380: Loss = 0.6402\n",
      "Epoch 3, Step 390: Loss = 0.7326\n",
      "Epoch 3, Step 400: Loss = 1.0319\n",
      "Epoch 3, Step 410: Loss = 2.0412\n",
      "Epoch 3, Step 420: Loss = 0.9399\n",
      "Epoch 3, Step 430: Loss = 0.3288\n",
      "Epoch 3, Step 440: Loss = 0.0204\n",
      "Epoch 3, Step 450: Loss = 1.2513\n",
      "Epoch 3, Step 460: Loss = 0.5049\n",
      "Epoch 3, Step 470: Loss = 0.6199\n",
      "Epoch 3, Step 480: Loss = 0.6418\n",
      "Epoch 3, Step 490: Loss = 0.3813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8090\n",
      "precision: 0.8150\n",
      "recall: 0.7995\n",
      "f1: 0.8072\n",
      "loss: 1.0917\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8150\n",
      "precision: 0.8182\n",
      "recall: 0.8100\n",
      "f1: 0.8141\n",
      "loss: 1.1312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 0: Loss = 0.0235\n",
      "Epoch 4, Step 10: Loss = 0.0675\n",
      "Epoch 4, Step 20: Loss = 1.7205\n",
      "Epoch 4, Step 30: Loss = 0.0070\n",
      "Epoch 4, Step 40: Loss = 1.5523\n",
      "Epoch 4, Step 50: Loss = 0.0353\n",
      "Epoch 4, Step 60: Loss = 1.0944\n",
      "Epoch 4, Step 70: Loss = 0.1157\n",
      "Epoch 4, Step 80: Loss = 0.0001\n",
      "Epoch 4, Step 90: Loss = 3.7725\n",
      "Epoch 4, Step 100: Loss = 0.1539\n",
      "Epoch 4, Step 110: Loss = 1.3683\n",
      "Epoch 4, Step 120: Loss = 0.2842\n",
      "Epoch 4, Step 130: Loss = 0.5060\n",
      "Epoch 4, Step 140: Loss = 1.1001\n",
      "Epoch 4, Step 150: Loss = 1.8131\n",
      "Epoch 4, Step 160: Loss = 0.0000\n",
      "Epoch 4, Step 170: Loss = 0.2027\n",
      "Epoch 4, Step 180: Loss = 0.8934\n",
      "Epoch 4, Step 190: Loss = 0.0274\n",
      "Epoch 4, Step 200: Loss = 2.2252\n",
      "Epoch 4, Step 210: Loss = 2.7705\n",
      "Epoch 4, Step 220: Loss = 0.0204\n",
      "Epoch 4, Step 230: Loss = 0.7386\n",
      "Epoch 4, Step 240: Loss = 1.1641\n",
      "Epoch 4, Step 250: Loss = 0.6545\n",
      "Epoch 4, Step 260: Loss = 1.7471\n",
      "Epoch 4, Step 270: Loss = 0.9409\n",
      "Epoch 4, Step 280: Loss = 0.5092\n",
      "Epoch 4, Step 290: Loss = 3.3073\n",
      "Epoch 4, Step 300: Loss = 1.3312\n",
      "Epoch 4, Step 310: Loss = 0.0235\n",
      "Epoch 4, Step 320: Loss = 1.0776\n",
      "Epoch 4, Step 330: Loss = 0.0004\n",
      "Epoch 4, Step 340: Loss = 1.5204\n",
      "Epoch 4, Step 350: Loss = 1.0115\n",
      "Epoch 4, Step 360: Loss = 0.0008\n",
      "Epoch 4, Step 370: Loss = 0.1369\n",
      "Epoch 4, Step 380: Loss = 1.2862\n",
      "Epoch 4, Step 390: Loss = 0.4306\n",
      "Epoch 4, Step 400: Loss = 0.0383\n",
      "Epoch 4, Step 410: Loss = 2.1684\n",
      "Epoch 4, Step 420: Loss = 3.1275\n",
      "Epoch 4, Step 430: Loss = 0.8175\n",
      "Epoch 4, Step 440: Loss = 5.5251\n",
      "Epoch 4, Step 450: Loss = 0.5217\n",
      "Epoch 4, Step 460: Loss = 0.3617\n",
      "Epoch 4, Step 470: Loss = 3.4278\n",
      "Epoch 4, Step 480: Loss = 0.0370\n",
      "Epoch 4, Step 490: Loss = 0.7215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8277\n",
      "precision: 0.8329\n",
      "recall: 0.8200\n",
      "f1: 0.8264\n",
      "loss: 0.9861\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8250\n",
      "precision: 0.8283\n",
      "recall: 0.8200\n",
      "f1: 0.8241\n",
      "loss: 1.0580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.0252\n",
      "Epoch 5, Step 10: Loss = 0.9990\n",
      "Epoch 5, Step 20: Loss = 0.2506\n",
      "Epoch 5, Step 30: Loss = 0.2359\n",
      "Epoch 5, Step 40: Loss = 0.5984\n",
      "Epoch 5, Step 50: Loss = 2.0826\n",
      "Epoch 5, Step 60: Loss = 0.2964\n",
      "Epoch 5, Step 70: Loss = 2.6683\n",
      "Epoch 5, Step 80: Loss = 1.2632\n",
      "Epoch 5, Step 90: Loss = 0.0204\n",
      "Epoch 5, Step 100: Loss = 0.4017\n",
      "Epoch 5, Step 110: Loss = 0.3554\n",
      "Epoch 5, Step 120: Loss = 0.0049\n",
      "Epoch 5, Step 130: Loss = 0.2204\n",
      "Epoch 5, Step 140: Loss = 0.7600\n",
      "Epoch 5, Step 150: Loss = 4.6856\n",
      "Epoch 5, Step 160: Loss = 0.7785\n",
      "Epoch 5, Step 170: Loss = 2.0710\n",
      "Epoch 5, Step 180: Loss = 0.7506\n",
      "Epoch 5, Step 190: Loss = 0.0309\n",
      "Epoch 5, Step 200: Loss = 0.0668\n",
      "Epoch 5, Step 210: Loss = 1.4275\n",
      "Epoch 5, Step 220: Loss = 3.5564\n",
      "Epoch 5, Step 230: Loss = 0.0254\n",
      "Epoch 5, Step 240: Loss = 2.2915\n",
      "Epoch 5, Step 250: Loss = 3.4798\n",
      "Epoch 5, Step 260: Loss = 2.8224\n",
      "Epoch 5, Step 270: Loss = 4.6205\n",
      "Epoch 5, Step 280: Loss = 5.9182\n",
      "Epoch 5, Step 290: Loss = 0.4326\n",
      "Epoch 5, Step 300: Loss = 1.2380\n",
      "Epoch 5, Step 310: Loss = 0.3188\n",
      "Epoch 5, Step 320: Loss = 0.1899\n",
      "Epoch 5, Step 330: Loss = 0.0023\n",
      "Epoch 5, Step 340: Loss = 1.5305\n",
      "Epoch 5, Step 350: Loss = 0.2242\n",
      "Epoch 5, Step 360: Loss = 0.6379\n",
      "Epoch 5, Step 370: Loss = 1.5434\n",
      "Epoch 5, Step 380: Loss = 0.3727\n",
      "Epoch 5, Step 390: Loss = 0.4534\n",
      "Epoch 5, Step 400: Loss = 2.0303\n",
      "Epoch 5, Step 410: Loss = 2.0496\n",
      "Epoch 5, Step 420: Loss = 0.0007\n",
      "Epoch 5, Step 430: Loss = 1.0498\n",
      "Epoch 5, Step 440: Loss = 4.6375\n",
      "Epoch 5, Step 450: Loss = 0.0345\n",
      "Epoch 5, Step 460: Loss = 1.0688\n",
      "Epoch 5, Step 470: Loss = 0.4627\n",
      "Epoch 5, Step 480: Loss = 0.2186\n",
      "Epoch 5, Step 490: Loss = 0.1550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8327\n",
      "precision: 0.8363\n",
      "recall: 0.8275\n",
      "f1: 0.8319\n",
      "loss: 0.9378\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8310\n",
      "precision: 0.8330\n",
      "recall: 0.8280\n",
      "f1: 0.8305\n",
      "loss: 1.0209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Step 0: Loss = 0.0105\n",
      "Epoch 6, Step 10: Loss = 1.4917\n",
      "Epoch 6, Step 20: Loss = 1.4822\n",
      "Epoch 6, Step 30: Loss = 0.0328\n",
      "Epoch 6, Step 40: Loss = 1.8999\n",
      "Epoch 6, Step 50: Loss = 2.8694\n",
      "Epoch 6, Step 60: Loss = 1.4722\n",
      "Epoch 6, Step 70: Loss = 0.0027\n",
      "Epoch 6, Step 80: Loss = 0.1290\n",
      "Epoch 6, Step 90: Loss = 0.1732\n",
      "Epoch 6, Step 100: Loss = 0.4670\n",
      "Epoch 6, Step 110: Loss = 0.0830\n",
      "Epoch 6, Step 120: Loss = 2.3553\n",
      "Epoch 6, Step 130: Loss = 0.4628\n",
      "Epoch 6, Step 140: Loss = 1.5506\n",
      "Epoch 6, Step 150: Loss = 0.7670\n",
      "Epoch 6, Step 160: Loss = 0.5367\n",
      "Epoch 6, Step 170: Loss = 0.8829\n",
      "Epoch 6, Step 180: Loss = 0.0512\n",
      "Epoch 6, Step 190: Loss = 0.0002\n",
      "Epoch 6, Step 200: Loss = 0.3914\n",
      "Epoch 6, Step 210: Loss = 0.9976\n",
      "Epoch 6, Step 220: Loss = 0.9878\n",
      "Epoch 6, Step 230: Loss = 0.5310\n",
      "Epoch 6, Step 240: Loss = 3.6658\n",
      "Epoch 6, Step 250: Loss = 4.5252\n",
      "Epoch 6, Step 260: Loss = 0.7195\n",
      "Epoch 6, Step 270: Loss = 1.6892\n",
      "Epoch 6, Step 280: Loss = 0.9117\n",
      "Epoch 6, Step 290: Loss = 1.9796\n",
      "Epoch 6, Step 300: Loss = 0.0748\n",
      "Epoch 6, Step 310: Loss = 2.4122\n",
      "Epoch 6, Step 320: Loss = 0.7010\n",
      "Epoch 6, Step 330: Loss = 0.1726\n",
      "Epoch 6, Step 340: Loss = 4.3093\n",
      "Epoch 6, Step 350: Loss = 1.4084\n",
      "Epoch 6, Step 360: Loss = 0.4913\n",
      "Epoch 6, Step 370: Loss = 0.0026\n",
      "Epoch 6, Step 380: Loss = 0.2246\n",
      "Epoch 6, Step 390: Loss = 0.0181\n",
      "Epoch 6, Step 400: Loss = 0.1518\n",
      "Epoch 6, Step 410: Loss = 0.3764\n",
      "Epoch 6, Step 420: Loss = 2.6090\n",
      "Epoch 6, Step 430: Loss = 4.9323\n",
      "Epoch 6, Step 440: Loss = 0.0001\n",
      "Epoch 6, Step 450: Loss = 0.7851\n",
      "Epoch 6, Step 460: Loss = 2.3120\n",
      "Epoch 6, Step 470: Loss = 1.4000\n",
      "Epoch 6, Step 480: Loss = 0.1744\n",
      "Epoch 6, Step 490: Loss = 2.8646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8373\n",
      "precision: 0.8412\n",
      "recall: 0.8315\n",
      "f1: 0.8363\n",
      "loss: 0.9063\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8350\n",
      "precision: 0.8370\n",
      "recall: 0.8320\n",
      "f1: 0.8345\n",
      "loss: 1.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Step 0: Loss = 1.4759\n",
      "Epoch 7, Step 10: Loss = 1.0453\n",
      "Epoch 7, Step 20: Loss = 1.0154\n",
      "Epoch 7, Step 30: Loss = 0.1179\n",
      "Epoch 7, Step 40: Loss = 2.3122\n",
      "Epoch 7, Step 50: Loss = 0.4518\n",
      "Epoch 7, Step 60: Loss = 0.7367\n",
      "Epoch 7, Step 70: Loss = 0.0109\n",
      "Epoch 7, Step 80: Loss = 0.0075\n",
      "Epoch 7, Step 90: Loss = 0.0212\n",
      "Epoch 7, Step 100: Loss = 0.0852\n",
      "Epoch 7, Step 110: Loss = 0.1691\n",
      "Epoch 7, Step 120: Loss = 0.0352\n",
      "Epoch 7, Step 130: Loss = 2.2136\n",
      "Epoch 7, Step 140: Loss = 0.8398\n",
      "Epoch 7, Step 150: Loss = 2.2052\n",
      "Epoch 7, Step 160: Loss = 0.0096\n",
      "Epoch 7, Step 170: Loss = 0.0016\n",
      "Epoch 7, Step 180: Loss = 0.5589\n",
      "Epoch 7, Step 190: Loss = 1.2883\n",
      "Epoch 7, Step 200: Loss = 2.7291\n",
      "Epoch 7, Step 210: Loss = 0.0200\n",
      "Epoch 7, Step 220: Loss = 0.7748\n",
      "Epoch 7, Step 230: Loss = 0.0274\n",
      "Epoch 7, Step 240: Loss = 0.0118\n",
      "Epoch 7, Step 250: Loss = 0.4291\n",
      "Epoch 7, Step 260: Loss = 3.4669\n",
      "Epoch 7, Step 270: Loss = 2.6309\n",
      "Epoch 7, Step 280: Loss = 0.1900\n",
      "Epoch 7, Step 290: Loss = 1.2725\n",
      "Epoch 7, Step 300: Loss = 3.7072\n",
      "Epoch 7, Step 310: Loss = 0.0238\n",
      "Epoch 7, Step 320: Loss = 0.0602\n",
      "Epoch 7, Step 330: Loss = 0.6501\n",
      "Epoch 7, Step 340: Loss = 1.6935\n",
      "Epoch 7, Step 350: Loss = 0.0010\n",
      "Epoch 7, Step 360: Loss = 0.7263\n",
      "Epoch 7, Step 370: Loss = 0.5690\n",
      "Epoch 7, Step 380: Loss = 0.5994\n",
      "Epoch 7, Step 390: Loss = 3.0723\n",
      "Epoch 7, Step 400: Loss = 1.9852\n",
      "Epoch 7, Step 410: Loss = 0.0133\n",
      "Epoch 7, Step 420: Loss = 0.0200\n",
      "Epoch 7, Step 430: Loss = 3.9146\n",
      "Epoch 7, Step 440: Loss = 0.0022\n",
      "Epoch 7, Step 450: Loss = 0.2249\n",
      "Epoch 7, Step 460: Loss = 1.1449\n",
      "Epoch 7, Step 470: Loss = 0.5015\n",
      "Epoch 7, Step 480: Loss = 1.0085\n",
      "Epoch 7, Step 490: Loss = 0.2734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8445\n",
      "precision: 0.8480\n",
      "recall: 0.8395\n",
      "f1: 0.8437\n",
      "loss: 0.9052\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8370\n",
      "precision: 0.8377\n",
      "recall: 0.8360\n",
      "f1: 0.8368\n",
      "loss: 0.9929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Step 0: Loss = 0.0014\n",
      "Epoch 8, Step 10: Loss = 0.0007\n",
      "Epoch 8, Step 20: Loss = 0.4336\n",
      "Epoch 8, Step 30: Loss = 0.2674\n",
      "Epoch 8, Step 40: Loss = 0.0563\n",
      "Epoch 8, Step 50: Loss = 1.7358\n",
      "Epoch 8, Step 60: Loss = 0.1541\n",
      "Epoch 8, Step 70: Loss = 0.2670\n",
      "Epoch 8, Step 80: Loss = 4.8683\n",
      "Epoch 8, Step 90: Loss = 0.7329\n",
      "Epoch 8, Step 100: Loss = 1.0300\n",
      "Epoch 8, Step 110: Loss = 0.0068\n",
      "Epoch 8, Step 120: Loss = 1.6105\n",
      "Epoch 8, Step 130: Loss = 2.3194\n",
      "Epoch 8, Step 140: Loss = 0.2140\n",
      "Epoch 8, Step 150: Loss = 1.3234\n",
      "Epoch 8, Step 160: Loss = 0.0667\n",
      "Epoch 8, Step 170: Loss = 2.4458\n",
      "Epoch 8, Step 180: Loss = 0.8809\n",
      "Epoch 8, Step 190: Loss = 0.7185\n",
      "Epoch 8, Step 200: Loss = 0.2951\n",
      "Epoch 8, Step 210: Loss = 0.9569\n",
      "Epoch 8, Step 220: Loss = 0.0390\n",
      "Epoch 8, Step 230: Loss = 0.4283\n",
      "Epoch 8, Step 240: Loss = 2.7035\n",
      "Epoch 8, Step 250: Loss = 1.0798\n",
      "Epoch 8, Step 260: Loss = 0.0638\n",
      "Epoch 8, Step 270: Loss = 0.4754\n",
      "Epoch 8, Step 280: Loss = 1.8602\n",
      "Epoch 8, Step 290: Loss = 1.2391\n",
      "Epoch 8, Step 300: Loss = 2.3353\n",
      "Epoch 8, Step 310: Loss = 0.0063\n",
      "Epoch 8, Step 320: Loss = 1.3217\n",
      "Epoch 8, Step 330: Loss = 0.8218\n",
      "Epoch 8, Step 340: Loss = 1.9920\n",
      "Epoch 8, Step 350: Loss = 4.2004\n",
      "Epoch 8, Step 360: Loss = 0.7333\n",
      "Epoch 8, Step 370: Loss = 0.8467\n",
      "Epoch 8, Step 380: Loss = 0.5726\n",
      "Epoch 8, Step 390: Loss = 4.2615\n",
      "Epoch 8, Step 400: Loss = 0.0483\n",
      "Epoch 8, Step 410: Loss = 1.7045\n",
      "Epoch 8, Step 420: Loss = 0.9896\n",
      "Epoch 8, Step 430: Loss = 0.0374\n",
      "Epoch 8, Step 440: Loss = 0.9479\n",
      "Epoch 8, Step 450: Loss = 0.0038\n",
      "Epoch 8, Step 460: Loss = 4.0675\n",
      "Epoch 8, Step 470: Loss = 0.0747\n",
      "Epoch 8, Step 480: Loss = 0.8742\n",
      "Epoch 8, Step 490: Loss = 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8430\n",
      "precision: 0.8458\n",
      "recall: 0.8390\n",
      "f1: 0.8424\n",
      "loss: 0.8889\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8370\n",
      "precision: 0.8377\n",
      "recall: 0.8360\n",
      "f1: 0.8368\n",
      "loss: 0.9870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 0: Loss = 2.5176\n",
      "Epoch 9, Step 10: Loss = 0.3947\n",
      "Epoch 9, Step 20: Loss = 0.0217\n",
      "Epoch 9, Step 30: Loss = 0.8879\n",
      "Epoch 9, Step 40: Loss = 1.2829\n",
      "Epoch 9, Step 50: Loss = 0.0131\n",
      "Epoch 9, Step 60: Loss = 3.5580\n",
      "Epoch 9, Step 70: Loss = 0.0556\n",
      "Epoch 9, Step 80: Loss = 0.0010\n",
      "Epoch 9, Step 90: Loss = 0.6217\n",
      "Epoch 9, Step 100: Loss = 0.2274\n",
      "Epoch 9, Step 110: Loss = 1.0076\n",
      "Epoch 9, Step 120: Loss = 0.3086\n",
      "Epoch 9, Step 130: Loss = 0.2957\n",
      "Epoch 9, Step 140: Loss = 2.1750\n",
      "Epoch 9, Step 150: Loss = 1.9072\n",
      "Epoch 9, Step 160: Loss = 1.6052\n",
      "Epoch 9, Step 170: Loss = 0.0998\n",
      "Epoch 9, Step 180: Loss = 1.3741\n",
      "Epoch 9, Step 190: Loss = 0.0251\n",
      "Epoch 9, Step 200: Loss = 1.7213\n",
      "Epoch 9, Step 210: Loss = 1.2618\n",
      "Epoch 9, Step 220: Loss = 0.0000\n",
      "Epoch 9, Step 230: Loss = 0.1913\n",
      "Epoch 9, Step 240: Loss = 0.9177\n",
      "Epoch 9, Step 250: Loss = 0.4012\n",
      "Epoch 9, Step 260: Loss = 1.3685\n",
      "Epoch 9, Step 270: Loss = 0.0023\n",
      "Epoch 9, Step 280: Loss = 0.9741\n",
      "Epoch 9, Step 290: Loss = 0.0424\n",
      "Epoch 9, Step 300: Loss = 0.7548\n",
      "Epoch 9, Step 310: Loss = 0.7583\n",
      "Epoch 9, Step 320: Loss = 0.9764\n",
      "Epoch 9, Step 330: Loss = 0.2961\n",
      "Epoch 9, Step 340: Loss = 0.7930\n",
      "Epoch 9, Step 350: Loss = 0.1153\n",
      "Epoch 9, Step 360: Loss = 0.3960\n",
      "Epoch 9, Step 370: Loss = 0.6521\n",
      "Epoch 9, Step 380: Loss = 1.2337\n",
      "Epoch 9, Step 390: Loss = 2.6109\n",
      "Epoch 9, Step 400: Loss = 0.1604\n",
      "Epoch 9, Step 410: Loss = 0.1618\n",
      "Epoch 9, Step 420: Loss = 0.0144\n",
      "Epoch 9, Step 430: Loss = 0.1314\n",
      "Epoch 9, Step 440: Loss = 1.6640\n",
      "Epoch 9, Step 450: Loss = 1.0453\n",
      "Epoch 9, Step 460: Loss = 0.0004\n",
      "Epoch 9, Step 470: Loss = 0.0001\n",
      "Epoch 9, Step 480: Loss = 4.0063\n",
      "Epoch 9, Step 490: Loss = 0.9388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8468\n",
      "precision: 0.8508\n",
      "recall: 0.8410\n",
      "f1: 0.8459\n",
      "loss: 0.8753\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8370\n",
      "precision: 0.8377\n",
      "recall: 0.8360\n",
      "f1: 0.8368\n",
      "loss: 0.9835\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (AutoTokenizer,AutoModelForSequenceClassification,\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "                           BitsAndBytesConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        self.emails_df = emails_df.copy()\n",
    "        self.emails_df['sender'] = self.emails_df['sender'].apply(clean_text)\n",
    "        self.emails_df['subject'] = self.emails_df['subject'].apply(clean_text)\n",
    "        self.emails_df['body'] = self.emails_df['body'].apply(clean_text)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        email = self.emails_df.iloc[idx]\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(email['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model_config.use_cache = False\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, output_dir):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "    \n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    return metrics\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9):\n",
    "    best_val_metrics = {'f1': 0}\n",
    "    best_model_state = None\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % 2 == 0:  # Gradient accumulation steps = 2\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(batch['label'].cpu().numpy())\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "  \n",
    "        train_metrics = compute_metrics(train_preds, train_labels)\n",
    "        train_metrics['loss'] = total_loss / len(train_loader)\n",
    "      \n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "      \n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(\"Training Metrics:\")\n",
    "        for metric, value in train_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        for metric, value in val_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_val_metrics['f1']:\n",
    "            best_val_metrics = val_metrics\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return best_model_state, best_val_metrics\n",
    "\n",
    "def main():\n",
    "    login(token=\n",
    "    device = setup_environment()\n",
    "    model_name = 'dreamgen/WizardLM-2-7B'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/final_data.csv\")\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "    \n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    \n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "    \n",
    "    train_dataset = EmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = EmailDataset(val_df, tokenizer, max_length=512)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    best_model_state, best_metrics = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/wizard8b_binary_classification_model\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    plot_confusion_matrix(best_metrics['confusion_matrix'], output_dir)\n",
    "    \n",
    "  \n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"best_metrics\": {k: float(v) if k != 'confusion_matrix' else v.tolist() \n",
    "                        for k, v in best_metrics.items()}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e094b649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n",
      "GPU Memory: 23.68 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 0.7362\n",
      "Epoch 1, Step 10: Loss = 0.8049\n",
      "Epoch 1, Step 20: Loss = 0.7581\n",
      "Epoch 1, Step 30: Loss = 0.7583\n",
      "Epoch 1, Step 40: Loss = 0.6847\n",
      "Epoch 1, Step 50: Loss = 0.7262\n",
      "Epoch 1, Step 60: Loss = 0.7710\n",
      "Epoch 1, Step 70: Loss = 0.6936\n",
      "Epoch 1, Step 80: Loss = 0.6389\n",
      "Epoch 1, Step 90: Loss = 0.6876\n",
      "Epoch 1, Step 100: Loss = 0.7184\n",
      "Epoch 1, Step 110: Loss = 0.7079\n",
      "Epoch 1, Step 120: Loss = 0.6588\n",
      "Epoch 1, Step 130: Loss = 0.7551\n",
      "Epoch 1, Step 140: Loss = 0.6582\n",
      "Epoch 1, Step 150: Loss = 0.7095\n",
      "Epoch 1, Step 160: Loss = 0.6176\n",
      "Epoch 1, Step 170: Loss = 0.6758\n",
      "Epoch 1, Step 180: Loss = 0.7016\n",
      "Epoch 1, Step 190: Loss = 0.6051\n",
      "Epoch 1, Step 200: Loss = 0.6323\n",
      "Epoch 1, Step 210: Loss = 0.6035\n",
      "Epoch 1, Step 220: Loss = 0.6091\n",
      "Epoch 1, Step 230: Loss = 0.5587\n",
      "Epoch 1, Step 240: Loss = 0.5516\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.5992\n",
      "precision: 0.6195\n",
      "recall: 0.5145\n",
      "f1: 0.5621\n",
      "loss: 0.6784\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8950\n",
      "precision: 0.8791\n",
      "recall: 0.9160\n",
      "f1: 0.8972\n",
      "loss: 0.5305\n",
      "Epoch 2, Step 0: Loss = 0.5718\n",
      "Epoch 2, Step 10: Loss = 0.5122\n",
      "Epoch 2, Step 20: Loss = 0.5495\n",
      "Epoch 2, Step 30: Loss = 0.5439\n",
      "Epoch 2, Step 40: Loss = 0.4631\n",
      "Epoch 2, Step 50: Loss = 0.3865\n",
      "Epoch 2, Step 60: Loss = 0.3700\n",
      "Epoch 2, Step 70: Loss = 0.3748\n",
      "Epoch 2, Step 80: Loss = 0.3427\n",
      "Epoch 2, Step 90: Loss = 0.3151\n",
      "Epoch 2, Step 100: Loss = 0.4066\n",
      "Epoch 2, Step 110: Loss = 0.2928\n",
      "Epoch 2, Step 120: Loss = 0.3104\n",
      "Epoch 2, Step 130: Loss = 0.2232\n",
      "Epoch 2, Step 140: Loss = 0.1969\n",
      "Epoch 2, Step 150: Loss = 0.2618\n",
      "Epoch 2, Step 160: Loss = 0.2460\n",
      "Epoch 2, Step 170: Loss = 0.1457\n",
      "Epoch 2, Step 180: Loss = 0.1368\n",
      "Epoch 2, Step 190: Loss = 0.1424\n",
      "Epoch 2, Step 200: Loss = 0.1213\n",
      "Epoch 2, Step 210: Loss = 0.0958\n",
      "Epoch 2, Step 220: Loss = 0.1237\n",
      "Epoch 2, Step 230: Loss = 0.0529\n",
      "Epoch 2, Step 240: Loss = 0.0638\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9415\n",
      "precision: 0.9415\n",
      "recall: 0.9415\n",
      "f1: 0.9415\n",
      "loss: 0.2869\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9760\n",
      "precision: 0.9741\n",
      "recall: 0.9780\n",
      "f1: 0.9760\n",
      "loss: 0.1001\n",
      "Epoch 3, Step 0: Loss = 0.0696\n",
      "Epoch 3, Step 10: Loss = 0.0788\n",
      "Epoch 3, Step 20: Loss = 0.0483\n",
      "Epoch 3, Step 30: Loss = 0.0413\n",
      "Epoch 3, Step 40: Loss = 0.0695\n",
      "Epoch 3, Step 50: Loss = 0.1370\n",
      "Epoch 3, Step 60: Loss = 0.2446\n",
      "Epoch 3, Step 70: Loss = 0.0294\n",
      "Epoch 3, Step 80: Loss = 0.4192\n",
      "Epoch 3, Step 90: Loss = 0.1233\n",
      "Epoch 3, Step 100: Loss = 0.0322\n",
      "Epoch 3, Step 110: Loss = 0.1845\n",
      "Epoch 3, Step 120: Loss = 0.1172\n",
      "Epoch 3, Step 130: Loss = 0.0866\n",
      "Epoch 3, Step 140: Loss = 0.0218\n",
      "Epoch 3, Step 150: Loss = 0.0209\n",
      "Epoch 3, Step 160: Loss = 0.0317\n",
      "Epoch 3, Step 170: Loss = 0.0193\n",
      "Epoch 3, Step 180: Loss = 0.0195\n",
      "Epoch 3, Step 190: Loss = 0.0185\n",
      "Epoch 3, Step 200: Loss = 0.0301\n",
      "Epoch 3, Step 210: Loss = 0.1033\n",
      "Epoch 3, Step 220: Loss = 0.0237\n",
      "Epoch 3, Step 230: Loss = 0.0440\n",
      "Epoch 3, Step 240: Loss = 0.0154\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9795\n",
      "precision: 0.9843\n",
      "recall: 0.9745\n",
      "f1: 0.9794\n",
      "loss: 0.0814\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9790\n",
      "precision: 0.9761\n",
      "recall: 0.9820\n",
      "f1: 0.9791\n",
      "loss: 0.0624\n",
      "Epoch 4, Step 0: Loss = 0.0132\n",
      "Epoch 4, Step 10: Loss = 0.5088\n",
      "Epoch 4, Step 20: Loss = 0.0140\n",
      "Epoch 4, Step 30: Loss = 0.0206\n",
      "Epoch 4, Step 40: Loss = 0.3187\n",
      "Epoch 4, Step 50: Loss = 0.1893\n",
      "Epoch 4, Step 60: Loss = 0.0274\n",
      "Epoch 4, Step 70: Loss = 0.0132\n",
      "Epoch 4, Step 80: Loss = 0.0260\n",
      "Epoch 4, Step 90: Loss = 0.0092\n",
      "Epoch 4, Step 100: Loss = 0.0102\n",
      "Epoch 4, Step 110: Loss = 0.0593\n",
      "Epoch 4, Step 120: Loss = 0.0107\n",
      "Epoch 4, Step 130: Loss = 0.2714\n",
      "Epoch 4, Step 140: Loss = 0.0093\n",
      "Epoch 4, Step 150: Loss = 0.0087\n",
      "Epoch 4, Step 160: Loss = 0.0085\n",
      "Epoch 4, Step 170: Loss = 0.0155\n",
      "Epoch 4, Step 180: Loss = 0.0101\n",
      "Epoch 4, Step 190: Loss = 0.0070\n",
      "Epoch 4, Step 200: Loss = 0.1802\n",
      "Epoch 4, Step 210: Loss = 0.0067\n",
      "Epoch 4, Step 220: Loss = 0.0110\n",
      "Epoch 4, Step 230: Loss = 0.1000\n",
      "Epoch 4, Step 240: Loss = 0.0291\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9842\n",
      "precision: 0.9909\n",
      "recall: 0.9775\n",
      "f1: 0.9841\n",
      "loss: 0.0542\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9850\n",
      "precision: 0.9879\n",
      "recall: 0.9820\n",
      "f1: 0.9850\n",
      "loss: 0.0444\n",
      "Epoch 5, Step 0: Loss = 0.0149\n",
      "Epoch 5, Step 10: Loss = 0.0405\n",
      "Epoch 5, Step 20: Loss = 0.0086\n",
      "Epoch 5, Step 30: Loss = 0.0063\n",
      "Epoch 5, Step 40: Loss = 0.0073\n",
      "Epoch 5, Step 50: Loss = 0.0141\n",
      "Epoch 5, Step 60: Loss = 0.0043\n",
      "Epoch 5, Step 70: Loss = 0.0540\n",
      "Epoch 5, Step 80: Loss = 0.0112\n",
      "Epoch 5, Step 90: Loss = 0.0043\n",
      "Epoch 5, Step 100: Loss = 0.0062\n",
      "Epoch 5, Step 110: Loss = 0.0233\n",
      "Epoch 5, Step 120: Loss = 0.0061\n",
      "Epoch 5, Step 130: Loss = 0.0043\n",
      "Epoch 5, Step 140: Loss = 0.0509\n",
      "Epoch 5, Step 150: Loss = 0.0930\n",
      "Epoch 5, Step 160: Loss = 0.0150\n",
      "Epoch 5, Step 170: Loss = 0.0095\n",
      "Epoch 5, Step 180: Loss = 0.2669\n",
      "Epoch 5, Step 190: Loss = 0.0139\n",
      "Epoch 5, Step 200: Loss = 0.0050\n",
      "Epoch 5, Step 210: Loss = 0.0104\n",
      "Epoch 5, Step 220: Loss = 0.0160\n",
      "Epoch 5, Step 230: Loss = 0.0052\n",
      "Epoch 5, Step 240: Loss = 0.0082\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9855\n",
      "precision: 0.9914\n",
      "recall: 0.9795\n",
      "f1: 0.9854\n",
      "loss: 0.0489\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9880\n",
      "precision: 0.9939\n",
      "recall: 0.9820\n",
      "f1: 0.9879\n",
      "loss: 0.0401\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import gc\n",
    "\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        self.emails_df = emails_df.copy()\n",
    "        self.emails_df['sender'] = self.emails_df['sender'].apply(clean_text)\n",
    "        self.emails_df['subject'] = self.emails_df['subject'].apply(clean_text)\n",
    "        self.emails_df['body'] = self.emails_df['body'].apply(clean_text)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        email = self.emails_df.iloc[idx]\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(email['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config\n",
    "    )\n",
    "    \n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"query\", \"value\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['label']\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "    \n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    return metrics\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=5):\n",
    "    best_val_metrics = {'f1': 0}\n",
    "    best_model_state = None\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['label']\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % 2 == 0:  # Gradient accumulation steps = 2\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(batch['label'].cpu().numpy())\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "  \n",
    "        train_metrics = compute_metrics(train_preds, train_labels)\n",
    "        train_metrics['loss'] = total_loss / len(train_loader)\n",
    "      \n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "      \n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(\"Training Metrics:\")\n",
    "        for metric, value in train_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        for metric, value in val_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        if val_metrics['f1'] > best_val_metrics['f1']:\n",
    "            best_val_metrics = val_metrics\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return best_model_state, best_val_metrics\n",
    "\n",
    "def main():\n",
    "    login(token=\n",
    "    device = setup_environment()\n",
    "    model_name = 'bert-base-uncased'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/final_data.csv\")\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "    \n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    \n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "    \n",
    "    train_dataset = EmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = EmailDataset(val_df, tokenizer, max_length=512)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)  \n",
    "    num_epochs = 5\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    best_model_state, best_metrics = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/bert_binary_classification_model\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    \n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 16,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"best_metrics\": {k: float(v) if k != 'confusion_matrix' else v.tolist() \n",
    "                        for k, v in best_metrics.items()}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf517b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "Loading model from /home/users/skuikel/Downloads/Tune/FineTune/lama7b_binary_classification_model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ff989afce74fd8999135fa75833b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Predictions saved successfully!\n",
      "                        Sender                               Subject  \\\n",
      "0       noreply@powerballs.com                         You Have Won!   \n",
      "1        noreply@paypalceo.com                         PayPal Breach   \n",
      "2     support@credit.chase.com  URGENT: Fraudulent activity detected   \n",
      "3               mary@yahoo.com             Donations needed for Mark   \n",
      "4  support@security.amazon.com                  Your Amazon Account    \n",
      "\n",
      "                                               Email Email_type  prediction  \\\n",
      "0  <p>*********PLEASE DO NOT RESPOND TO THIS EMAI...   Phishing           1   \n",
      "1  <p>********* RESPONES TO THIS EMAIL WILL NOT B...   Phishing           1   \n",
      "2  <p>Hello,&nbsp;</p><p>&nbsp;</p><p>We are writ...   Phishing           1   \n",
      "3  <p>Hello,</p><p>&nbsp;</p><p>I&#39;m contactin...   Phishing           1   \n",
      "4  <p><strong>The account number associated with ...   Phishing           1   \n",
      "\n",
      "   confidence_score  \n",
      "0          0.795860  \n",
      "1          0.913134  \n",
      "2          0.805710  \n",
      "3          0.886202  \n",
      "4          0.834758  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Set up GPU/CPU environment and optimize CUDA settings.\"\"\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'  # Set GPU device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    return device\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text data.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()  # Convert to lowercase and remove leading/trailing spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    return text\n",
    "\n",
    "def load_model(model_dir, device):\n",
    "    \"\"\"Load fine-tuned LLaMA model and tokenizer from local directory.\"\"\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        raise FileNotFoundError(f\"Trained model directory not found: {model_dir}\")\n",
    "\n",
    "    print(f\"Loading model from {model_dir}...\")\n",
    "    model = LlamaForSequenceClassification.from_pretrained(model_dir).to(device)\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def predict_email(model, tokenizer, email_df, device, max_length=512):\n",
    "    \"\"\"Predict whether an email is Ham (0) or Phishing (1) with confidence scores.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    confidence_scores = []\n",
    "\n",
    "    for _, email in email_df.iterrows():\n",
    "        input_text = f\"Sender: {clean_text(email['Sender'])} [SEP] Subject: {clean_text(email['Subject'])} [SEP] {clean_text(email['Email'])}\"\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)  \n",
    "        confidence, pred_label = torch.max(probs, dim=-1)\n",
    "\n",
    "        predictions.append(pred_label.item())\n",
    "        confidence_scores.append(confidence.item())\n",
    "\n",
    "    email_df['prediction'] = predictions  \n",
    "    email_df['confidence_score'] = confidence_scores\n",
    "\n",
    "    \n",
    "    required_columns = ['Sender', 'Subject', 'Email']\n",
    "    if 'Email_type' in email_df.columns:\n",
    "        required_columns.append('Email_type')\n",
    "\n",
    "    final_df = email_df[required_columns + ['prediction', 'confidence_score']]\n",
    "    return final_df\n",
    "\n",
    "def main():\n",
    "    \n",
    "    try:\n",
    "        login(token=\"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\") \n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        return\n",
    "\n",
    "    device = setup_environment()\n",
    "\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/lama7b_binary_classification_model\")\n",
    "    model, tokenizer = load_model(model_dir, device)\n",
    "\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Data file not found: {data_path}\")\n",
    "        return\n",
    "\n",
    "    new_emails_df = pd.read_excel(data_path)\n",
    "\n",
    "    required_columns = {'Sender', 'Subject', 'Email'}\n",
    "    if not required_columns.issubset(new_emails_df.columns):\n",
    "        raise ValueError(f\"Dataset must contain the following columns: {required_columns}\")\n",
    "\n",
    "    predictions_df = predict_email(model, tokenizer, new_emails_df, device)\n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/lama7b_predictions\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    predictions_df.to_csv(os.path.join(output_dir, \"lama7b_predictions.csv\"), index=False)\n",
    "    \n",
    "    print(\"Predictions saved successfully!\")\n",
    "    print(predictions_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "186ab201",
   "metadata": {},
   "outputs": [],
   "source": [
    " file = pd.read_csv(\"lama7b_predictions/lama7b_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "194af4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sender</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Email</th>\n",
       "      <th>Email_type</th>\n",
       "      <th>prediction</th>\n",
       "      <th>confidence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>noreply@powerballs.com</td>\n",
       "      <td>You Have Won!</td>\n",
       "      <td>&lt;p&gt;*********PLEASE DO NOT RESPOND TO THIS EMAI...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.795860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>noreply@paypalceo.com</td>\n",
       "      <td>PayPal Breach</td>\n",
       "      <td>&lt;p&gt;********* RESPONES TO THIS EMAIL WILL NOT B...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.913134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>support@credit.chase.com</td>\n",
       "      <td>URGENT: Fraudulent activity detected</td>\n",
       "      <td>&lt;p&gt;Hello,&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;We are writ...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.805710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mary@yahoo.com</td>\n",
       "      <td>Donations needed for Mark</td>\n",
       "      <td>&lt;p&gt;Hello,&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;I&amp;#39;m contactin...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.886202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>support@security.amazon.com</td>\n",
       "      <td>Your Amazon Account</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;The account number associated with ...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.834758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>no-reply@yahoo.com</td>\n",
       "      <td>Password change for your Yahoo account</td>\n",
       "      <td>Hi Ethan,&lt;br&gt;&lt;br&gt;The password for your Yahoo a...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1</td>\n",
       "      <td>0.887091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>communications@em.aetna.com</td>\n",
       "      <td>Protect your health records on your Aetna memb...</td>\n",
       "      <td>&lt;h2 align=\"center\"&gt;Protecting your personal in...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1</td>\n",
       "      <td>0.895285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>no-reply@dropboxmail.com</td>\n",
       "      <td>jacab invited you to check out Dropbox</td>\n",
       "      <td>Hi there,&lt;br&gt;&lt;p&gt;Jacob (jacob14@gmail.com) thin...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1</td>\n",
       "      <td>0.793260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>help-check@human.resource.com</td>\n",
       "      <td>An important email to respond</td>\n",
       "      <td>&lt;p&gt;Hello!&lt;/p&gt;&lt;p&gt;This email is used for &lt;b&gt;atte...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1</td>\n",
       "      <td>0.847476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>help-check@human.resource.com</td>\n",
       "      <td>An important email to respond</td>\n",
       "      <td>&lt;p&gt;Hello!&lt;/p&gt;&lt;p&gt;This email is used for &lt;b&gt;atte...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1</td>\n",
       "      <td>0.851366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Sender  \\\n",
       "0           noreply@powerballs.com   \n",
       "1            noreply@paypalceo.com   \n",
       "2         support@credit.chase.com   \n",
       "3                   mary@yahoo.com   \n",
       "4      support@security.amazon.com   \n",
       "..                             ...   \n",
       "236             no-reply@yahoo.com   \n",
       "237    communications@em.aetna.com   \n",
       "238       no-reply@dropboxmail.com   \n",
       "239  help-check@human.resource.com   \n",
       "240  help-check@human.resource.com   \n",
       "\n",
       "                                               Subject  \\\n",
       "0                                        You Have Won!   \n",
       "1                                        PayPal Breach   \n",
       "2                 URGENT: Fraudulent activity detected   \n",
       "3                            Donations needed for Mark   \n",
       "4                                 Your Amazon Account    \n",
       "..                                                 ...   \n",
       "236             Password change for your Yahoo account   \n",
       "237  Protect your health records on your Aetna memb...   \n",
       "238             jacab invited you to check out Dropbox   \n",
       "239                      An important email to respond   \n",
       "240                      An important email to respond   \n",
       "\n",
       "                                                 Email Email_type  prediction  \\\n",
       "0    <p>*********PLEASE DO NOT RESPOND TO THIS EMAI...   Phishing           1   \n",
       "1    <p>********* RESPONES TO THIS EMAIL WILL NOT B...   Phishing           1   \n",
       "2    <p>Hello,&nbsp;</p><p>&nbsp;</p><p>We are writ...   Phishing           1   \n",
       "3    <p>Hello,</p><p>&nbsp;</p><p>I&#39;m contactin...   Phishing           1   \n",
       "4    <p><strong>The account number associated with ...   Phishing           1   \n",
       "..                                                 ...        ...         ...   \n",
       "236  Hi Ethan,<br><br>The password for your Yahoo a...        ham           1   \n",
       "237  <h2 align=\"center\">Protecting your personal in...        ham           1   \n",
       "238  Hi there,<br><p>Jacob (jacob14@gmail.com) thin...        ham           1   \n",
       "239  <p>Hello!</p><p>This email is used for <b>atte...        ham           1   \n",
       "240  <p>Hello!</p><p>This email is used for <b>atte...        ham           1   \n",
       "\n",
       "     confidence_score  \n",
       "0            0.795860  \n",
       "1            0.913134  \n",
       "2            0.805710  \n",
       "3            0.886202  \n",
       "4            0.834758  \n",
       "..                ...  \n",
       "236          0.887091  \n",
       "237          0.895285  \n",
       "238          0.793260  \n",
       "239          0.847476  \n",
       "240          0.851366  \n",
       "\n",
       "[241 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8693e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "Loading model from /home/users/skuikel/Downloads/Tune/FineTune/llama8b_binary_classification_model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34703334c92458395a95433e42a0a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Predictions saved successfully!\n",
      "                        Sender                               Subject  \\\n",
      "0       noreply@powerballs.com                         You Have Won!   \n",
      "1        noreply@paypalceo.com                         PayPal Breach   \n",
      "2     support@credit.chase.com  URGENT: Fraudulent activity detected   \n",
      "3               mary@yahoo.com             Donations needed for Mark   \n",
      "4  support@security.amazon.com                  Your Amazon Account    \n",
      "\n",
      "                                               Email Email_type  prediction  \\\n",
      "0  <p>*********PLEASE DO NOT RESPOND TO THIS EMAI...   Phishing           1   \n",
      "1  <p>********* RESPONES TO THIS EMAIL WILL NOT B...   Phishing           1   \n",
      "2  <p>Hello,&nbsp;</p><p>&nbsp;</p><p>We are writ...   Phishing           1   \n",
      "3  <p>Hello,</p><p>&nbsp;</p><p>I&#39;m contactin...   Phishing           1   \n",
      "4  <p><strong>The account number associated with ...   Phishing           1   \n",
      "\n",
      "   confidence_score  \n",
      "0          0.914909  \n",
      "1          0.976488  \n",
      "2          0.984792  \n",
      "3          0.971145  \n",
      "4          0.984316  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text data.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()  \n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    return text\n",
    "\n",
    "def load_model(model_dir, device):\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        raise FileNotFoundError(f\"Trained model directory not found: {model_dir}\")\n",
    "\n",
    "    print(f\"Loading model from {model_dir}...\")\n",
    "    model = LlamaForSequenceClassification.from_pretrained(model_dir).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def predict_email(model, tokenizer, email_df, device, max_length=512):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    confidence_scores = []\n",
    "\n",
    "    for _, email in email_df.iterrows():\n",
    "        input_text = f\"Sender: {clean_text(email['Sender'])} [SEP] Subject: {clean_text(email['Subject'])} [SEP] {clean_text(email['Email'])}\"\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)  \n",
    "        confidence, pred_label = torch.max(probs, dim=-1)\n",
    "\n",
    "        predictions.append(pred_label.item())\n",
    "        confidence_scores.append(confidence.item())\n",
    "\n",
    "    email_df['prediction'] = predictions  \n",
    "    email_df['confidence_score'] = confidence_scores\n",
    "\n",
    "    # Ensure required columns are included\n",
    "    required_columns = ['Sender', 'Subject', 'Email']\n",
    "    if 'Email_type' in email_df.columns:\n",
    "        required_columns.append('Email_type')\n",
    "\n",
    "    final_df = email_df[required_columns + ['prediction', 'confidence_score']]\n",
    "    return final_df\n",
    "\n",
    "def main():\n",
    "    \n",
    "    try:\n",
    "        login(token= \n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        return\n",
    "\n",
    "    device = setup_environment()\n",
    "\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/llama8b_binary_classification_model\")\n",
    "    model, tokenizer = load_model(model_dir, device)\n",
    "\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Data file not found: {data_path}\")\n",
    "        return\n",
    "\n",
    "    new_emails_df = pd.read_excel(data_path)\n",
    "\n",
    "    required_columns = {'Sender', 'Subject', 'Email'}\n",
    "    if not required_columns.issubset(new_emails_df.columns):\n",
    "        raise ValueError(f\"Dataset must contain the following columns: {required_columns}\")\n",
    "\n",
    "    predictions_df = predict_email(model, tokenizer, new_emails_df, device)\n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/lama7b_predictions\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    predictions_df.to_csv(os.path.join(output_dir, \"lama8b_predictions.csv\"), index=False)\n",
    "    \n",
    "    print(\"Predictions saved successfully!\")\n",
    "    print(predictions_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f96294d",
   "metadata": {},
   "outputs": [],
   "source": [
    " file = pd.read_csv(\"lama7b_predictions/lama8b_predictions.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7bbba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Ham emails: 1\n",
      "Number of Phishing emails: 240\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file = pd.read_csv(\"lama7b_predictions/lama8b_predictions.csv\")\n",
    "\n",
    "pred_counts = file[\"prediction\"].value_counts()\n",
    "\n",
    "\n",
    "print(f\"Number of Ham emails: {pred_counts.get(0, 0)}\")\n",
    "print(f\"Number of Phishing emails: {pred_counts.get(1, 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23d19861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "Loading model from /home/users/skuikel/Downloads/Tune/FineTune/wizard8b_binary_classification_model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9446b8bb654ad9a36deb6be36e342c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at dreamgen/WizardLM-2-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Predictions saved successfully!\n",
      "                        Sender                               Subject  \\\n",
      "0       noreply@powerballs.com                         You Have Won!   \n",
      "1        noreply@paypalceo.com                         PayPal Breach   \n",
      "2     support@credit.chase.com  URGENT: Fraudulent activity detected   \n",
      "3               mary@yahoo.com             Donations needed for Mark   \n",
      "4  support@security.amazon.com                  Your Amazon Account    \n",
      "\n",
      "                                               Email Email_type  prediction  \\\n",
      "0  <p>*********PLEASE DO NOT RESPOND TO THIS EMAI...   Phishing           1   \n",
      "1  <p>********* RESPONES TO THIS EMAIL WILL NOT B...   Phishing           0   \n",
      "2  <p>Hello,&nbsp;</p><p>&nbsp;</p><p>We are writ...   Phishing           1   \n",
      "3  <p>Hello,</p><p>&nbsp;</p><p>I&#39;m contactin...   Phishing           0   \n",
      "4  <p><strong>The account number associated with ...   Phishing           1   \n",
      "\n",
      "   confidence_score  \n",
      "0          0.577586  \n",
      "1          0.996301  \n",
      "2          0.778545  \n",
      "3          0.652719  \n",
      "4          0.781194  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()  \n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    return text\n",
    "\n",
    "def load_model(model_dir, device):\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        raise FileNotFoundError(f\"Trained model directory not found: {model_dir}\")\n",
    "\n",
    "    print(f\"Loading model from {model_dir}...\")\n",
    "    model = LlamaForSequenceClassification.from_pretrained(model_dir).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def predict_email(model, tokenizer, email_df, device, max_length=512):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    confidence_scores = []\n",
    "\n",
    "    for _, email in email_df.iterrows():\n",
    "        input_text = f\"Sender: {clean_text(email['Sender'])} [SEP] Subject: {clean_text(email['Subject'])} [SEP] {clean_text(email['Email'])}\"\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)  \n",
    "        confidence, pred_label = torch.max(probs, dim=-1)\n",
    "\n",
    "        predictions.append(pred_label.item())\n",
    "        confidence_scores.append(confidence.item())\n",
    "\n",
    "    email_df['prediction'] = predictions  \n",
    "    email_df['confidence_score'] = confidence_scores\n",
    "\n",
    "    # Ensure required columns are included\n",
    "    required_columns = ['Sender', 'Subject', 'Email']\n",
    "    if 'Email_type' in email_df.columns:\n",
    "        required_columns.append('Email_type')\n",
    "\n",
    "    final_df = email_df[required_columns + ['prediction', 'confidence_score']]\n",
    "    return final_df\n",
    "\n",
    "def main():\n",
    "    \n",
    "    try:\n",
    "        login(token= \n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        return\n",
    "\n",
    "    device = setup_environment()\n",
    "\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/wizard8b_binary_classification_model\")\n",
    "    model, tokenizer = load_model(model_dir, device)\n",
    "\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Data file not found: {data_path}\")\n",
    "        return\n",
    "\n",
    "    new_emails_df = pd.read_excel(data_path)\n",
    "\n",
    "    required_columns = {'Sender', 'Subject', 'Email'}\n",
    "    if not required_columns.issubset(new_emails_df.columns):\n",
    "        raise ValueError(f\"Dataset must contain the following columns: {required_columns}\")\n",
    "\n",
    "    predictions_df = predict_email(model, tokenizer, new_emails_df, device)\n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/lama7b_predictions\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    predictions_df.to_csv(os.path.join(output_dir, \"wizard8b_predictions.csv\"), index=False)\n",
    "    \n",
    "    print(\"Predictions saved successfully!\")\n",
    "    print(predictions_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8a4948b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Ham emails: 121\n",
      "Number of Phishing emails: 120\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file = pd.read_csv(\"lama7b_predictions/wizard8b_predictions.csv\")\n",
    "\n",
    "pred_counts = file[\"prediction\"].value_counts()\n",
    "\n",
    "\n",
    "print(f\"Number of Ham emails: {pred_counts.get(0, 0)}\")\n",
    "print(f\"Number of Phishing emails: {pred_counts.get(1, 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cbff689",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv(\"lama7b_predictions/wizard8b_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc4e996a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sender</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Email</th>\n",
       "      <th>Email_type</th>\n",
       "      <th>prediction</th>\n",
       "      <th>confidence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>noreply@powerballs.com</td>\n",
       "      <td>You Have Won!</td>\n",
       "      <td>&lt;p&gt;*********PLEASE DO NOT RESPOND TO THIS EMAI...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.577586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>noreply@paypalceo.com</td>\n",
       "      <td>PayPal Breach</td>\n",
       "      <td>&lt;p&gt;********* RESPONES TO THIS EMAIL WILL NOT B...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>support@credit.chase.com</td>\n",
       "      <td>URGENT: Fraudulent activity detected</td>\n",
       "      <td>&lt;p&gt;Hello,&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;We are writ...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.778545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mary@yahoo.com</td>\n",
       "      <td>Donations needed for Mark</td>\n",
       "      <td>&lt;p&gt;Hello,&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;I&amp;#39;m contactin...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>0</td>\n",
       "      <td>0.652719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>support@security.amazon.com</td>\n",
       "      <td>Your Amazon Account</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;The account number associated with ...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.781194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>account@micrsoft.com</td>\n",
       "      <td>RE: Help Desk</td>\n",
       "      <td>&lt;p&gt;Dear user,&lt;/p&gt;&lt;p&gt;Regarding your account, we...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.935164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>safety@privacy.chase.com</td>\n",
       "      <td>Update for you Account</td>\n",
       "      <td>&lt;p&gt;Dear Valued Chase Memeber:&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbs...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>customer@fed.ex.com</td>\n",
       "      <td>RE: FED EX TRACKING NUMBER</td>\n",
       "      <td>&lt;p&gt;Dear user,&lt;/p&gt;&lt;p&gt;Unfortunately, we missed y...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.900277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jeremyp@gmail.com</td>\n",
       "      <td>Summer Internship Application</td>\n",
       "      <td>&lt;p&gt;To Whom It May Concern:&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>0</td>\n",
       "      <td>0.695772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>offer@coupons.walmart.com</td>\n",
       "      <td>Walmart Reward Coupons</td>\n",
       "      <td>&lt;p&gt;Account No: 108-455294-800125-MN&lt;/p&gt;&lt;p&gt;&amp;nbs...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>0</td>\n",
       "      <td>0.928215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>account@micrsoft.com</td>\n",
       "      <td>Evaluations Reminder Message</td>\n",
       "      <td>&lt;p&gt;The following evaluations have been assigne...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.957499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>account@micrsoft.com</td>\n",
       "      <td>Evaluations Assigned</td>\n",
       "      <td>&lt;p&gt;The following evaluations have been assigne...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.988254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>notification@gmails.com</td>\n",
       "      <td>Unauthorized Access To Your Account Detected</td>\n",
       "      <td>&lt;p&gt;The following entry was recently addded to ...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.862323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>account@secure.paypal.com</td>\n",
       "      <td>Paypal Account Reminder</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;This is a reminder that your accoun...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>account@secure.paypal.com</td>\n",
       "      <td>Account up for grabs</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;This is a reminder that there&amp;#39;s...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>0</td>\n",
       "      <td>0.923413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>notification@order.amazon.com</td>\n",
       "      <td>Your Amazon.com order of EFFORTINC Vintage Cha...</td>\n",
       "      <td>&lt;p&gt;Hey customer,&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Please cli...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>0</td>\n",
       "      <td>0.940819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>offer@jpmorgan.chase.com</td>\n",
       "      <td>Qualification for a New 30 Year Fixed Term Mor...</td>\n",
       "      <td>&lt;p&gt;Dear Loyal Customer:&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;We ...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.859434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Sender  \\\n",
       "0          noreply@powerballs.com   \n",
       "1           noreply@paypalceo.com   \n",
       "2        support@credit.chase.com   \n",
       "3                  mary@yahoo.com   \n",
       "4     support@security.amazon.com   \n",
       "5            account@micrsoft.com   \n",
       "6        safety@privacy.chase.com   \n",
       "7             customer@fed.ex.com   \n",
       "8               jeremyp@gmail.com   \n",
       "9       offer@coupons.walmart.com   \n",
       "10           account@micrsoft.com   \n",
       "11           account@micrsoft.com   \n",
       "12        notification@gmails.com   \n",
       "13      account@secure.paypal.com   \n",
       "14      account@secure.paypal.com   \n",
       "15  notification@order.amazon.com   \n",
       "16       offer@jpmorgan.chase.com   \n",
       "\n",
       "                                              Subject  \\\n",
       "0                                       You Have Won!   \n",
       "1                                       PayPal Breach   \n",
       "2                URGENT: Fraudulent activity detected   \n",
       "3                           Donations needed for Mark   \n",
       "4                                Your Amazon Account    \n",
       "5                                       RE: Help Desk   \n",
       "6                              Update for you Account   \n",
       "7                          RE: FED EX TRACKING NUMBER   \n",
       "8                       Summer Internship Application   \n",
       "9                              Walmart Reward Coupons   \n",
       "10                       Evaluations Reminder Message   \n",
       "11                               Evaluations Assigned   \n",
       "12       Unauthorized Access To Your Account Detected   \n",
       "13                            Paypal Account Reminder   \n",
       "14                               Account up for grabs   \n",
       "15  Your Amazon.com order of EFFORTINC Vintage Cha...   \n",
       "16  Qualification for a New 30 Year Fixed Term Mor...   \n",
       "\n",
       "                                                Email Email_type  prediction  \\\n",
       "0   <p>*********PLEASE DO NOT RESPOND TO THIS EMAI...   Phishing           1   \n",
       "1   <p>********* RESPONES TO THIS EMAIL WILL NOT B...   Phishing           0   \n",
       "2   <p>Hello,&nbsp;</p><p>&nbsp;</p><p>We are writ...   Phishing           1   \n",
       "3   <p>Hello,</p><p>&nbsp;</p><p>I&#39;m contactin...   Phishing           0   \n",
       "4   <p><strong>The account number associated with ...   Phishing           1   \n",
       "5   <p>Dear user,</p><p>Regarding your account, we...   Phishing           1   \n",
       "6   <p>Dear Valued Chase Memeber:&nbsp;</p><p>&nbs...   Phishing           1   \n",
       "7   <p>Dear user,</p><p>Unfortunately, we missed y...   Phishing           1   \n",
       "8   <p>To Whom It May Concern:</p><p>&nbsp;</p><p>...   Phishing           0   \n",
       "9   <p>Account No: 108-455294-800125-MN</p><p>&nbs...   Phishing           0   \n",
       "10  <p>The following evaluations have been assigne...   Phishing           1   \n",
       "11  <p>The following evaluations have been assigne...   Phishing           1   \n",
       "12  <p>The following entry was recently addded to ...   Phishing           1   \n",
       "13  <p><strong>This is a reminder that your accoun...   Phishing           0   \n",
       "14  <p><strong>This is a reminder that there&#39;s...   Phishing           0   \n",
       "15  <p>Hey customer,</p><p>&nbsp;</p><p>Please cli...   Phishing           0   \n",
       "16  <p>Dear Loyal Customer:</p><p>&nbsp;</p><p>We ...   Phishing           1   \n",
       "\n",
       "    confidence_score  \n",
       "0           0.577586  \n",
       "1           0.996301  \n",
       "2           0.778545  \n",
       "3           0.652719  \n",
       "4           0.781194  \n",
       "5           0.935164  \n",
       "6           0.909046  \n",
       "7           0.900277  \n",
       "8           0.695772  \n",
       "9           0.928215  \n",
       "10          0.957499  \n",
       "11          0.988254  \n",
       "12          0.862323  \n",
       "13          0.997729  \n",
       "14          0.923413  \n",
       "15          0.940819  \n",
       "16          0.859434  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.head(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e79df43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "Loading model from /home/users/skuikel/Downloads/Tune/FineTune/bert_binary_classification_model...\n",
      "Model loaded successfully!\n",
      "Predictions saved successfully!\n",
      "                        Sender                               Subject  \\\n",
      "0       noreply@powerballs.com                         You Have Won!   \n",
      "1        noreply@paypalceo.com                         PayPal Breach   \n",
      "2     support@credit.chase.com  URGENT: Fraudulent activity detected   \n",
      "3               mary@yahoo.com             Donations needed for Mark   \n",
      "4  support@security.amazon.com                  Your Amazon Account    \n",
      "\n",
      "                                               Email Email_type  prediction  \\\n",
      "0  <p>*********PLEASE DO NOT RESPOND TO THIS EMAI...   Phishing           1   \n",
      "1  <p>********* RESPONES TO THIS EMAIL WILL NOT B...   Phishing           1   \n",
      "2  <p>Hello,&nbsp;</p><p>&nbsp;</p><p>We are writ...   Phishing           1   \n",
      "3  <p>Hello,</p><p>&nbsp;</p><p>I&#39;m contactin...   Phishing           0   \n",
      "4  <p><strong>The account number associated with ...   Phishing           1   \n",
      "\n",
      "   confidence_score  \n",
      "0          0.729289  \n",
      "1          0.576073  \n",
      "2          0.680934  \n",
      "3          0.640179  \n",
      "4          0.886676  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text data.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()  \n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    return text\n",
    "\n",
    "def load_model(model_dir, device):\n",
    "    \"\"\"Load fine-tuned BERT model and tokenizer from local directory.\"\"\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        raise FileNotFoundError(f\"Trained model directory not found: {model_dir}\")\n",
    "\n",
    "    print(f\"Loading model from {model_dir}...\")\n",
    "    model = BertForSequenceClassification.from_pretrained(model_dir).to(device)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "    \n",
    "  \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def predict_email(model, tokenizer, email_df, device, max_length=512):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    confidence_scores = []\n",
    "\n",
    "    for _, email in email_df.iterrows():\n",
    "        input_text = f\"Sender: {clean_text(email['Sender'])} [SEP] Subject: {clean_text(email['Subject'])} [SEP] {clean_text(email['Email'])}\"\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)  \n",
    "        confidence, pred_label = torch.max(probs, dim=-1)\n",
    "\n",
    "        predictions.append(pred_label.item())\n",
    "        confidence_scores.append(confidence.item())\n",
    "\n",
    "    email_df['prediction'] = predictions  \n",
    "    email_df['confidence_score'] = confidence_scores\n",
    "\n",
    "   \n",
    "    required_columns = ['Sender', 'Subject', 'Email']\n",
    "    if 'Email_type' in email_df.columns:\n",
    "        required_columns.append('Email_type')\n",
    "\n",
    "    final_df = email_df[required_columns + ['prediction', 'confidence_score']]\n",
    "    return final_df\n",
    "\n",
    "def main():\n",
    "    \n",
    "    try:\n",
    "        login(token= \n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        return\n",
    "\n",
    "    device = setup_environment()\n",
    "\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/bert_binary_classification_model\")\n",
    "    model, tokenizer = load_model(model_dir, device)\n",
    "\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Data file not found: {data_path}\")\n",
    "        return\n",
    "\n",
    "    new_emails_df = pd.read_excel(data_path)\n",
    "\n",
    "    required_columns = {'Sender', 'Subject','Email_ID' 'Email'}\n",
    "    if not required_columns.issubset(new_emails_df.columns):\n",
    "        raise ValueError(f\"Dataset must contain the following columns: {required_columns}\")\n",
    "\n",
    "    predictions_df = predict_email(model, tokenizer, new_emails_df, device)\n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/lama7b_predictions\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    predictions_df.to_csv(os.path.join(output_dir, \"bert_predictions.csv\"), index=False)\n",
    "    \n",
    "    print(\"Predictions saved successfully!\")\n",
    "    print(predictions_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4add1baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Ham emails: 100\n",
      "Number of Phishing emails: 141\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file = pd.read_csv(\"lama7b_predictions/bert_predictions.csv\")\n",
    "\n",
    "pred_counts = file[\"prediction\"].value_counts()\n",
    "\n",
    "\n",
    "print(f\"Number of Ham emails: {pred_counts.get(0, 0)}\")\n",
    "print(f\"Number of Phishing emails: {pred_counts.get(1, 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99899f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv(\"lama7b_predictions/bert_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6750951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sender</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Email</th>\n",
       "      <th>Email_type</th>\n",
       "      <th>prediction</th>\n",
       "      <th>confidence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>noreply@powerballs.com</td>\n",
       "      <td>You Have Won!</td>\n",
       "      <td>&lt;p&gt;*********PLEASE DO NOT RESPOND TO THIS EMAI...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.729289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>noreply@paypalceo.com</td>\n",
       "      <td>PayPal Breach</td>\n",
       "      <td>&lt;p&gt;********* RESPONES TO THIS EMAIL WILL NOT B...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.576073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>support@credit.chase.com</td>\n",
       "      <td>URGENT: Fraudulent activity detected</td>\n",
       "      <td>&lt;p&gt;Hello,&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;We are writ...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.680934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mary@yahoo.com</td>\n",
       "      <td>Donations needed for Mark</td>\n",
       "      <td>&lt;p&gt;Hello,&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;I&amp;#39;m contactin...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>0</td>\n",
       "      <td>0.640179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>support@security.amazon.com</td>\n",
       "      <td>Your Amazon Account</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;The account number associated with ...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.886676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>account@micrsoft.com</td>\n",
       "      <td>RE: Help Desk</td>\n",
       "      <td>&lt;p&gt;Dear user,&lt;/p&gt;&lt;p&gt;Regarding your account, we...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.906669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>safety@privacy.chase.com</td>\n",
       "      <td>Update for you Account</td>\n",
       "      <td>&lt;p&gt;Dear Valued Chase Memeber:&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbs...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.902423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>customer@fed.ex.com</td>\n",
       "      <td>RE: FED EX TRACKING NUMBER</td>\n",
       "      <td>&lt;p&gt;Dear user,&lt;/p&gt;&lt;p&gt;Unfortunately, we missed y...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.985623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jeremyp@gmail.com</td>\n",
       "      <td>Summer Internship Application</td>\n",
       "      <td>&lt;p&gt;To Whom It May Concern:&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>0</td>\n",
       "      <td>0.990661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>offer@coupons.walmart.com</td>\n",
       "      <td>Walmart Reward Coupons</td>\n",
       "      <td>&lt;p&gt;Account No: 108-455294-800125-MN&lt;/p&gt;&lt;p&gt;&amp;nbs...</td>\n",
       "      <td>Phishing</td>\n",
       "      <td>0</td>\n",
       "      <td>0.784708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Sender                               Subject  \\\n",
       "0       noreply@powerballs.com                         You Have Won!   \n",
       "1        noreply@paypalceo.com                         PayPal Breach   \n",
       "2     support@credit.chase.com  URGENT: Fraudulent activity detected   \n",
       "3               mary@yahoo.com             Donations needed for Mark   \n",
       "4  support@security.amazon.com                  Your Amazon Account    \n",
       "5         account@micrsoft.com                         RE: Help Desk   \n",
       "6     safety@privacy.chase.com                Update for you Account   \n",
       "7          customer@fed.ex.com            RE: FED EX TRACKING NUMBER   \n",
       "8            jeremyp@gmail.com         Summer Internship Application   \n",
       "9    offer@coupons.walmart.com                Walmart Reward Coupons   \n",
       "\n",
       "                                               Email Email_type  prediction  \\\n",
       "0  <p>*********PLEASE DO NOT RESPOND TO THIS EMAI...   Phishing           1   \n",
       "1  <p>********* RESPONES TO THIS EMAIL WILL NOT B...   Phishing           1   \n",
       "2  <p>Hello,&nbsp;</p><p>&nbsp;</p><p>We are writ...   Phishing           1   \n",
       "3  <p>Hello,</p><p>&nbsp;</p><p>I&#39;m contactin...   Phishing           0   \n",
       "4  <p><strong>The account number associated with ...   Phishing           1   \n",
       "5  <p>Dear user,</p><p>Regarding your account, we...   Phishing           1   \n",
       "6  <p>Dear Valued Chase Memeber:&nbsp;</p><p>&nbs...   Phishing           1   \n",
       "7  <p>Dear user,</p><p>Unfortunately, we missed y...   Phishing           1   \n",
       "8  <p>To Whom It May Concern:</p><p>&nbsp;</p><p>...   Phishing           0   \n",
       "9  <p>Account No: 108-455294-800125-MN</p><p>&nbs...   Phishing           0   \n",
       "\n",
       "   confidence_score  \n",
       "0          0.729289  \n",
       "1          0.576073  \n",
       "2          0.680934  \n",
       "3          0.640179  \n",
       "4          0.886676  \n",
       "5          0.906669  \n",
       "6          0.902423  \n",
       "7          0.985623  \n",
       "8          0.990661  \n",
       "9          0.784708  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a357ccee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225f242c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9805\n",
      "Training Loss: 0.1227047289093097\n",
      "Validation Accuracy: 0.984\n",
      "Validation Loss: 0.12054307129765304\n",
      "Predictions saved to /home/users/skuikel/Downloads/Tune/FineTune/lsa_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "import traceback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    #text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def main():\n",
    "\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/final_data.csv\")\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "\n",
    "    df['sender'] = df['sender'].astype(str).apply(clean_text)\n",
    "    df['subject'] = df['subject'].astype(str).apply(clean_text)\n",
    "    df['body'] = df['body'].astype(str).apply(clean_text)\n",
    "    df['text'] = \"Sender: \" + df['sender'] + \" Subject: \" + df['subject'] + \" \" + df['body']\n",
    "    \n",
    "    \n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "    \n",
    "  \n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=10000)),\n",
    "        ('lsa', TruncatedSVD(n_components=300, random_state=42)),\n",
    "        ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    pipeline.fit(train_df['text'], train_df['label'])\n",
    "    \n",
    "  \n",
    "    train_preds = pipeline.predict(train_df['text'])\n",
    "    train_proba = pipeline.predict_proba(train_df['text'])\n",
    "    train_accuracy = accuracy_score(train_df['label'], train_preds)\n",
    "    train_loss = log_loss(train_df['label'], train_proba)\n",
    " \n",
    "    val_preds = pipeline.predict(val_df['text'])\n",
    "    val_proba = pipeline.predict_proba(val_df['text'])\n",
    "    val_accuracy = accuracy_score(val_df['label'], val_preds)\n",
    "    val_loss = log_loss(val_df['label'], val_proba)\n",
    "    \n",
    "    print(\"Training Accuracy:\", train_accuracy)\n",
    "    print(\"Training Loss:\", train_loss)\n",
    "    print(\"Validation Accuracy:\", val_accuracy)\n",
    "    print(\"Validation Loss:\", val_loss)\n",
    "    \n",
    "   \n",
    "    df['prediction'] = pipeline.predict(df['text'])\n",
    "    df['prediction_probability'] = pipeline.predict_proba(df['text']).max(axis=1)\n",
    "    \n",
    "   \n",
    "    output_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/lsa_predictions.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7433a3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "Loading model from /home/users/skuikel/Downloads/Tune/FineTune/llama_7b_dpo123_classification_model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5681ef23914c52afa86d8a6e60d90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Predictions saved successfully!\n",
      "                        Sender                               Subject  \\\n",
      "0       noreply@powerballs.com                         You Have Won!   \n",
      "1        noreply@paypalceo.com                         PayPal Breach   \n",
      "2     support@credit.chase.com  URGENT: Fraudulent activity detected   \n",
      "3               mary@yahoo.com             Donations needed for Mark   \n",
      "4  support@security.amazon.com                  Your Amazon Account    \n",
      "\n",
      "                                               Email Email_type  prediction  \\\n",
      "0  <p>*********PLEASE DO NOT RESPOND TO THIS EMAI...   Phishing           1   \n",
      "1  <p>********* RESPONES TO THIS EMAIL WILL NOT B...   Phishing           1   \n",
      "2  <p>Hello,&nbsp;</p><p>&nbsp;</p><p>We are writ...   Phishing           1   \n",
      "3  <p>Hello,</p><p>&nbsp;</p><p>I&#39;m contactin...   Phishing           1   \n",
      "4  <p><strong>The account number associated with ...   Phishing           1   \n",
      "\n",
      "   confidence_score  \n",
      "0          0.918008  \n",
      "1          0.945985  \n",
      "2          0.940728  \n",
      "3          0.951987  \n",
      "4          0.956017  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Set up GPU/CPU environment and optimize CUDA settings.\"\"\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'  # Set GPU device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    return device\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text data.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()  # Convert to lowercase and remove leading/trailing spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    return text\n",
    "\n",
    "def load_model(model_dir, device):\n",
    "    \"\"\"Load fine-tuned LLaMA model and tokenizer from local directory.\"\"\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        raise FileNotFoundError(f\"Trained model directory not found: {model_dir}\")\n",
    "\n",
    "    print(f\"Loading model from {model_dir}...\")\n",
    "    model = LlamaForSequenceClassification.from_pretrained(model_dir).to(device)\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def predict_email(model, tokenizer, email_df, device, max_length=512):\n",
    "    \"\"\"Predict whether an email is Ham (0) or Phishing (1) with confidence scores.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    confidence_scores = []\n",
    "\n",
    "    for _, email in email_df.iterrows():\n",
    "        input_text = f\"Sender: {clean_text(email['Sender'])} [SEP] Subject: {clean_text(email['Subject'])} [SEP] {clean_text(email['Email'])}\"\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)  \n",
    "        confidence, pred_label = torch.max(probs, dim=-1)\n",
    "\n",
    "        predictions.append(pred_label.item())\n",
    "        confidence_scores.append(confidence.item())\n",
    "\n",
    "    email_df['prediction'] = predictions  \n",
    "    email_df['confidence_score'] = confidence_scores\n",
    "\n",
    "    \n",
    "    required_columns = ['Sender', 'Subject', 'Email']\n",
    "    if 'Email_type' in email_df.columns:\n",
    "        required_columns.append('Email_type')\n",
    "\n",
    "    final_df = email_df[required_columns + ['prediction', 'confidence_score']]\n",
    "    return final_df\n",
    "\n",
    "def main():\n",
    "    \n",
    "    try:\n",
    "        login(token=\n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        return\n",
    "\n",
    "    device = setup_environment()\n",
    "\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/llama_7b_dpo123_classification_model\")\n",
    "    model, tokenizer = load_model(model_dir, device)\n",
    "\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Data file not found: {data_path}\")\n",
    "        return\n",
    "\n",
    "    new_emails_df = pd.read_excel(data_path)\n",
    "\n",
    "    required_columns = {'Sender', 'Subject', 'Email'}\n",
    "    if not required_columns.issubset(new_emails_df.columns):\n",
    "        raise ValueError(f\"Dataset must contain the following columns: {required_columns}\")\n",
    "\n",
    "    predictions_df = predict_email(model, tokenizer, new_emails_df, device)\n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/lama7b_predictions\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    predictions_df.to_csv(os.path.join(output_dir, \"lama7b_predictions_dpo.csv\"), index=False)\n",
    "    \n",
    "    print(\"Predictions saved successfully!\")\n",
    "    print(predictions_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c56c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "Loading model from /home/users/skuikel/Downloads/Tune/FineTune/llama_8b_dpo123_classification_model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef742ff856a945ca9a59ce61de3ac1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Predictions saved successfully!\n",
      "                        Sender                               Subject  \\\n",
      "0       noreply@powerballs.com                         You Have Won!   \n",
      "1        noreply@paypalceo.com                         PayPal Breach   \n",
      "2     support@credit.chase.com  URGENT: Fraudulent activity detected   \n",
      "3               mary@yahoo.com             Donations needed for Mark   \n",
      "4  support@security.amazon.com                  Your Amazon Account    \n",
      "\n",
      "                                               Email Email_type  prediction  \\\n",
      "0  <p>*********PLEASE DO NOT RESPOND TO THIS EMAI...   Phishing           1   \n",
      "1  <p>********* RESPONES TO THIS EMAIL WILL NOT B...   Phishing           1   \n",
      "2  <p>Hello,&nbsp;</p><p>&nbsp;</p><p>We are writ...   Phishing           1   \n",
      "3  <p>Hello,</p><p>&nbsp;</p><p>I&#39;m contactin...   Phishing           1   \n",
      "4  <p><strong>The account number associated with ...   Phishing           1   \n",
      "\n",
      "   confidence_score  \n",
      "0          0.990923  \n",
      "1          0.990719  \n",
      "2          0.951557  \n",
      "3          0.892504  \n",
      "4          0.840207  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text data.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()  \n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    return text\n",
    "\n",
    "def load_model(model_dir, device):\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        raise FileNotFoundError(f\"Trained model directory not found: {model_dir}\")\n",
    "\n",
    "    print(f\"Loading model from {model_dir}...\")\n",
    "    model = LlamaForSequenceClassification.from_pretrained(model_dir).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def predict_email(model, tokenizer, email_df, device, max_length=512):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    confidence_scores = []\n",
    "\n",
    "    for _, email in email_df.iterrows():\n",
    "        input_text = f\"Sender: {clean_text(email['Sender'])} [SEP] Subject: {clean_text(email['Subject'])} [SEP] {clean_text(email['Email'])}\"\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)  \n",
    "        confidence, pred_label = torch.max(probs, dim=-1)\n",
    "\n",
    "        predictions.append(pred_label.item())\n",
    "        confidence_scores.append(confidence.item())\n",
    "\n",
    "    email_df['prediction'] = predictions  \n",
    "    email_df['confidence_score'] = confidence_scores\n",
    "\n",
    "    # Ensure required columns are included\n",
    "    required_columns = ['Sender', 'Subject', 'Email']\n",
    "    if 'Email_type' in email_df.columns:\n",
    "        required_columns.append('Email_type')\n",
    "\n",
    "    final_df = email_df[required_columns + ['prediction', 'confidence_score']]\n",
    "    return final_df\n",
    "\n",
    "def main():\n",
    "    \n",
    "    try:\n",
    "        login(token= \n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        return\n",
    "\n",
    "    device = setup_environment()\n",
    "\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/llama_8b_dpo123_classification_model\")\n",
    "    model, tokenizer = load_model(model_dir, device)\n",
    "\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Data file not found: {data_path}\")\n",
    "        return\n",
    "\n",
    "    new_emails_df = pd.read_excel(data_path)\n",
    "\n",
    "    required_columns = {'Sender', 'Subject', 'Email'}\n",
    "    if not required_columns.issubset(new_emails_df.columns):\n",
    "        raise ValueError(f\"Dataset must contain the following columns: {required_columns}\")\n",
    "\n",
    "    predictions_df = predict_email(model, tokenizer, new_emails_df, device)\n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/lama7b_predictions\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    predictions_df.to_csv(os.path.join(output_dir, \"lama8b_predictions_dpo.csv\"), index=False)\n",
    "    \n",
    "    print(\"Predictions saved successfully!\")\n",
    "    print(predictions_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "927006ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "Loading model from /home/users/skuikel/Downloads/Tune/FineTune/wizard_7b_dpo_classification_model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5558658bee9741b6876198125434e7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at dreamgen/WizardLM-2-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Predictions saved successfully!\n",
      "                        Sender                               Subject  \\\n",
      "0       noreply@powerballs.com                         You Have Won!   \n",
      "1        noreply@paypalceo.com                         PayPal Breach   \n",
      "2     support@credit.chase.com  URGENT: Fraudulent activity detected   \n",
      "3               mary@yahoo.com             Donations needed for Mark   \n",
      "4  support@security.amazon.com                  Your Amazon Account    \n",
      "\n",
      "                                               Email Email_type  prediction  \\\n",
      "0  <p>*********PLEASE DO NOT RESPOND TO THIS EMAI...   Phishing           1   \n",
      "1  <p>********* RESPONES TO THIS EMAIL WILL NOT B...   Phishing           1   \n",
      "2  <p>Hello,&nbsp;</p><p>&nbsp;</p><p>We are writ...   Phishing           1   \n",
      "3  <p>Hello,</p><p>&nbsp;</p><p>I&#39;m contactin...   Phishing           1   \n",
      "4  <p><strong>The account number associated with ...   Phishing           1   \n",
      "\n",
      "   confidence_score  \n",
      "0          0.875942  \n",
      "1          0.964653  \n",
      "2          0.872883  \n",
      "3          0.828756  \n",
      "4          0.867542  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Set up GPU/CPU environment and optimize CUDA settings.\"\"\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'  \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    return device\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text data.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()  \n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "    return text\n",
    "\n",
    "def load_model(model_dir, device):\n",
    "    \"\"\"Load fine-tuned LLaMA model and tokenizer from local directory.\"\"\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        raise FileNotFoundError(f\"Trained model directory not found: {model_dir}\")\n",
    "\n",
    "    print(f\"Loading model from {model_dir}...\")\n",
    "    model = LlamaForSequenceClassification.from_pretrained(model_dir).to(device)\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def predict_email(model, tokenizer, email_df, device, max_length=512):\n",
    "    \"\"\"Predict whether an email is Ham (0) or Phishing (1) with confidence scores.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    confidence_scores = []\n",
    "\n",
    "    for _, email in email_df.iterrows():\n",
    "        input_text = f\"Sender: {clean_text(email['Sender'])} [SEP] Subject: {clean_text(email['Subject'])} [SEP] {clean_text(email['Email'])}\"\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)  \n",
    "        confidence, pred_label = torch.max(probs, dim=-1)\n",
    "\n",
    "        predictions.append(pred_label.item())\n",
    "        confidence_scores.append(confidence.item())\n",
    "\n",
    "    email_df['prediction'] = predictions  \n",
    "    email_df['confidence_score'] = confidence_scores\n",
    "\n",
    "    \n",
    "    required_columns = ['Sender', 'Subject', 'Email']\n",
    "    if 'Email_type' in email_df.columns:\n",
    "        required_columns.append('Email_type')\n",
    "\n",
    "    final_df = email_df[required_columns + ['prediction', 'confidence_score']]\n",
    "    return final_df\n",
    "\n",
    "def main():\n",
    "    \n",
    "    try:\n",
    "        login(token=\n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        return\n",
    "\n",
    "    device = setup_environment()\n",
    "\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/wizard_7b_dpo_classification_model\")\n",
    "    model, tokenizer = load_model(model_dir, device)\n",
    "\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Data file not found: {data_path}\")\n",
    "        return\n",
    "\n",
    "    new_emails_df = pd.read_excel(data_path)\n",
    "\n",
    "    required_columns = {'Sender', 'Subject', 'Email'}\n",
    "    if not required_columns.issubset(new_emails_df.columns):\n",
    "        raise ValueError(f\"Dataset must contain the following columns: {required_columns}\")\n",
    "\n",
    "    predictions_df = predict_email(model, tokenizer, new_emails_df, device)\n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/lama7b_predictions\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    predictions_df.to_csv(os.path.join(output_dir, \"wizard7b_predictions_dpo.csv\"), index=False)\n",
    "    \n",
    "    print(\"Predictions saved successfully!\")\n",
    "    print(predictions_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d88e558d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "Loading model from /home/users/skuikel/Downloads/Tune/FineTune/bert_dpo123_classification_model...\n",
      "Model loaded successfully!\n",
      "Predictions saved successfully!\n",
      "                        Sender                               Subject  \\\n",
      "0       noreply@powerballs.com                         You Have Won!   \n",
      "1        noreply@paypalceo.com                         PayPal Breach   \n",
      "2     support@credit.chase.com  URGENT: Fraudulent activity detected   \n",
      "3               mary@yahoo.com             Donations needed for Mark   \n",
      "4  support@security.amazon.com                  Your Amazon Account    \n",
      "\n",
      "                                               Email Email_type  prediction  \\\n",
      "0  <p>*********PLEASE DO NOT RESPOND TO THIS EMAI...   Phishing           0   \n",
      "1  <p>********* RESPONES TO THIS EMAIL WILL NOT B...   Phishing           0   \n",
      "2  <p>Hello,&nbsp;</p><p>&nbsp;</p><p>We are writ...   Phishing           1   \n",
      "3  <p>Hello,</p><p>&nbsp;</p><p>I&#39;m contactin...   Phishing           0   \n",
      "4  <p><strong>The account number associated with ...   Phishing           0   \n",
      "\n",
      "   confidence_score  \n",
      "0          0.700753  \n",
      "1          0.594131  \n",
      "2          0.511362  \n",
      "3          0.575096  \n",
      "4          0.587970  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text data.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()  \n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    return text\n",
    "\n",
    "def load_model(model_dir, device):\n",
    "    \"\"\"Load fine-tuned BERT model and tokenizer from local directory.\"\"\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        raise FileNotFoundError(f\"Trained model directory not found: {model_dir}\")\n",
    "\n",
    "    print(f\"Loading model from {model_dir}...\")\n",
    "    model = BertForSequenceClassification.from_pretrained(model_dir).to(device)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "    \n",
    "  \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def predict_email(model, tokenizer, email_df, device, max_length=512):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    confidence_scores = []\n",
    "\n",
    "    for _, email in email_df.iterrows():\n",
    "        input_text = f\"Sender: {clean_text(email['Sender'])} [SEP] Subject: {clean_text(email['Subject'])} [SEP] {clean_text(email['Email'])}\"\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)  \n",
    "        confidence, pred_label = torch.max(probs, dim=-1)\n",
    "\n",
    "        predictions.append(pred_label.item())\n",
    "        confidence_scores.append(confidence.item())\n",
    "\n",
    "    email_df['prediction'] = predictions  \n",
    "    email_df['confidence_score'] = confidence_scores\n",
    "\n",
    "   \n",
    "    required_columns = ['Sender', 'Subject', 'Email']\n",
    "    if 'Email_type' in email_df.columns:\n",
    "        required_columns.append('Email_type')\n",
    "\n",
    "    final_df = email_df[required_columns + ['prediction', 'confidence_score']]\n",
    "    return final_df\n",
    "\n",
    "def main():\n",
    "    \n",
    "    try:\n",
    "        login(token= \n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        return\n",
    "\n",
    "    device = setup_environment()\n",
    "\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/bert_dpo123_classification_model\")\n",
    "    model, tokenizer = load_model(model_dir, device)\n",
    "\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Data file not found: {data_path}\")\n",
    "        return\n",
    "\n",
    "    new_emails_df = pd.read_excel(data_path)\n",
    "\n",
    "    required_columns = {'Sender', 'Subject', 'Email_ID', 'Email'}\n",
    "\n",
    "    if not required_columns.issubset(new_emails_df.columns):\n",
    "        raise ValueError(f\"Dataset must contain the following columns: {required_columns}\")\n",
    "\n",
    "    predictions_df = predict_email(model, tokenizer, new_emails_df, device)\n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/lama7b_predictions\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    predictions_df.to_csv(os.path.join(output_dir, \"bert_predictions_dpo.csv\"), index=False)\n",
    "    \n",
    "    print(\"Predictions saved successfully!\")\n",
    "    print(predictions_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
