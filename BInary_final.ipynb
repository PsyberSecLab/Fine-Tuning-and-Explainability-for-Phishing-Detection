{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dbdd543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n",
      "GPU Memory: 23.68 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d361de8f58ff4fbfbabf2a179f989dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 1.6707\n",
      "Epoch 1, Step 10: Loss = 1.1563\n",
      "Epoch 1, Step 20: Loss = 3.1397\n",
      "Epoch 1, Step 30: Loss = 1.0593\n",
      "Epoch 1, Step 40: Loss = 2.2507\n",
      "Epoch 1, Step 50: Loss = 1.3200\n",
      "Epoch 1, Step 60: Loss = 0.8110\n",
      "Epoch 1, Step 70: Loss = 3.3866\n",
      "Epoch 1, Step 80: Loss = 1.5304\n",
      "Epoch 1, Step 90: Loss = 3.1797\n",
      "Epoch 1, Step 100: Loss = 1.3592\n",
      "Epoch 1, Step 110: Loss = 1.3921\n",
      "Epoch 1, Step 120: Loss = 1.3436\n",
      "Epoch 1, Step 130: Loss = 0.3368\n",
      "Epoch 1, Step 140: Loss = 1.7129\n",
      "Epoch 1, Step 150: Loss = 0.8212\n",
      "Epoch 1, Step 160: Loss = 2.4932\n",
      "Epoch 1, Step 170: Loss = 2.8405\n",
      "Epoch 1, Step 180: Loss = 0.8025\n",
      "Epoch 1, Step 190: Loss = 2.3386\n",
      "Epoch 1, Step 200: Loss = 2.2928\n",
      "Epoch 1, Step 210: Loss = 1.2434\n",
      "Epoch 1, Step 220: Loss = 1.9094\n",
      "Epoch 1, Step 230: Loss = 2.4838\n",
      "Epoch 1, Step 240: Loss = 0.4476\n",
      "Epoch 1, Step 250: Loss = 2.3106\n",
      "Epoch 1, Step 260: Loss = 0.9868\n",
      "Epoch 1, Step 270: Loss = 2.3863\n",
      "Epoch 1, Step 280: Loss = 0.8535\n",
      "Epoch 1, Step 290: Loss = 2.2728\n",
      "Epoch 1, Step 300: Loss = 2.6004\n",
      "Epoch 1, Step 310: Loss = 2.0321\n",
      "Epoch 1, Step 320: Loss = 2.2255\n",
      "Epoch 1, Step 330: Loss = 2.6731\n",
      "Epoch 1, Step 340: Loss = 1.9761\n",
      "Epoch 1, Step 350: Loss = 0.6952\n",
      "Epoch 1, Step 360: Loss = 3.1384\n",
      "Epoch 1, Step 370: Loss = 2.8944\n",
      "Epoch 1, Step 380: Loss = 1.0389\n",
      "Epoch 1, Step 390: Loss = 0.6441\n",
      "Epoch 1, Step 400: Loss = 0.9535\n",
      "Epoch 1, Step 410: Loss = 1.6018\n",
      "Epoch 1, Step 420: Loss = 1.2791\n",
      "Epoch 1, Step 430: Loss = 1.3132\n",
      "Epoch 1, Step 440: Loss = 1.3291\n",
      "Epoch 1, Step 450: Loss = 3.4351\n",
      "Epoch 1, Step 460: Loss = 1.6491\n",
      "Epoch 1, Step 470: Loss = 0.4860\n",
      "Epoch 1, Step 480: Loss = 1.2679\n",
      "Epoch 1, Step 490: Loss = 1.1863\n",
      "Epoch 1, Step 500: Loss = 0.8205\n",
      "Epoch 1, Step 510: Loss = 1.4695\n",
      "Epoch 1, Step 520: Loss = 0.6273\n",
      "Epoch 1, Step 530: Loss = 1.4037\n",
      "Epoch 1, Step 540: Loss = 0.8089\n",
      "Epoch 1, Step 550: Loss = 0.4668\n",
      "Epoch 1, Step 560: Loss = 1.4260\n",
      "Epoch 1, Step 570: Loss = 0.6676\n",
      "Epoch 1, Step 580: Loss = 1.3501\n",
      "Epoch 1, Step 590: Loss = 1.2827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.5615\n",
      "precision: 0.5400\n",
      "recall: 0.8300\n",
      "f1: 0.6543\n",
      "loss: 1.5871\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.5700\n",
      "precision: 0.5500\n",
      "recall: 0.7700\n",
      "f1: 0.6417\n",
      "loss: 1.1153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 0: Loss = 0.0726\n",
      "Epoch 2, Step 10: Loss = 0.2109\n",
      "Epoch 2, Step 20: Loss = 0.1589\n",
      "Epoch 2, Step 30: Loss = 1.8821\n",
      "Epoch 2, Step 40: Loss = 1.2603\n",
      "Epoch 2, Step 50: Loss = 1.0367\n",
      "Epoch 2, Step 60: Loss = 1.4289\n",
      "Epoch 2, Step 70: Loss = 0.9062\n",
      "Epoch 2, Step 80: Loss = 1.5054\n",
      "Epoch 2, Step 90: Loss = 1.0338\n",
      "Epoch 2, Step 100: Loss = 1.1755\n",
      "Epoch 2, Step 110: Loss = 0.9556\n",
      "Epoch 2, Step 120: Loss = 1.4119\n",
      "Epoch 2, Step 130: Loss = 1.2785\n",
      "Epoch 2, Step 140: Loss = 0.4764\n",
      "Epoch 2, Step 150: Loss = 1.5515\n",
      "Epoch 2, Step 160: Loss = 0.9627\n",
      "Epoch 2, Step 170: Loss = 0.4925\n",
      "Epoch 2, Step 180: Loss = 0.6196\n",
      "Epoch 2, Step 190: Loss = 1.1876\n",
      "Epoch 2, Step 200: Loss = 0.7838\n",
      "Epoch 2, Step 210: Loss = 0.2412\n",
      "Epoch 2, Step 220: Loss = 0.5748\n",
      "Epoch 2, Step 230: Loss = 0.1831\n",
      "Epoch 2, Step 240: Loss = 0.2321\n",
      "Epoch 2, Step 250: Loss = 0.9516\n",
      "Epoch 2, Step 260: Loss = 0.9907\n",
      "Epoch 2, Step 270: Loss = 1.1221\n",
      "Epoch 2, Step 280: Loss = 0.5315\n",
      "Epoch 2, Step 290: Loss = 0.9457\n",
      "Epoch 2, Step 300: Loss = 0.9887\n",
      "Epoch 2, Step 310: Loss = 1.3395\n",
      "Epoch 2, Step 320: Loss = 0.3794\n",
      "Epoch 2, Step 330: Loss = 1.2488\n",
      "Epoch 2, Step 340: Loss = 1.0133\n",
      "Epoch 2, Step 350: Loss = 0.3881\n",
      "Epoch 2, Step 360: Loss = 1.2347\n",
      "Epoch 2, Step 370: Loss = 1.1398\n",
      "Epoch 2, Step 380: Loss = 1.1759\n",
      "Epoch 2, Step 390: Loss = 1.3915\n",
      "Epoch 2, Step 400: Loss = 0.5890\n",
      "Epoch 2, Step 410: Loss = 1.0457\n",
      "Epoch 2, Step 420: Loss = 0.4346\n",
      "Epoch 2, Step 430: Loss = 0.4432\n",
      "Epoch 2, Step 440: Loss = 0.8155\n",
      "Epoch 2, Step 450: Loss = 0.9899\n",
      "Epoch 2, Step 460: Loss = 0.5839\n",
      "Epoch 2, Step 470: Loss = 0.7843\n",
      "Epoch 2, Step 480: Loss = 1.4883\n",
      "Epoch 2, Step 490: Loss = 0.8809\n",
      "Epoch 2, Step 500: Loss = 1.1454\n",
      "Epoch 2, Step 510: Loss = 1.2241\n",
      "Epoch 2, Step 520: Loss = 0.4798\n",
      "Epoch 2, Step 530: Loss = 0.4827\n",
      "Epoch 2, Step 540: Loss = 0.9656\n",
      "Epoch 2, Step 550: Loss = 0.5849\n",
      "Epoch 2, Step 560: Loss = 1.1081\n",
      "Epoch 2, Step 570: Loss = 0.2160\n",
      "Epoch 2, Step 580: Loss = 1.3652\n",
      "Epoch 2, Step 590: Loss = 0.2928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.6115\n",
      "precision: 0.5936\n",
      "recall: 0.7067\n",
      "f1: 0.6452\n",
      "loss: 0.9141\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.6625\n",
      "precision: 0.6462\n",
      "recall: 0.7183\n",
      "f1: 0.6803\n",
      "loss: 0.7498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 0: Loss = 0.3411\n",
      "Epoch 3, Step 10: Loss = 0.9979\n",
      "Epoch 3, Step 20: Loss = 0.9834\n",
      "Epoch 3, Step 30: Loss = 1.1315\n",
      "Epoch 3, Step 40: Loss = 1.1075\n",
      "Epoch 3, Step 50: Loss = 0.7082\n",
      "Epoch 3, Step 60: Loss = 0.7770\n",
      "Epoch 3, Step 70: Loss = 1.1067\n",
      "Epoch 3, Step 80: Loss = 0.6393\n",
      "Epoch 3, Step 90: Loss = 1.4462\n",
      "Epoch 3, Step 100: Loss = 0.7740\n",
      "Epoch 3, Step 110: Loss = 1.4950\n",
      "Epoch 3, Step 120: Loss = 0.2545\n",
      "Epoch 3, Step 130: Loss = 0.5613\n",
      "Epoch 3, Step 140: Loss = 0.4366\n",
      "Epoch 3, Step 150: Loss = 0.4676\n",
      "Epoch 3, Step 160: Loss = 0.8820\n",
      "Epoch 3, Step 170: Loss = 0.6953\n",
      "Epoch 3, Step 180: Loss = 1.8080\n",
      "Epoch 3, Step 190: Loss = 1.0954\n",
      "Epoch 3, Step 200: Loss = 0.2043\n",
      "Epoch 3, Step 210: Loss = 0.3592\n",
      "Epoch 3, Step 220: Loss = 0.3342\n",
      "Epoch 3, Step 230: Loss = 0.7810\n",
      "Epoch 3, Step 240: Loss = 0.3942\n",
      "Epoch 3, Step 250: Loss = 0.5145\n",
      "Epoch 3, Step 260: Loss = 0.4310\n",
      "Epoch 3, Step 270: Loss = 1.3418\n",
      "Epoch 3, Step 280: Loss = 1.4158\n",
      "Epoch 3, Step 290: Loss = 0.4292\n",
      "Epoch 3, Step 300: Loss = 0.3994\n",
      "Epoch 3, Step 310: Loss = 0.7315\n",
      "Epoch 3, Step 320: Loss = 0.5860\n",
      "Epoch 3, Step 330: Loss = 0.7485\n",
      "Epoch 3, Step 340: Loss = 0.4207\n",
      "Epoch 3, Step 350: Loss = 0.4410\n",
      "Epoch 3, Step 360: Loss = 0.6345\n",
      "Epoch 3, Step 370: Loss = 0.7233\n",
      "Epoch 3, Step 380: Loss = 1.0086\n",
      "Epoch 3, Step 390: Loss = 0.3104\n",
      "Epoch 3, Step 400: Loss = 0.5451\n",
      "Epoch 3, Step 410: Loss = 0.2226\n",
      "Epoch 3, Step 420: Loss = 0.8796\n",
      "Epoch 3, Step 430: Loss = 1.1064\n",
      "Epoch 3, Step 440: Loss = 0.4772\n",
      "Epoch 3, Step 450: Loss = 0.1646\n",
      "Epoch 3, Step 460: Loss = 0.3135\n",
      "Epoch 3, Step 470: Loss = 0.4079\n",
      "Epoch 3, Step 480: Loss = 0.2416\n",
      "Epoch 3, Step 490: Loss = 1.2641\n",
      "Epoch 3, Step 500: Loss = 0.3791\n",
      "Epoch 3, Step 510: Loss = 0.3854\n",
      "Epoch 3, Step 520: Loss = 0.3703\n",
      "Epoch 3, Step 530: Loss = 0.6418\n",
      "Epoch 3, Step 540: Loss = 0.7252\n",
      "Epoch 3, Step 550: Loss = 0.6018\n",
      "Epoch 3, Step 560: Loss = 0.6228\n",
      "Epoch 3, Step 570: Loss = 2.0549\n",
      "Epoch 3, Step 580: Loss = 0.1198\n",
      "Epoch 3, Step 590: Loss = 0.1134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.6946\n",
      "precision: 0.6837\n",
      "recall: 0.7242\n",
      "f1: 0.7034\n",
      "loss: 0.6917\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7208\n",
      "precision: 0.7011\n",
      "recall: 0.7700\n",
      "f1: 0.7339\n",
      "loss: 0.6187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 0: Loss = 0.1367\n",
      "Epoch 4, Step 10: Loss = 0.3252\n",
      "Epoch 4, Step 20: Loss = 0.7450\n",
      "Epoch 4, Step 30: Loss = 1.2430\n",
      "Epoch 4, Step 40: Loss = 0.2624\n",
      "Epoch 4, Step 50: Loss = 0.5536\n",
      "Epoch 4, Step 60: Loss = 0.2195\n",
      "Epoch 4, Step 70: Loss = 0.2389\n",
      "Epoch 4, Step 80: Loss = 0.2511\n",
      "Epoch 4, Step 90: Loss = 0.9341\n",
      "Epoch 4, Step 100: Loss = 1.2449\n",
      "Epoch 4, Step 110: Loss = 0.7884\n",
      "Epoch 4, Step 120: Loss = 0.4777\n",
      "Epoch 4, Step 130: Loss = 0.3839\n",
      "Epoch 4, Step 140: Loss = 0.9442\n",
      "Epoch 4, Step 150: Loss = 0.7652\n",
      "Epoch 4, Step 160: Loss = 0.8584\n",
      "Epoch 4, Step 170: Loss = 0.3920\n",
      "Epoch 4, Step 180: Loss = 0.8740\n",
      "Epoch 4, Step 190: Loss = 0.8319\n",
      "Epoch 4, Step 200: Loss = 0.2694\n",
      "Epoch 4, Step 210: Loss = 0.7588\n",
      "Epoch 4, Step 220: Loss = 0.4001\n",
      "Epoch 4, Step 230: Loss = 0.8200\n",
      "Epoch 4, Step 240: Loss = 0.2321\n",
      "Epoch 4, Step 250: Loss = 0.6252\n",
      "Epoch 4, Step 260: Loss = 0.3301\n",
      "Epoch 4, Step 270: Loss = 0.3267\n",
      "Epoch 4, Step 280: Loss = 0.5645\n",
      "Epoch 4, Step 290: Loss = 0.5221\n",
      "Epoch 4, Step 300: Loss = 0.3042\n",
      "Epoch 4, Step 310: Loss = 0.4551\n",
      "Epoch 4, Step 320: Loss = 1.0526\n",
      "Epoch 4, Step 330: Loss = 0.2739\n",
      "Epoch 4, Step 340: Loss = 0.4368\n",
      "Epoch 4, Step 350: Loss = 0.3440\n",
      "Epoch 4, Step 360: Loss = 0.3528\n",
      "Epoch 4, Step 370: Loss = 0.7893\n",
      "Epoch 4, Step 380: Loss = 0.2807\n",
      "Epoch 4, Step 390: Loss = 0.0960\n",
      "Epoch 4, Step 400: Loss = 0.7480\n",
      "Epoch 4, Step 410: Loss = 0.1786\n",
      "Epoch 4, Step 420: Loss = 1.5261\n",
      "Epoch 4, Step 430: Loss = 0.7479\n",
      "Epoch 4, Step 440: Loss = 1.0184\n",
      "Epoch 4, Step 450: Loss = 0.3500\n",
      "Epoch 4, Step 460: Loss = 1.0731\n",
      "Epoch 4, Step 470: Loss = 0.1250\n",
      "Epoch 4, Step 480: Loss = 0.2534\n",
      "Epoch 4, Step 490: Loss = 0.5487\n",
      "Epoch 4, Step 500: Loss = 1.9000\n",
      "Epoch 4, Step 510: Loss = 0.3856\n",
      "Epoch 4, Step 520: Loss = 0.7192\n",
      "Epoch 4, Step 530: Loss = 0.4590\n",
      "Epoch 4, Step 540: Loss = 0.3470\n",
      "Epoch 4, Step 550: Loss = 0.2850\n",
      "Epoch 4, Step 560: Loss = 0.3597\n",
      "Epoch 4, Step 570: Loss = 0.3971\n",
      "Epoch 4, Step 580: Loss = 0.4636\n",
      "Epoch 4, Step 590: Loss = 0.4815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7310\n",
      "precision: 0.7214\n",
      "recall: 0.7529\n",
      "f1: 0.7368\n",
      "loss: 0.6051\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7458\n",
      "precision: 0.7294\n",
      "recall: 0.7817\n",
      "f1: 0.7546\n",
      "loss: 0.5679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.6638\n",
      "Epoch 5, Step 10: Loss = 0.7121\n",
      "Epoch 5, Step 20: Loss = 2.0743\n",
      "Epoch 5, Step 30: Loss = 0.4075\n",
      "Epoch 5, Step 40: Loss = 0.2275\n",
      "Epoch 5, Step 50: Loss = 0.5907\n",
      "Epoch 5, Step 60: Loss = 0.4010\n",
      "Epoch 5, Step 70: Loss = 0.6592\n",
      "Epoch 5, Step 80: Loss = 0.4173\n",
      "Epoch 5, Step 90: Loss = 0.7529\n",
      "Epoch 5, Step 100: Loss = 1.1738\n",
      "Epoch 5, Step 110: Loss = 0.0558\n",
      "Epoch 5, Step 120: Loss = 1.6863\n",
      "Epoch 5, Step 130: Loss = 1.0101\n",
      "Epoch 5, Step 140: Loss = 0.4699\n",
      "Epoch 5, Step 150: Loss = 0.8325\n",
      "Epoch 5, Step 160: Loss = 0.7231\n",
      "Epoch 5, Step 170: Loss = 1.0997\n",
      "Epoch 5, Step 180: Loss = 0.6439\n",
      "Epoch 5, Step 190: Loss = 0.3900\n",
      "Epoch 5, Step 200: Loss = 0.3667\n",
      "Epoch 5, Step 210: Loss = 0.1978\n",
      "Epoch 5, Step 220: Loss = 0.3740\n",
      "Epoch 5, Step 230: Loss = 0.8362\n",
      "Epoch 5, Step 240: Loss = 0.4573\n",
      "Epoch 5, Step 250: Loss = 0.1732\n",
      "Epoch 5, Step 260: Loss = 0.1033\n",
      "Epoch 5, Step 270: Loss = 1.1292\n",
      "Epoch 5, Step 280: Loss = 1.0765\n",
      "Epoch 5, Step 290: Loss = 0.6569\n",
      "Epoch 5, Step 300: Loss = 0.6665\n",
      "Epoch 5, Step 310: Loss = 0.1943\n",
      "Epoch 5, Step 320: Loss = 0.4342\n",
      "Epoch 5, Step 330: Loss = 0.5888\n",
      "Epoch 5, Step 340: Loss = 0.3374\n",
      "Epoch 5, Step 350: Loss = 0.1893\n",
      "Epoch 5, Step 360: Loss = 0.5306\n",
      "Epoch 5, Step 370: Loss = 0.3662\n",
      "Epoch 5, Step 380: Loss = 0.5066\n",
      "Epoch 5, Step 390: Loss = 0.3717\n",
      "Epoch 5, Step 400: Loss = 0.7698\n",
      "Epoch 5, Step 410: Loss = 0.5349\n",
      "Epoch 5, Step 420: Loss = 0.5433\n",
      "Epoch 5, Step 430: Loss = 0.4939\n",
      "Epoch 5, Step 440: Loss = 0.4208\n",
      "Epoch 5, Step 450: Loss = 0.3843\n",
      "Epoch 5, Step 460: Loss = 1.3012\n",
      "Epoch 5, Step 470: Loss = 0.4353\n",
      "Epoch 5, Step 480: Loss = 0.8534\n",
      "Epoch 5, Step 490: Loss = 0.5286\n",
      "Epoch 5, Step 500: Loss = 0.2830\n",
      "Epoch 5, Step 510: Loss = 0.6698\n",
      "Epoch 5, Step 520: Loss = 0.4064\n",
      "Epoch 5, Step 530: Loss = 0.1445\n",
      "Epoch 5, Step 540: Loss = 0.4677\n",
      "Epoch 5, Step 550: Loss = 0.7590\n",
      "Epoch 5, Step 560: Loss = 0.6380\n",
      "Epoch 5, Step 570: Loss = 0.7389\n",
      "Epoch 5, Step 580: Loss = 0.2423\n",
      "Epoch 5, Step 590: Loss = 0.5364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7506\n",
      "precision: 0.7397\n",
      "recall: 0.7733\n",
      "f1: 0.7562\n",
      "loss: 0.5677\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7550\n",
      "precision: 0.7347\n",
      "recall: 0.7983\n",
      "f1: 0.7652\n",
      "loss: 0.5460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Step 0: Loss = 0.7407\n",
      "Epoch 6, Step 10: Loss = 0.7848\n",
      "Epoch 6, Step 20: Loss = 0.6443\n",
      "Epoch 6, Step 30: Loss = 0.6357\n",
      "Epoch 6, Step 40: Loss = 1.0533\n",
      "Epoch 6, Step 50: Loss = 0.3318\n",
      "Epoch 6, Step 60: Loss = 0.6540\n",
      "Epoch 6, Step 70: Loss = 0.2913\n",
      "Epoch 6, Step 80: Loss = 0.7108\n",
      "Epoch 6, Step 90: Loss = 1.2605\n",
      "Epoch 6, Step 100: Loss = 0.9451\n",
      "Epoch 6, Step 110: Loss = 0.3485\n",
      "Epoch 6, Step 120: Loss = 0.1465\n",
      "Epoch 6, Step 130: Loss = 0.2862\n",
      "Epoch 6, Step 140: Loss = 0.9014\n",
      "Epoch 6, Step 150: Loss = 0.1590\n",
      "Epoch 6, Step 160: Loss = 0.6082\n",
      "Epoch 6, Step 170: Loss = 0.5092\n",
      "Epoch 6, Step 180: Loss = 0.2405\n",
      "Epoch 6, Step 190: Loss = 0.3369\n",
      "Epoch 6, Step 200: Loss = 0.1340\n",
      "Epoch 6, Step 210: Loss = 0.7112\n",
      "Epoch 6, Step 220: Loss = 0.9717\n",
      "Epoch 6, Step 230: Loss = 0.3481\n",
      "Epoch 6, Step 240: Loss = 0.7195\n",
      "Epoch 6, Step 250: Loss = 0.7212\n",
      "Epoch 6, Step 260: Loss = 0.1951\n",
      "Epoch 6, Step 270: Loss = 0.8287\n",
      "Epoch 6, Step 280: Loss = 0.7093\n",
      "Epoch 6, Step 290: Loss = 0.5658\n",
      "Epoch 6, Step 300: Loss = 0.4571\n",
      "Epoch 6, Step 310: Loss = 0.5552\n",
      "Epoch 6, Step 320: Loss = 0.6458\n",
      "Epoch 6, Step 330: Loss = 0.5582\n",
      "Epoch 6, Step 340: Loss = 0.8304\n",
      "Epoch 6, Step 350: Loss = 0.3574\n",
      "Epoch 6, Step 360: Loss = 0.5120\n",
      "Epoch 6, Step 370: Loss = 0.4137\n",
      "Epoch 6, Step 380: Loss = 0.3306\n",
      "Epoch 6, Step 390: Loss = 0.6210\n",
      "Epoch 6, Step 400: Loss = 0.1235\n",
      "Epoch 6, Step 410: Loss = 0.2471\n",
      "Epoch 6, Step 420: Loss = 0.4561\n",
      "Epoch 6, Step 430: Loss = 0.1282\n",
      "Epoch 6, Step 440: Loss = 0.3284\n",
      "Epoch 6, Step 450: Loss = 0.3005\n",
      "Epoch 6, Step 460: Loss = 0.6687\n",
      "Epoch 6, Step 470: Loss = 0.8095\n",
      "Epoch 6, Step 480: Loss = 0.0872\n",
      "Epoch 6, Step 490: Loss = 0.7859\n",
      "Epoch 6, Step 500: Loss = 0.4922\n",
      "Epoch 6, Step 510: Loss = 0.4730\n",
      "Epoch 6, Step 520: Loss = 0.6813\n",
      "Epoch 6, Step 530: Loss = 0.3441\n",
      "Epoch 6, Step 540: Loss = 0.4352\n",
      "Epoch 6, Step 550: Loss = 0.5155\n",
      "Epoch 6, Step 560: Loss = 0.7433\n",
      "Epoch 6, Step 570: Loss = 0.4465\n",
      "Epoch 6, Step 580: Loss = 0.7923\n",
      "Epoch 6, Step 590: Loss = 0.4210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7590\n",
      "precision: 0.7485\n",
      "recall: 0.7800\n",
      "f1: 0.7639\n",
      "loss: 0.5521\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7608\n",
      "precision: 0.7404\n",
      "recall: 0.8033\n",
      "f1: 0.7706\n",
      "loss: 0.5355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Step 0: Loss = 0.1310\n",
      "Epoch 7, Step 10: Loss = 0.6518\n",
      "Epoch 7, Step 20: Loss = 0.7682\n",
      "Epoch 7, Step 30: Loss = 0.6046\n",
      "Epoch 7, Step 40: Loss = 0.7722\n",
      "Epoch 7, Step 50: Loss = 0.3215\n",
      "Epoch 7, Step 60: Loss = 0.7306\n",
      "Epoch 7, Step 70: Loss = 0.7962\n",
      "Epoch 7, Step 80: Loss = 0.7922\n",
      "Epoch 7, Step 90: Loss = 0.9849\n",
      "Epoch 7, Step 100: Loss = 0.1571\n",
      "Epoch 7, Step 110: Loss = 0.8098\n",
      "Epoch 7, Step 120: Loss = 0.3091\n",
      "Epoch 7, Step 130: Loss = 0.4993\n",
      "Epoch 7, Step 140: Loss = 0.4128\n",
      "Epoch 7, Step 150: Loss = 0.6371\n",
      "Epoch 7, Step 160: Loss = 0.5698\n",
      "Epoch 7, Step 170: Loss = 0.1370\n",
      "Epoch 7, Step 180: Loss = 0.9648\n",
      "Epoch 7, Step 190: Loss = 0.0418\n",
      "Epoch 7, Step 200: Loss = 1.0346\n",
      "Epoch 7, Step 210: Loss = 0.9877\n",
      "Epoch 7, Step 220: Loss = 0.5247\n",
      "Epoch 7, Step 230: Loss = 0.2863\n",
      "Epoch 7, Step 240: Loss = 0.2896\n",
      "Epoch 7, Step 250: Loss = 0.6123\n",
      "Epoch 7, Step 260: Loss = 0.6313\n",
      "Epoch 7, Step 270: Loss = 0.5180\n",
      "Epoch 7, Step 280: Loss = 0.1816\n",
      "Epoch 7, Step 290: Loss = 0.5728\n",
      "Epoch 7, Step 300: Loss = 0.3479\n",
      "Epoch 7, Step 310: Loss = 0.3892\n",
      "Epoch 7, Step 320: Loss = 0.2487\n",
      "Epoch 7, Step 330: Loss = 0.7334\n",
      "Epoch 7, Step 340: Loss = 0.1993\n",
      "Epoch 7, Step 350: Loss = 0.8214\n",
      "Epoch 7, Step 360: Loss = 0.9570\n",
      "Epoch 7, Step 370: Loss = 0.5460\n",
      "Epoch 7, Step 380: Loss = 1.0465\n",
      "Epoch 7, Step 390: Loss = 0.4160\n",
      "Epoch 7, Step 400: Loss = 0.1731\n",
      "Epoch 7, Step 410: Loss = 0.9948\n",
      "Epoch 7, Step 420: Loss = 0.1307\n",
      "Epoch 7, Step 430: Loss = 0.3465\n",
      "Epoch 7, Step 440: Loss = 1.1495\n",
      "Epoch 7, Step 450: Loss = 0.9679\n",
      "Epoch 7, Step 460: Loss = 0.3966\n",
      "Epoch 7, Step 470: Loss = 0.2922\n",
      "Epoch 7, Step 480: Loss = 0.6751\n",
      "Epoch 7, Step 490: Loss = 0.3985\n",
      "Epoch 7, Step 500: Loss = 0.4820\n",
      "Epoch 7, Step 510: Loss = 0.5361\n",
      "Epoch 7, Step 520: Loss = 0.5047\n",
      "Epoch 7, Step 530: Loss = 0.6818\n",
      "Epoch 7, Step 540: Loss = 0.3944\n",
      "Epoch 7, Step 550: Loss = 0.7440\n",
      "Epoch 7, Step 560: Loss = 0.5448\n",
      "Epoch 7, Step 570: Loss = 0.4910\n",
      "Epoch 7, Step 580: Loss = 0.5010\n",
      "Epoch 7, Step 590: Loss = 0.9644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7594\n",
      "precision: 0.7465\n",
      "recall: 0.7854\n",
      "f1: 0.7655\n",
      "loss: 0.5438\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7650\n",
      "precision: 0.7424\n",
      "recall: 0.8117\n",
      "f1: 0.7755\n",
      "loss: 0.5305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Step 0: Loss = 0.2499\n",
      "Epoch 8, Step 10: Loss = 0.2583\n",
      "Epoch 8, Step 20: Loss = 0.3926\n",
      "Epoch 8, Step 30: Loss = 0.8756\n",
      "Epoch 8, Step 40: Loss = 0.2008\n",
      "Epoch 8, Step 50: Loss = 0.2009\n",
      "Epoch 8, Step 60: Loss = 0.9405\n",
      "Epoch 8, Step 70: Loss = 1.1549\n",
      "Epoch 8, Step 80: Loss = 0.6907\n",
      "Epoch 8, Step 90: Loss = 0.8426\n",
      "Epoch 8, Step 100: Loss = 0.6862\n",
      "Epoch 8, Step 110: Loss = 1.0415\n",
      "Epoch 8, Step 120: Loss = 0.1298\n",
      "Epoch 8, Step 130: Loss = 0.3687\n",
      "Epoch 8, Step 140: Loss = 0.6199\n",
      "Epoch 8, Step 150: Loss = 0.6075\n",
      "Epoch 8, Step 160: Loss = 0.2628\n",
      "Epoch 8, Step 170: Loss = 0.2515\n",
      "Epoch 8, Step 180: Loss = 1.1116\n",
      "Epoch 8, Step 190: Loss = 0.7796\n",
      "Epoch 8, Step 200: Loss = 0.6909\n",
      "Epoch 8, Step 210: Loss = 0.4987\n",
      "Epoch 8, Step 220: Loss = 0.4791\n",
      "Epoch 8, Step 230: Loss = 0.5402\n",
      "Epoch 8, Step 240: Loss = 0.5270\n",
      "Epoch 8, Step 250: Loss = 0.8260\n",
      "Epoch 8, Step 260: Loss = 0.8567\n",
      "Epoch 8, Step 270: Loss = 0.4813\n",
      "Epoch 8, Step 280: Loss = 0.3010\n",
      "Epoch 8, Step 290: Loss = 0.5699\n",
      "Epoch 8, Step 300: Loss = 0.7823\n",
      "Epoch 8, Step 310: Loss = 0.2771\n",
      "Epoch 8, Step 320: Loss = 0.8328\n",
      "Epoch 8, Step 330: Loss = 0.5451\n",
      "Epoch 8, Step 340: Loss = 0.3690\n",
      "Epoch 8, Step 350: Loss = 0.9300\n",
      "Epoch 8, Step 360: Loss = 0.6271\n",
      "Epoch 8, Step 370: Loss = 0.6492\n",
      "Epoch 8, Step 380: Loss = 0.7352\n",
      "Epoch 8, Step 390: Loss = 0.4091\n",
      "Epoch 8, Step 400: Loss = 0.4879\n",
      "Epoch 8, Step 410: Loss = 0.8124\n",
      "Epoch 8, Step 420: Loss = 0.7938\n",
      "Epoch 8, Step 430: Loss = 0.4295\n",
      "Epoch 8, Step 440: Loss = 0.2180\n",
      "Epoch 8, Step 450: Loss = 0.6826\n",
      "Epoch 8, Step 460: Loss = 0.6310\n",
      "Epoch 8, Step 470: Loss = 0.1497\n",
      "Epoch 8, Step 480: Loss = 0.1697\n",
      "Epoch 8, Step 490: Loss = 0.3358\n",
      "Epoch 8, Step 500: Loss = 0.3829\n",
      "Epoch 8, Step 510: Loss = 0.7487\n",
      "Epoch 8, Step 520: Loss = 0.7344\n",
      "Epoch 8, Step 530: Loss = 0.7497\n",
      "Epoch 8, Step 540: Loss = 0.2682\n",
      "Epoch 8, Step 550: Loss = 1.0366\n",
      "Epoch 8, Step 560: Loss = 0.4161\n",
      "Epoch 8, Step 570: Loss = 0.4386\n",
      "Epoch 8, Step 580: Loss = 0.3369\n",
      "Epoch 8, Step 590: Loss = 0.6313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7631\n",
      "precision: 0.7497\n",
      "recall: 0.7900\n",
      "f1: 0.7693\n",
      "loss: 0.5396\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7683\n",
      "precision: 0.7439\n",
      "recall: 0.8183\n",
      "f1: 0.7794\n",
      "loss: 0.5283\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        self.emails_df = emails_df.copy()\n",
    "        self.emails_df['sender'] = self.emails_df['sender'].apply(clean_text)\n",
    "        self.emails_df['subject'] = self.emails_df['subject'].apply(clean_text)\n",
    "        self.emails_df['body'] = self.emails_df['body'].apply(clean_text)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        email = self.emails_df.iloc[idx]\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(email['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    #text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    #text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model_config.use_cache = False\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, output_dir):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    #plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "    \n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    return metrics\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=8):\n",
    "    best_val_metrics = {'f1': 0}\n",
    "    best_model_state = None\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % 2 == 0:  # Gradient accumulation steps = 2\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(batch['label'].cpu().numpy())\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "  \n",
    "        train_metrics = compute_metrics(train_preds, train_labels)\n",
    "        train_metrics['loss'] = total_loss / len(train_loader)\n",
    "      \n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "      \n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(\"Training Metrics:\")\n",
    "        for metric, value in train_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        for metric, value in val_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_val_metrics['f1']:\n",
    "            best_val_metrics = val_metrics\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return best_model_state, best_val_metrics\n",
    "\n",
    "def main():\n",
    "    login(token=\"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\")\n",
    "    device = setup_environment()\n",
    "    model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/newdata_cleaned.csv\")\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "    \n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    \n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "    \n",
    "    train_dataset = EmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = EmailDataset(val_df, tokenizer, max_length=512)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 8\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    best_model_state, best_metrics = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/binary_classification_7B\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    #plot_confusion_matrix(best_metrics['confusion_matrix'], output_dir)\n",
    "    \n",
    "  \n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"best_metrics\": {k: float(v) if k != 'confusion_matrix' else v.tolist() \n",
    "                        for k, v in best_metrics.items()}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e66482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n",
      "GPU Memory: 23.68 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd65862ffb0462eb3befa730843083d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 1.0201\n",
      "Epoch 1, Step 10: Loss = 2.0799\n",
      "Epoch 1, Step 20: Loss = 1.9430\n",
      "Epoch 1, Step 30: Loss = 1.0707\n",
      "Epoch 1, Step 40: Loss = 2.6649\n",
      "Epoch 1, Step 50: Loss = 1.5183\n",
      "Epoch 1, Step 60: Loss = 0.7090\n",
      "Epoch 1, Step 70: Loss = 1.5923\n",
      "Epoch 1, Step 80: Loss = 0.7225\n",
      "Epoch 1, Step 90: Loss = 0.4653\n",
      "Epoch 1, Step 100: Loss = 0.8534\n",
      "Epoch 1, Step 110: Loss = 0.7550\n",
      "Epoch 1, Step 120: Loss = 1.0739\n",
      "Epoch 1, Step 130: Loss = 1.2751\n",
      "Epoch 1, Step 140: Loss = 1.4228\n",
      "Epoch 1, Step 150: Loss = 0.9395\n",
      "Epoch 1, Step 160: Loss = 2.1517\n",
      "Epoch 1, Step 170: Loss = 0.8936\n",
      "Epoch 1, Step 180: Loss = 1.7863\n",
      "Epoch 1, Step 190: Loss = 1.6126\n",
      "Epoch 1, Step 200: Loss = 1.9987\n",
      "Epoch 1, Step 210: Loss = 1.5510\n",
      "Epoch 1, Step 220: Loss = 0.4270\n",
      "Epoch 1, Step 230: Loss = 1.2550\n",
      "Epoch 1, Step 240: Loss = 1.0416\n",
      "Epoch 1, Step 250: Loss = 0.9989\n",
      "Epoch 1, Step 260: Loss = 0.8839\n",
      "Epoch 1, Step 270: Loss = 1.5619\n",
      "Epoch 1, Step 280: Loss = 2.2744\n",
      "Epoch 1, Step 290: Loss = 0.7689\n",
      "Epoch 1, Step 300: Loss = 1.1412\n",
      "Epoch 1, Step 310: Loss = 0.5238\n",
      "Epoch 1, Step 320: Loss = 0.7966\n",
      "Epoch 1, Step 330: Loss = 1.8631\n",
      "Epoch 1, Step 340: Loss = 1.8492\n",
      "Epoch 1, Step 350: Loss = 0.6432\n",
      "Epoch 1, Step 360: Loss = 0.9267\n",
      "Epoch 1, Step 370: Loss = 0.8888\n",
      "Epoch 1, Step 380: Loss = 0.8905\n",
      "Epoch 1, Step 390: Loss = 1.6513\n",
      "Epoch 1, Step 400: Loss = 1.9777\n",
      "Epoch 1, Step 410: Loss = 0.6197\n",
      "Epoch 1, Step 420: Loss = 1.3146\n",
      "Epoch 1, Step 430: Loss = 1.2614\n",
      "Epoch 1, Step 440: Loss = 0.9521\n",
      "Epoch 1, Step 450: Loss = 0.5458\n",
      "Epoch 1, Step 460: Loss = 0.6064\n",
      "Epoch 1, Step 470: Loss = 1.4041\n",
      "Epoch 1, Step 480: Loss = 0.3804\n",
      "Epoch 1, Step 490: Loss = 0.7011\n",
      "Epoch 1, Step 500: Loss = 0.8887\n",
      "Epoch 1, Step 510: Loss = 0.6253\n",
      "Epoch 1, Step 520: Loss = 0.5639\n",
      "Epoch 1, Step 530: Loss = 0.4027\n",
      "Epoch 1, Step 540: Loss = 1.1511\n",
      "Epoch 1, Step 550: Loss = 0.9835\n",
      "Epoch 1, Step 560: Loss = 0.5931\n",
      "Epoch 1, Step 570: Loss = 1.0741\n",
      "Epoch 1, Step 580: Loss = 2.4127\n",
      "Epoch 1, Step 590: Loss = 1.7940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.5671\n",
      "precision: 0.5574\n",
      "recall: 0.6517\n",
      "f1: 0.6008\n",
      "loss: 1.1971\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.6283\n",
      "precision: 0.6258\n",
      "recall: 0.6383\n",
      "f1: 0.6320\n",
      "loss: 0.8962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 0: Loss = 1.2067\n",
      "Epoch 2, Step 10: Loss = 1.4690\n",
      "Epoch 2, Step 20: Loss = 1.3225\n",
      "Epoch 2, Step 30: Loss = 1.2704\n",
      "Epoch 2, Step 40: Loss = 0.7351\n",
      "Epoch 2, Step 50: Loss = 1.2196\n",
      "Epoch 2, Step 60: Loss = 0.3089\n",
      "Epoch 2, Step 70: Loss = 1.4756\n",
      "Epoch 2, Step 80: Loss = 0.6224\n",
      "Epoch 2, Step 90: Loss = 0.5824\n",
      "Epoch 2, Step 100: Loss = 1.6997\n",
      "Epoch 2, Step 110: Loss = 0.6794\n",
      "Epoch 2, Step 120: Loss = 0.4628\n",
      "Epoch 2, Step 130: Loss = 1.1423\n",
      "Epoch 2, Step 140: Loss = 0.9324\n",
      "Epoch 2, Step 150: Loss = 0.8387\n",
      "Epoch 2, Step 160: Loss = 1.3702\n",
      "Epoch 2, Step 170: Loss = 1.0165\n",
      "Epoch 2, Step 180: Loss = 0.7973\n",
      "Epoch 2, Step 190: Loss = 0.3767\n",
      "Epoch 2, Step 200: Loss = 0.5214\n",
      "Epoch 2, Step 210: Loss = 0.6168\n",
      "Epoch 2, Step 220: Loss = 0.6362\n",
      "Epoch 2, Step 230: Loss = 0.4896\n",
      "Epoch 2, Step 240: Loss = 0.4474\n",
      "Epoch 2, Step 250: Loss = 0.5449\n",
      "Epoch 2, Step 260: Loss = 0.9650\n",
      "Epoch 2, Step 270: Loss = 0.5175\n",
      "Epoch 2, Step 280: Loss = 0.4356\n",
      "Epoch 2, Step 290: Loss = 0.2591\n",
      "Epoch 2, Step 300: Loss = 0.3380\n",
      "Epoch 2, Step 310: Loss = 0.5903\n",
      "Epoch 2, Step 320: Loss = 0.3942\n",
      "Epoch 2, Step 330: Loss = 0.1970\n",
      "Epoch 2, Step 340: Loss = 1.0968\n",
      "Epoch 2, Step 350: Loss = 0.8868\n",
      "Epoch 2, Step 360: Loss = 0.5793\n",
      "Epoch 2, Step 370: Loss = 1.0958\n",
      "Epoch 2, Step 380: Loss = 0.9031\n",
      "Epoch 2, Step 390: Loss = 0.2305\n",
      "Epoch 2, Step 400: Loss = 0.7727\n",
      "Epoch 2, Step 410: Loss = 0.5214\n",
      "Epoch 2, Step 420: Loss = 0.8444\n",
      "Epoch 2, Step 430: Loss = 1.2314\n",
      "Epoch 2, Step 440: Loss = 0.0428\n",
      "Epoch 2, Step 450: Loss = 0.7043\n",
      "Epoch 2, Step 460: Loss = 0.7282\n",
      "Epoch 2, Step 470: Loss = 0.5502\n",
      "Epoch 2, Step 480: Loss = 0.6669\n",
      "Epoch 2, Step 490: Loss = 0.3520\n",
      "Epoch 2, Step 500: Loss = 0.8432\n",
      "Epoch 2, Step 510: Loss = 0.3284\n",
      "Epoch 2, Step 520: Loss = 0.4522\n",
      "Epoch 2, Step 530: Loss = 0.5162\n",
      "Epoch 2, Step 540: Loss = 0.3885\n",
      "Epoch 2, Step 550: Loss = 0.6003\n",
      "Epoch 2, Step 560: Loss = 1.8948\n",
      "Epoch 2, Step 570: Loss = 0.6004\n",
      "Epoch 2, Step 580: Loss = 0.5430\n",
      "Epoch 2, Step 590: Loss = 0.3817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.6923\n",
      "precision: 0.6895\n",
      "recall: 0.6996\n",
      "f1: 0.6945\n",
      "loss: 0.7210\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7575\n",
      "precision: 0.7537\n",
      "recall: 0.7650\n",
      "f1: 0.7593\n",
      "loss: 0.5665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 0: Loss = 0.6685\n",
      "Epoch 3, Step 10: Loss = 1.2859\n",
      "Epoch 3, Step 20: Loss = 0.2171\n",
      "Epoch 3, Step 30: Loss = 0.9447\n",
      "Epoch 3, Step 40: Loss = 0.4545\n",
      "Epoch 3, Step 50: Loss = 0.7475\n",
      "Epoch 3, Step 60: Loss = 0.8197\n",
      "Epoch 3, Step 70: Loss = 0.4796\n",
      "Epoch 3, Step 80: Loss = 0.3637\n",
      "Epoch 3, Step 90: Loss = 0.7266\n",
      "Epoch 3, Step 100: Loss = 1.1577\n",
      "Epoch 3, Step 110: Loss = 0.7384\n",
      "Epoch 3, Step 120: Loss = 0.2847\n",
      "Epoch 3, Step 130: Loss = 0.7295\n",
      "Epoch 3, Step 140: Loss = 0.2544\n",
      "Epoch 3, Step 150: Loss = 0.3054\n",
      "Epoch 3, Step 160: Loss = 0.4911\n",
      "Epoch 3, Step 170: Loss = 0.7533\n",
      "Epoch 3, Step 180: Loss = 0.2588\n",
      "Epoch 3, Step 190: Loss = 0.4652\n",
      "Epoch 3, Step 200: Loss = 0.4506\n",
      "Epoch 3, Step 210: Loss = 0.6996\n",
      "Epoch 3, Step 220: Loss = 0.2090\n",
      "Epoch 3, Step 230: Loss = 0.2631\n",
      "Epoch 3, Step 240: Loss = 0.3075\n",
      "Epoch 3, Step 250: Loss = 0.6726\n",
      "Epoch 3, Step 260: Loss = 0.1591\n",
      "Epoch 3, Step 270: Loss = 0.2432\n",
      "Epoch 3, Step 280: Loss = 0.6471\n",
      "Epoch 3, Step 290: Loss = 0.7295\n",
      "Epoch 3, Step 300: Loss = 0.2303\n",
      "Epoch 3, Step 310: Loss = 0.9328\n",
      "Epoch 3, Step 320: Loss = 0.4464\n",
      "Epoch 3, Step 330: Loss = 0.2540\n",
      "Epoch 3, Step 340: Loss = 0.9505\n",
      "Epoch 3, Step 350: Loss = 0.8387\n",
      "Epoch 3, Step 360: Loss = 0.8232\n",
      "Epoch 3, Step 370: Loss = 0.9074\n",
      "Epoch 3, Step 380: Loss = 0.3926\n",
      "Epoch 3, Step 390: Loss = 0.3429\n",
      "Epoch 3, Step 400: Loss = 0.0969\n",
      "Epoch 3, Step 410: Loss = 0.5880\n",
      "Epoch 3, Step 420: Loss = 0.0796\n",
      "Epoch 3, Step 430: Loss = 1.0346\n",
      "Epoch 3, Step 440: Loss = 1.1327\n",
      "Epoch 3, Step 450: Loss = 0.9171\n",
      "Epoch 3, Step 460: Loss = 0.4086\n",
      "Epoch 3, Step 470: Loss = 0.6609\n",
      "Epoch 3, Step 480: Loss = 1.9427\n",
      "Epoch 3, Step 490: Loss = 0.0716\n",
      "Epoch 3, Step 500: Loss = 1.0133\n",
      "Epoch 3, Step 510: Loss = 1.1968\n",
      "Epoch 3, Step 520: Loss = 0.9192\n",
      "Epoch 3, Step 530: Loss = 0.7886\n",
      "Epoch 3, Step 540: Loss = 0.1373\n",
      "Epoch 3, Step 550: Loss = 0.1076\n",
      "Epoch 3, Step 560: Loss = 0.2080\n",
      "Epoch 3, Step 570: Loss = 0.3345\n",
      "Epoch 3, Step 580: Loss = 0.0798\n",
      "Epoch 3, Step 590: Loss = 0.4834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7700\n",
      "precision: 0.7711\n",
      "recall: 0.7679\n",
      "f1: 0.7695\n",
      "loss: 0.5193\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8050\n",
      "precision: 0.7952\n",
      "recall: 0.8217\n",
      "f1: 0.8082\n",
      "loss: 0.4694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 0: Loss = 0.2062\n",
      "Epoch 4, Step 10: Loss = 0.1328\n",
      "Epoch 4, Step 20: Loss = 0.8352\n",
      "Epoch 4, Step 30: Loss = 0.6021\n",
      "Epoch 4, Step 40: Loss = 0.3119\n",
      "Epoch 4, Step 50: Loss = 0.2443\n",
      "Epoch 4, Step 60: Loss = 0.2862\n",
      "Epoch 4, Step 70: Loss = 0.4691\n",
      "Epoch 4, Step 80: Loss = 0.8902\n",
      "Epoch 4, Step 90: Loss = 0.8654\n",
      "Epoch 4, Step 100: Loss = 0.1640\n",
      "Epoch 4, Step 110: Loss = 0.6538\n",
      "Epoch 4, Step 120: Loss = 0.8195\n",
      "Epoch 4, Step 130: Loss = 0.4799\n",
      "Epoch 4, Step 140: Loss = 0.4633\n",
      "Epoch 4, Step 150: Loss = 0.7258\n",
      "Epoch 4, Step 160: Loss = 0.4624\n",
      "Epoch 4, Step 170: Loss = 0.9507\n",
      "Epoch 4, Step 180: Loss = 0.4534\n",
      "Epoch 4, Step 190: Loss = 0.4945\n",
      "Epoch 4, Step 200: Loss = 0.6972\n",
      "Epoch 4, Step 210: Loss = 0.8402\n",
      "Epoch 4, Step 220: Loss = 0.4330\n",
      "Epoch 4, Step 230: Loss = 0.4441\n",
      "Epoch 4, Step 240: Loss = 0.4102\n",
      "Epoch 4, Step 250: Loss = 0.0778\n",
      "Epoch 4, Step 260: Loss = 0.2228\n",
      "Epoch 4, Step 270: Loss = 0.1479\n",
      "Epoch 4, Step 280: Loss = 0.4418\n",
      "Epoch 4, Step 290: Loss = 0.0814\n",
      "Epoch 4, Step 300: Loss = 0.6952\n",
      "Epoch 4, Step 310: Loss = 0.3911\n",
      "Epoch 4, Step 320: Loss = 0.6440\n",
      "Epoch 4, Step 330: Loss = 1.0992\n",
      "Epoch 4, Step 340: Loss = 1.1424\n",
      "Epoch 4, Step 350: Loss = 0.9232\n",
      "Epoch 4, Step 360: Loss = 0.9123\n",
      "Epoch 4, Step 370: Loss = 0.6447\n",
      "Epoch 4, Step 380: Loss = 0.1740\n",
      "Epoch 4, Step 390: Loss = 0.4356\n",
      "Epoch 4, Step 400: Loss = 0.5930\n",
      "Epoch 4, Step 410: Loss = 0.8460\n",
      "Epoch 4, Step 420: Loss = 0.6954\n",
      "Epoch 4, Step 430: Loss = 0.5112\n",
      "Epoch 4, Step 440: Loss = 0.5961\n",
      "Epoch 4, Step 450: Loss = 0.0472\n",
      "Epoch 4, Step 460: Loss = 0.2513\n",
      "Epoch 4, Step 470: Loss = 0.5058\n",
      "Epoch 4, Step 480: Loss = 0.7733\n",
      "Epoch 4, Step 490: Loss = 0.5575\n",
      "Epoch 4, Step 500: Loss = 0.5863\n",
      "Epoch 4, Step 510: Loss = 0.3198\n",
      "Epoch 4, Step 520: Loss = 0.1146\n",
      "Epoch 4, Step 530: Loss = 0.2078\n",
      "Epoch 4, Step 540: Loss = 0.2888\n",
      "Epoch 4, Step 550: Loss = 0.4015\n",
      "Epoch 4, Step 560: Loss = 1.3126\n",
      "Epoch 4, Step 570: Loss = 0.3828\n",
      "Epoch 4, Step 580: Loss = 0.2296\n",
      "Epoch 4, Step 590: Loss = 0.6078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8054\n",
      "precision: 0.8047\n",
      "recall: 0.8067\n",
      "f1: 0.8057\n",
      "loss: 0.4493\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8242\n",
      "precision: 0.8173\n",
      "recall: 0.8350\n",
      "f1: 0.8261\n",
      "loss: 0.4342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.0990\n",
      "Epoch 5, Step 10: Loss = 0.1100\n",
      "Epoch 5, Step 20: Loss = 0.6624\n",
      "Epoch 5, Step 30: Loss = 0.3131\n",
      "Epoch 5, Step 40: Loss = 0.2279\n",
      "Epoch 5, Step 50: Loss = 0.3831\n",
      "Epoch 5, Step 60: Loss = 0.4696\n",
      "Epoch 5, Step 70: Loss = 0.5470\n",
      "Epoch 5, Step 80: Loss = 0.4089\n",
      "Epoch 5, Step 90: Loss = 0.8191\n",
      "Epoch 5, Step 100: Loss = 0.1950\n",
      "Epoch 5, Step 110: Loss = 0.1967\n",
      "Epoch 5, Step 120: Loss = 0.4207\n",
      "Epoch 5, Step 130: Loss = 0.5370\n",
      "Epoch 5, Step 140: Loss = 0.8639\n",
      "Epoch 5, Step 150: Loss = 1.5746\n",
      "Epoch 5, Step 160: Loss = 0.1322\n",
      "Epoch 5, Step 170: Loss = 0.5886\n",
      "Epoch 5, Step 180: Loss = 0.4048\n",
      "Epoch 5, Step 190: Loss = 0.5198\n",
      "Epoch 5, Step 200: Loss = 0.1504\n",
      "Epoch 5, Step 210: Loss = 0.7037\n",
      "Epoch 5, Step 220: Loss = 0.4896\n",
      "Epoch 5, Step 230: Loss = 0.2120\n",
      "Epoch 5, Step 240: Loss = 0.6935\n",
      "Epoch 5, Step 250: Loss = 0.6593\n",
      "Epoch 5, Step 260: Loss = 0.7328\n",
      "Epoch 5, Step 270: Loss = 0.7225\n",
      "Epoch 5, Step 280: Loss = 0.0739\n",
      "Epoch 5, Step 290: Loss = 0.6100\n",
      "Epoch 5, Step 300: Loss = 0.6213\n",
      "Epoch 5, Step 310: Loss = 0.3153\n",
      "Epoch 5, Step 320: Loss = 0.6299\n",
      "Epoch 5, Step 330: Loss = 1.0915\n",
      "Epoch 5, Step 340: Loss = 0.3936\n",
      "Epoch 5, Step 350: Loss = 0.5947\n",
      "Epoch 5, Step 360: Loss = 0.2178\n",
      "Epoch 5, Step 370: Loss = 0.8311\n",
      "Epoch 5, Step 380: Loss = 1.0792\n",
      "Epoch 5, Step 390: Loss = 0.2344\n",
      "Epoch 5, Step 400: Loss = 0.0932\n",
      "Epoch 5, Step 410: Loss = 0.3515\n",
      "Epoch 5, Step 420: Loss = 0.6417\n",
      "Epoch 5, Step 430: Loss = 0.4709\n",
      "Epoch 5, Step 440: Loss = 0.1756\n",
      "Epoch 5, Step 450: Loss = 1.0696\n",
      "Epoch 5, Step 460: Loss = 0.0799\n",
      "Epoch 5, Step 470: Loss = 0.0538\n",
      "Epoch 5, Step 480: Loss = 0.4782\n",
      "Epoch 5, Step 490: Loss = 0.2639\n",
      "Epoch 5, Step 500: Loss = 0.0823\n",
      "Epoch 5, Step 510: Loss = 0.2680\n",
      "Epoch 5, Step 520: Loss = 0.1421\n",
      "Epoch 5, Step 530: Loss = 0.6698\n",
      "Epoch 5, Step 540: Loss = 0.4732\n",
      "Epoch 5, Step 550: Loss = 0.4667\n",
      "Epoch 5, Step 560: Loss = 0.5110\n",
      "Epoch 5, Step 570: Loss = 0.4079\n",
      "Epoch 5, Step 580: Loss = 0.8274\n",
      "Epoch 5, Step 590: Loss = 0.6801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8133\n",
      "precision: 0.8144\n",
      "recall: 0.8117\n",
      "f1: 0.8130\n",
      "loss: 0.4296\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8333\n",
      "precision: 0.8279\n",
      "recall: 0.8417\n",
      "f1: 0.8347\n",
      "loss: 0.4172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Step 0: Loss = 0.1668\n",
      "Epoch 6, Step 10: Loss = 0.0474\n",
      "Epoch 6, Step 20: Loss = 0.6506\n",
      "Epoch 6, Step 30: Loss = 1.3475\n",
      "Epoch 6, Step 40: Loss = 0.5238\n",
      "Epoch 6, Step 50: Loss = 0.5794\n",
      "Epoch 6, Step 60: Loss = 0.3678\n",
      "Epoch 6, Step 70: Loss = 0.1993\n",
      "Epoch 6, Step 80: Loss = 0.2345\n",
      "Epoch 6, Step 90: Loss = 0.4459\n",
      "Epoch 6, Step 100: Loss = 0.4330\n",
      "Epoch 6, Step 110: Loss = 0.7010\n",
      "Epoch 6, Step 120: Loss = 0.5625\n",
      "Epoch 6, Step 130: Loss = 0.1081\n",
      "Epoch 6, Step 140: Loss = 0.2626\n",
      "Epoch 6, Step 150: Loss = 0.3033\n",
      "Epoch 6, Step 160: Loss = 0.7171\n",
      "Epoch 6, Step 170: Loss = 0.4904\n",
      "Epoch 6, Step 180: Loss = 0.5293\n",
      "Epoch 6, Step 190: Loss = 0.7592\n",
      "Epoch 6, Step 200: Loss = 0.1043\n",
      "Epoch 6, Step 210: Loss = 0.2384\n",
      "Epoch 6, Step 220: Loss = 0.5612\n",
      "Epoch 6, Step 230: Loss = 0.2620\n",
      "Epoch 6, Step 240: Loss = 0.5511\n",
      "Epoch 6, Step 250: Loss = 0.6964\n",
      "Epoch 6, Step 260: Loss = 0.5176\n",
      "Epoch 6, Step 270: Loss = 0.3460\n",
      "Epoch 6, Step 280: Loss = 0.3603\n",
      "Epoch 6, Step 290: Loss = 0.2600\n",
      "Epoch 6, Step 300: Loss = 0.2824\n",
      "Epoch 6, Step 310: Loss = 0.1245\n",
      "Epoch 6, Step 320: Loss = 0.1094\n",
      "Epoch 6, Step 330: Loss = 0.2184\n",
      "Epoch 6, Step 340: Loss = 0.3422\n",
      "Epoch 6, Step 350: Loss = 0.5172\n",
      "Epoch 6, Step 360: Loss = 0.8843\n",
      "Epoch 6, Step 370: Loss = 0.3018\n",
      "Epoch 6, Step 380: Loss = 0.2568\n",
      "Epoch 6, Step 390: Loss = 0.3693\n",
      "Epoch 6, Step 400: Loss = 0.1249\n",
      "Epoch 6, Step 410: Loss = 0.2639\n",
      "Epoch 6, Step 420: Loss = 0.2609\n",
      "Epoch 6, Step 430: Loss = 0.7832\n",
      "Epoch 6, Step 440: Loss = 0.3275\n",
      "Epoch 6, Step 450: Loss = 0.6184\n",
      "Epoch 6, Step 460: Loss = 0.5077\n",
      "Epoch 6, Step 470: Loss = 0.3882\n",
      "Epoch 6, Step 480: Loss = 0.3726\n",
      "Epoch 6, Step 490: Loss = 0.1248\n",
      "Epoch 6, Step 500: Loss = 0.3988\n",
      "Epoch 6, Step 510: Loss = 0.2461\n",
      "Epoch 6, Step 520: Loss = 0.1855\n",
      "Epoch 6, Step 530: Loss = 0.2628\n",
      "Epoch 6, Step 540: Loss = 0.2325\n",
      "Epoch 6, Step 550: Loss = 0.2396\n",
      "Epoch 6, Step 560: Loss = 0.3557\n",
      "Epoch 6, Step 570: Loss = 0.4028\n",
      "Epoch 6, Step 580: Loss = 0.6881\n",
      "Epoch 6, Step 590: Loss = 0.0874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8233\n",
      "precision: 0.8261\n",
      "recall: 0.8192\n",
      "f1: 0.8226\n",
      "loss: 0.4108\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8350\n",
      "precision: 0.8295\n",
      "recall: 0.8433\n",
      "f1: 0.8364\n",
      "loss: 0.4095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Step 0: Loss = 0.2659\n",
      "Epoch 7, Step 10: Loss = 0.2016\n",
      "Epoch 7, Step 20: Loss = 0.2459\n",
      "Epoch 7, Step 30: Loss = 0.6207\n",
      "Epoch 7, Step 40: Loss = 0.2176\n",
      "Epoch 7, Step 50: Loss = 0.1260\n",
      "Epoch 7, Step 60: Loss = 0.7005\n",
      "Epoch 7, Step 70: Loss = 0.2711\n",
      "Epoch 7, Step 80: Loss = 0.3098\n",
      "Epoch 7, Step 90: Loss = 0.0592\n",
      "Epoch 7, Step 100: Loss = 0.8338\n",
      "Epoch 7, Step 110: Loss = 0.1684\n",
      "Epoch 7, Step 120: Loss = 0.4756\n",
      "Epoch 7, Step 130: Loss = 0.2388\n",
      "Epoch 7, Step 140: Loss = 0.2030\n",
      "Epoch 7, Step 150: Loss = 0.1851\n",
      "Epoch 7, Step 160: Loss = 0.2112\n",
      "Epoch 7, Step 170: Loss = 0.2918\n",
      "Epoch 7, Step 180: Loss = 0.0681\n",
      "Epoch 7, Step 190: Loss = 0.1299\n",
      "Epoch 7, Step 200: Loss = 0.0904\n",
      "Epoch 7, Step 210: Loss = 0.2553\n",
      "Epoch 7, Step 220: Loss = 0.2253\n",
      "Epoch 7, Step 230: Loss = 0.2416\n",
      "Epoch 7, Step 240: Loss = 0.8246\n",
      "Epoch 7, Step 250: Loss = 0.4017\n",
      "Epoch 7, Step 260: Loss = 0.2774\n",
      "Epoch 7, Step 270: Loss = 0.3267\n",
      "Epoch 7, Step 280: Loss = 0.6528\n",
      "Epoch 7, Step 290: Loss = 0.2252\n",
      "Epoch 7, Step 300: Loss = 0.1232\n",
      "Epoch 7, Step 310: Loss = 0.7199\n",
      "Epoch 7, Step 320: Loss = 0.3336\n",
      "Epoch 7, Step 330: Loss = 0.2531\n",
      "Epoch 7, Step 340: Loss = 0.0440\n",
      "Epoch 7, Step 350: Loss = 0.5144\n",
      "Epoch 7, Step 360: Loss = 0.2150\n",
      "Epoch 7, Step 370: Loss = 0.3084\n",
      "Epoch 7, Step 380: Loss = 0.1986\n",
      "Epoch 7, Step 390: Loss = 0.5562\n",
      "Epoch 7, Step 400: Loss = 0.6606\n",
      "Epoch 7, Step 410: Loss = 0.8815\n",
      "Epoch 7, Step 420: Loss = 0.6361\n",
      "Epoch 7, Step 430: Loss = 0.6214\n",
      "Epoch 7, Step 440: Loss = 0.2616\n",
      "Epoch 7, Step 450: Loss = 0.2995\n",
      "Epoch 7, Step 460: Loss = 0.1175\n",
      "Epoch 7, Step 470: Loss = 0.4479\n",
      "Epoch 7, Step 480: Loss = 0.0732\n",
      "Epoch 7, Step 490: Loss = 0.3075\n",
      "Epoch 7, Step 500: Loss = 1.4736\n",
      "Epoch 7, Step 510: Loss = 0.7479\n",
      "Epoch 7, Step 520: Loss = 0.1802\n",
      "Epoch 7, Step 530: Loss = 0.2597\n",
      "Epoch 7, Step 540: Loss = 0.3943\n",
      "Epoch 7, Step 550: Loss = 0.4035\n",
      "Epoch 7, Step 560: Loss = 0.0678\n",
      "Epoch 7, Step 570: Loss = 0.0402\n",
      "Epoch 7, Step 580: Loss = 0.6326\n",
      "Epoch 7, Step 590: Loss = 0.3583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8246\n",
      "precision: 0.8273\n",
      "recall: 0.8204\n",
      "f1: 0.8238\n",
      "loss: 0.4100\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8342\n",
      "precision: 0.8282\n",
      "recall: 0.8433\n",
      "f1: 0.8357\n",
      "loss: 0.4060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Step 0: Loss = 0.3762\n",
      "Epoch 8, Step 10: Loss = 0.3037\n",
      "Epoch 8, Step 20: Loss = 0.3188\n",
      "Epoch 8, Step 30: Loss = 0.6653\n",
      "Epoch 8, Step 40: Loss = 0.2065\n",
      "Epoch 8, Step 50: Loss = 0.5300\n",
      "Epoch 8, Step 60: Loss = 0.1579\n",
      "Epoch 8, Step 70: Loss = 0.3381\n",
      "Epoch 8, Step 80: Loss = 0.1750\n",
      "Epoch 8, Step 90: Loss = 0.4464\n",
      "Epoch 8, Step 100: Loss = 0.0978\n",
      "Epoch 8, Step 110: Loss = 0.2467\n",
      "Epoch 8, Step 120: Loss = 0.0698\n",
      "Epoch 8, Step 130: Loss = 0.5287\n",
      "Epoch 8, Step 140: Loss = 0.3284\n",
      "Epoch 8, Step 150: Loss = 0.1188\n",
      "Epoch 8, Step 160: Loss = 0.3246\n",
      "Epoch 8, Step 170: Loss = 0.9202\n",
      "Epoch 8, Step 180: Loss = 0.3063\n",
      "Epoch 8, Step 190: Loss = 0.3213\n",
      "Epoch 8, Step 200: Loss = 0.2877\n",
      "Epoch 8, Step 210: Loss = 0.2384\n",
      "Epoch 8, Step 220: Loss = 0.5121\n",
      "Epoch 8, Step 230: Loss = 0.4663\n",
      "Epoch 8, Step 240: Loss = 0.6268\n",
      "Epoch 8, Step 250: Loss = 0.4933\n",
      "Epoch 8, Step 260: Loss = 0.1397\n",
      "Epoch 8, Step 270: Loss = 0.4174\n",
      "Epoch 8, Step 280: Loss = 0.1364\n",
      "Epoch 8, Step 290: Loss = 0.1532\n",
      "Epoch 8, Step 300: Loss = 0.3396\n",
      "Epoch 8, Step 310: Loss = 0.7553\n",
      "Epoch 8, Step 320: Loss = 0.8189\n",
      "Epoch 8, Step 330: Loss = 0.4397\n",
      "Epoch 8, Step 340: Loss = 0.5861\n",
      "Epoch 8, Step 350: Loss = 0.1279\n",
      "Epoch 8, Step 360: Loss = 0.2915\n",
      "Epoch 8, Step 370: Loss = 0.5103\n",
      "Epoch 8, Step 380: Loss = 0.2831\n",
      "Epoch 8, Step 390: Loss = 0.1770\n",
      "Epoch 8, Step 400: Loss = 0.2618\n",
      "Epoch 8, Step 410: Loss = 0.3569\n",
      "Epoch 8, Step 420: Loss = 0.3612\n",
      "Epoch 8, Step 430: Loss = 0.0460\n",
      "Epoch 8, Step 440: Loss = 0.2871\n",
      "Epoch 8, Step 450: Loss = 0.1482\n",
      "Epoch 8, Step 460: Loss = 0.3389\n",
      "Epoch 8, Step 470: Loss = 0.6323\n",
      "Epoch 8, Step 480: Loss = 0.7421\n",
      "Epoch 8, Step 490: Loss = 0.1212\n",
      "Epoch 8, Step 500: Loss = 0.1596\n",
      "Epoch 8, Step 510: Loss = 0.8137\n",
      "Epoch 8, Step 520: Loss = 0.1602\n",
      "Epoch 8, Step 530: Loss = 0.9194\n",
      "Epoch 8, Step 540: Loss = 0.6226\n",
      "Epoch 8, Step 550: Loss = 0.1164\n",
      "Epoch 8, Step 560: Loss = 0.1176\n",
      "Epoch 8, Step 570: Loss = 0.6863\n",
      "Epoch 8, Step 580: Loss = 0.4380\n",
      "Epoch 8, Step 590: Loss = 0.6605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8281\n",
      "precision: 0.8277\n",
      "recall: 0.8287\n",
      "f1: 0.8282\n",
      "loss: 0.4043\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8350\n",
      "precision: 0.8295\n",
      "recall: 0.8433\n",
      "f1: 0.8364\n",
      "loss: 0.4043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 0: Loss = 0.0709\n",
      "Epoch 9, Step 10: Loss = 0.1444\n",
      "Epoch 9, Step 20: Loss = 0.1741\n",
      "Epoch 9, Step 30: Loss = 0.0682\n",
      "Epoch 9, Step 40: Loss = 0.1114\n",
      "Epoch 9, Step 50: Loss = 0.4929\n",
      "Epoch 9, Step 60: Loss = 0.1031\n",
      "Epoch 9, Step 70: Loss = 0.4192\n",
      "Epoch 9, Step 80: Loss = 0.8291\n",
      "Epoch 9, Step 90: Loss = 0.7346\n",
      "Epoch 9, Step 100: Loss = 0.4333\n",
      "Epoch 9, Step 110: Loss = 0.2631\n",
      "Epoch 9, Step 120: Loss = 0.2886\n",
      "Epoch 9, Step 130: Loss = 0.7934\n",
      "Epoch 9, Step 140: Loss = 0.5070\n",
      "Epoch 9, Step 150: Loss = 0.7782\n",
      "Epoch 9, Step 160: Loss = 0.5225\n",
      "Epoch 9, Step 170: Loss = 0.1582\n",
      "Epoch 9, Step 180: Loss = 0.8176\n",
      "Epoch 9, Step 190: Loss = 0.1243\n",
      "Epoch 9, Step 200: Loss = 0.5370\n",
      "Epoch 9, Step 210: Loss = 0.2017\n",
      "Epoch 9, Step 220: Loss = 0.0919\n",
      "Epoch 9, Step 230: Loss = 0.1657\n",
      "Epoch 9, Step 240: Loss = 0.6046\n",
      "Epoch 9, Step 250: Loss = 0.2597\n",
      "Epoch 9, Step 260: Loss = 0.4361\n",
      "Epoch 9, Step 270: Loss = 0.2859\n",
      "Epoch 9, Step 280: Loss = 0.5497\n",
      "Epoch 9, Step 290: Loss = 0.1168\n",
      "Epoch 9, Step 300: Loss = 0.9313\n",
      "Epoch 9, Step 310: Loss = 1.2748\n",
      "Epoch 9, Step 320: Loss = 0.3322\n",
      "Epoch 9, Step 330: Loss = 0.3028\n",
      "Epoch 9, Step 340: Loss = 0.1061\n",
      "Epoch 9, Step 350: Loss = 0.3492\n",
      "Epoch 9, Step 360: Loss = 0.3700\n",
      "Epoch 9, Step 370: Loss = 0.7062\n",
      "Epoch 9, Step 380: Loss = 0.4383\n",
      "Epoch 9, Step 390: Loss = 0.3060\n",
      "Epoch 9, Step 400: Loss = 0.2192\n",
      "Epoch 9, Step 410: Loss = 0.5796\n",
      "Epoch 9, Step 420: Loss = 0.4703\n",
      "Epoch 9, Step 430: Loss = 0.2537\n",
      "Epoch 9, Step 440: Loss = 0.1143\n",
      "Epoch 9, Step 450: Loss = 0.7199\n",
      "Epoch 9, Step 460: Loss = 0.1567\n",
      "Epoch 9, Step 470: Loss = 0.4015\n",
      "Epoch 9, Step 480: Loss = 0.1761\n",
      "Epoch 9, Step 490: Loss = 0.5593\n",
      "Epoch 9, Step 500: Loss = 0.4835\n",
      "Epoch 9, Step 510: Loss = 0.2987\n",
      "Epoch 9, Step 520: Loss = 0.4410\n",
      "Epoch 9, Step 530: Loss = 0.4197\n",
      "Epoch 9, Step 540: Loss = 0.1041\n",
      "Epoch 9, Step 550: Loss = 0.4446\n",
      "Epoch 9, Step 560: Loss = 0.5963\n",
      "Epoch 9, Step 570: Loss = 0.2612\n",
      "Epoch 9, Step 580: Loss = 0.3410\n",
      "Epoch 9, Step 590: Loss = 0.1450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8310\n",
      "precision: 0.8312\n",
      "recall: 0.8308\n",
      "f1: 0.8310\n",
      "loss: 0.4024\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8358\n",
      "precision: 0.8309\n",
      "recall: 0.8433\n",
      "f1: 0.8371\n",
      "loss: 0.4034\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        self.emails_df = emails_df.copy()\n",
    "        self.emails_df['sender'] = self.emails_df['sender'].apply(clean_text)\n",
    "        self.emails_df['subject'] = self.emails_df['subject'].apply(clean_text)\n",
    "        self.emails_df['body'] = self.emails_df['body'].apply(clean_text)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        email = self.emails_df.iloc[idx]\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(email['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    #text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    #text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model_config.use_cache = False\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, output_dir):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    #plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "    \n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    return metrics\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9):\n",
    "    best_val_metrics = {'f1': 0}\n",
    "    best_model_state = None\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % 2 == 0:  # Gradient accumulation steps = 2\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(batch['label'].cpu().numpy())\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "  \n",
    "        train_metrics = compute_metrics(train_preds, train_labels)\n",
    "        train_metrics['loss'] = total_loss / len(train_loader)\n",
    "      \n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "      \n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(\"Training Metrics:\")\n",
    "        for metric, value in train_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        for metric, value in val_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_val_metrics['f1']:\n",
    "            best_val_metrics = val_metrics\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return best_model_state, best_val_metrics\n",
    "\n",
    "def main():\n",
    "    login(token=\"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\")\n",
    "    device = setup_environment()\n",
    "    model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/newdata_cleaned.csv\")\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "    \n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    \n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "    \n",
    "    train_dataset = EmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = EmailDataset(val_df, tokenizer, max_length=512)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    best_model_state, best_metrics = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/binary_classification_8B_revised\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    #plot_confusion_matrix(best_metrics['confusion_matrix'], output_dir)\n",
    "    \n",
    "  \n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"best_metrics\": {k: float(v) if k != 'confusion_matrix' else v.tolist() \n",
    "                        for k, v in best_metrics.items()}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ae14743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n",
      "GPU Memory: 23.68 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 0.7377\n",
      "Epoch 1, Step 10: Loss = 0.7115\n",
      "Epoch 1, Step 20: Loss = 0.6869\n",
      "Epoch 1, Step 30: Loss = 0.6944\n",
      "Epoch 1, Step 40: Loss = 0.6701\n",
      "Epoch 1, Step 50: Loss = 0.6761\n",
      "Epoch 1, Step 60: Loss = 0.7054\n",
      "Epoch 1, Step 70: Loss = 0.6676\n",
      "Epoch 1, Step 80: Loss = 0.6628\n",
      "Epoch 1, Step 90: Loss = 0.6530\n",
      "Epoch 1, Step 100: Loss = 0.6286\n",
      "Epoch 1, Step 110: Loss = 0.6690\n",
      "Epoch 1, Step 120: Loss = 0.6444\n",
      "Epoch 1, Step 130: Loss = 0.6622\n",
      "Epoch 1, Step 140: Loss = 0.7084\n",
      "Epoch 1, Step 150: Loss = 0.6539\n",
      "Epoch 1, Step 160: Loss = 0.6355\n",
      "Epoch 1, Step 170: Loss = 0.6604\n",
      "Epoch 1, Step 180: Loss = 0.6192\n",
      "Epoch 1, Step 190: Loss = 0.6211\n",
      "Epoch 1, Step 200: Loss = 0.5759\n",
      "Epoch 1, Step 210: Loss = 0.5991\n",
      "Epoch 1, Step 220: Loss = 0.6746\n",
      "Epoch 1, Step 230: Loss = 0.6010\n",
      "Epoch 1, Step 240: Loss = 0.6221\n",
      "Epoch 1, Step 250: Loss = 0.5301\n",
      "Epoch 1, Step 260: Loss = 0.6151\n",
      "Epoch 1, Step 270: Loss = 0.5844\n",
      "Epoch 1, Step 280: Loss = 0.4754\n",
      "Epoch 1, Step 290: Loss = 0.5471\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.6719\n",
      "precision: 0.6828\n",
      "recall: 0.6421\n",
      "f1: 0.6618\n",
      "loss: 0.6337\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8267\n",
      "precision: 0.8368\n",
      "recall: 0.8117\n",
      "f1: 0.8240\n",
      "loss: 0.4943\n",
      "Epoch 2, Step 0: Loss = 0.5113\n",
      "Epoch 2, Step 10: Loss = 0.4941\n",
      "Epoch 2, Step 20: Loss = 0.4019\n",
      "Epoch 2, Step 30: Loss = 0.5730\n",
      "Epoch 2, Step 40: Loss = 0.4922\n",
      "Epoch 2, Step 50: Loss = 0.4348\n",
      "Epoch 2, Step 60: Loss = 0.5158\n",
      "Epoch 2, Step 70: Loss = 0.4136\n",
      "Epoch 2, Step 80: Loss = 0.2943\n",
      "Epoch 2, Step 90: Loss = 0.3196\n",
      "Epoch 2, Step 100: Loss = 0.3795\n",
      "Epoch 2, Step 110: Loss = 0.2856\n",
      "Epoch 2, Step 120: Loss = 0.3064\n",
      "Epoch 2, Step 130: Loss = 0.2066\n",
      "Epoch 2, Step 140: Loss = 0.1988\n",
      "Epoch 2, Step 150: Loss = 0.3391\n",
      "Epoch 2, Step 160: Loss = 0.1453\n",
      "Epoch 2, Step 170: Loss = 0.1486\n",
      "Epoch 2, Step 180: Loss = 0.1114\n",
      "Epoch 2, Step 190: Loss = 0.1956\n",
      "Epoch 2, Step 200: Loss = 0.1913\n",
      "Epoch 2, Step 210: Loss = 0.0991\n",
      "Epoch 2, Step 220: Loss = 0.0548\n",
      "Epoch 2, Step 230: Loss = 0.1442\n",
      "Epoch 2, Step 240: Loss = 0.1512\n",
      "Epoch 2, Step 250: Loss = 0.2345\n",
      "Epoch 2, Step 260: Loss = 0.1285\n",
      "Epoch 2, Step 270: Loss = 0.2743\n",
      "Epoch 2, Step 280: Loss = 0.1077\n",
      "Epoch 2, Step 290: Loss = 0.1713\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9137\n",
      "precision: 0.9371\n",
      "recall: 0.8871\n",
      "f1: 0.9114\n",
      "loss: 0.2662\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9600\n",
      "precision: 0.9600\n",
      "recall: 0.9600\n",
      "f1: 0.9600\n",
      "loss: 0.1164\n",
      "Epoch 3, Step 0: Loss = 0.4439\n",
      "Epoch 3, Step 10: Loss = 0.0693\n",
      "Epoch 3, Step 20: Loss = 0.0836\n",
      "Epoch 3, Step 30: Loss = 0.1519\n",
      "Epoch 3, Step 40: Loss = 0.0539\n",
      "Epoch 3, Step 50: Loss = 0.0484\n",
      "Epoch 3, Step 60: Loss = 0.2264\n",
      "Epoch 3, Step 70: Loss = 0.1308\n",
      "Epoch 3, Step 80: Loss = 0.2168\n",
      "Epoch 3, Step 90: Loss = 0.0301\n",
      "Epoch 3, Step 100: Loss = 0.1033\n",
      "Epoch 3, Step 110: Loss = 0.2318\n",
      "Epoch 3, Step 120: Loss = 0.1390\n",
      "Epoch 3, Step 130: Loss = 0.0501\n",
      "Epoch 3, Step 140: Loss = 0.1675\n",
      "Epoch 3, Step 150: Loss = 0.1261\n",
      "Epoch 3, Step 160: Loss = 0.2331\n",
      "Epoch 3, Step 170: Loss = 0.0292\n",
      "Epoch 3, Step 180: Loss = 0.2269\n",
      "Epoch 3, Step 190: Loss = 0.0560\n",
      "Epoch 3, Step 200: Loss = 0.1539\n",
      "Epoch 3, Step 210: Loss = 0.1738\n",
      "Epoch 3, Step 220: Loss = 0.1378\n",
      "Epoch 3, Step 230: Loss = 0.1450\n",
      "Epoch 3, Step 240: Loss = 0.0555\n",
      "Epoch 3, Step 250: Loss = 0.0316\n",
      "Epoch 3, Step 260: Loss = 0.0858\n",
      "Epoch 3, Step 270: Loss = 0.0280\n",
      "Epoch 3, Step 280: Loss = 0.0228\n",
      "Epoch 3, Step 290: Loss = 0.2668\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9673\n",
      "precision: 0.9710\n",
      "recall: 0.9633\n",
      "f1: 0.9672\n",
      "loss: 0.1012\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9742\n",
      "precision: 0.9798\n",
      "recall: 0.9683\n",
      "f1: 0.9740\n",
      "loss: 0.0738\n",
      "Epoch 4, Step 0: Loss = 0.2596\n",
      "Epoch 4, Step 10: Loss = 0.1533\n",
      "Epoch 4, Step 20: Loss = 0.0296\n",
      "Epoch 4, Step 30: Loss = 0.2988\n",
      "Epoch 4, Step 40: Loss = 0.0164\n",
      "Epoch 4, Step 50: Loss = 0.0418\n",
      "Epoch 4, Step 60: Loss = 0.0354\n",
      "Epoch 4, Step 70: Loss = 0.0106\n",
      "Epoch 4, Step 80: Loss = 0.1268\n",
      "Epoch 4, Step 90: Loss = 0.0574\n",
      "Epoch 4, Step 100: Loss = 0.0568\n",
      "Epoch 4, Step 110: Loss = 0.0263\n",
      "Epoch 4, Step 120: Loss = 0.0192\n",
      "Epoch 4, Step 130: Loss = 0.0697\n",
      "Epoch 4, Step 140: Loss = 0.0119\n",
      "Epoch 4, Step 150: Loss = 0.0128\n",
      "Epoch 4, Step 160: Loss = 0.0414\n",
      "Epoch 4, Step 170: Loss = 0.0173\n",
      "Epoch 4, Step 180: Loss = 0.0136\n",
      "Epoch 4, Step 190: Loss = 0.1497\n",
      "Epoch 4, Step 200: Loss = 0.0193\n",
      "Epoch 4, Step 210: Loss = 0.1018\n",
      "Epoch 4, Step 220: Loss = 0.0145\n",
      "Epoch 4, Step 230: Loss = 0.0080\n",
      "Epoch 4, Step 240: Loss = 0.0139\n",
      "Epoch 4, Step 250: Loss = 0.0522\n",
      "Epoch 4, Step 260: Loss = 0.0262\n",
      "Epoch 4, Step 270: Loss = 0.0180\n",
      "Epoch 4, Step 280: Loss = 0.0687\n",
      "Epoch 4, Step 290: Loss = 0.0424\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9725\n",
      "precision: 0.9741\n",
      "recall: 0.9708\n",
      "f1: 0.9725\n",
      "loss: 0.0771\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9792\n",
      "precision: 0.9816\n",
      "recall: 0.9767\n",
      "f1: 0.9791\n",
      "loss: 0.0595\n",
      "Epoch 5, Step 0: Loss = 0.0165\n",
      "Epoch 5, Step 10: Loss = 0.0052\n",
      "Epoch 5, Step 20: Loss = 0.1075\n",
      "Epoch 5, Step 30: Loss = 0.0218\n",
      "Epoch 5, Step 40: Loss = 0.0070\n",
      "Epoch 5, Step 50: Loss = 0.0272\n",
      "Epoch 5, Step 60: Loss = 0.0126\n",
      "Epoch 5, Step 70: Loss = 0.0713\n",
      "Epoch 5, Step 80: Loss = 0.0215\n",
      "Epoch 5, Step 90: Loss = 0.2173\n",
      "Epoch 5, Step 100: Loss = 0.0082\n",
      "Epoch 5, Step 110: Loss = 0.1451\n",
      "Epoch 5, Step 120: Loss = 0.0177\n",
      "Epoch 5, Step 130: Loss = 0.0358\n",
      "Epoch 5, Step 140: Loss = 0.0076\n",
      "Epoch 5, Step 150: Loss = 0.0087\n",
      "Epoch 5, Step 160: Loss = 0.0425\n",
      "Epoch 5, Step 170: Loss = 0.1463\n",
      "Epoch 5, Step 180: Loss = 0.0110\n",
      "Epoch 5, Step 190: Loss = 0.0129\n",
      "Epoch 5, Step 200: Loss = 0.2926\n",
      "Epoch 5, Step 210: Loss = 0.0084\n",
      "Epoch 5, Step 220: Loss = 0.0474\n",
      "Epoch 5, Step 230: Loss = 0.0890\n",
      "Epoch 5, Step 240: Loss = 0.0058\n",
      "Epoch 5, Step 250: Loss = 0.0097\n",
      "Epoch 5, Step 260: Loss = 0.0051\n",
      "Epoch 5, Step 270: Loss = 0.0251\n",
      "Epoch 5, Step 280: Loss = 0.0044\n",
      "Epoch 5, Step 290: Loss = 0.0798\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9769\n",
      "precision: 0.9779\n",
      "recall: 0.9758\n",
      "f1: 0.9769\n",
      "loss: 0.0710\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9808\n",
      "precision: 0.9898\n",
      "recall: 0.9717\n",
      "f1: 0.9807\n",
      "loss: 0.0558\n",
      "Epoch 6, Step 0: Loss = 0.0053\n",
      "Epoch 6, Step 10: Loss = 0.0058\n",
      "Epoch 6, Step 20: Loss = 0.0051\n",
      "Epoch 6, Step 30: Loss = 0.0051\n",
      "Epoch 6, Step 40: Loss = 0.0420\n",
      "Epoch 6, Step 50: Loss = 0.0356\n",
      "Epoch 6, Step 60: Loss = 0.0192\n",
      "Epoch 6, Step 70: Loss = 0.0165\n",
      "Epoch 6, Step 80: Loss = 0.0243\n",
      "Epoch 6, Step 90: Loss = 0.0694\n",
      "Epoch 6, Step 100: Loss = 0.0245\n",
      "Epoch 6, Step 110: Loss = 0.0066\n",
      "Epoch 6, Step 120: Loss = 0.1787\n",
      "Epoch 6, Step 130: Loss = 0.1861\n",
      "Epoch 6, Step 140: Loss = 0.0959\n",
      "Epoch 6, Step 150: Loss = 0.1784\n",
      "Epoch 6, Step 160: Loss = 0.0036\n",
      "Epoch 6, Step 170: Loss = 0.1099\n",
      "Epoch 6, Step 180: Loss = 0.0139\n",
      "Epoch 6, Step 190: Loss = 0.0094\n",
      "Epoch 6, Step 200: Loss = 0.0082\n",
      "Epoch 6, Step 210: Loss = 0.1211\n",
      "Epoch 6, Step 220: Loss = 0.2279\n",
      "Epoch 6, Step 230: Loss = 0.0039\n",
      "Epoch 6, Step 240: Loss = 0.0078\n",
      "Epoch 6, Step 250: Loss = 0.0093\n",
      "Epoch 6, Step 260: Loss = 0.2109\n",
      "Epoch 6, Step 270: Loss = 0.0070\n",
      "Epoch 6, Step 280: Loss = 0.0403\n",
      "Epoch 6, Step 290: Loss = 0.0035\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9810\n",
      "precision: 0.9849\n",
      "recall: 0.9771\n",
      "f1: 0.9810\n",
      "loss: 0.0617\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9817\n",
      "precision: 0.9785\n",
      "recall: 0.9850\n",
      "f1: 0.9817\n",
      "loss: 0.0522\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import gc\n",
    "\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        self.emails_df = emails_df.copy()\n",
    "        self.emails_df['sender'] = self.emails_df['sender'].apply(clean_text)\n",
    "        self.emails_df['subject'] = self.emails_df['subject'].apply(clean_text)\n",
    "        self.emails_df['body'] = self.emails_df['body'].apply(clean_text)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        email = self.emails_df.iloc[idx]\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(email['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "   \n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config\n",
    "    )\n",
    "    \n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"query\", \"value\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['label']\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "    \n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    return metrics\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=8):\n",
    "    best_val_metrics = {'f1': 0}\n",
    "    best_model_state = None\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['label']\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % 2 == 0:  # Gradient accumulation steps = 2\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(batch['label'].cpu().numpy())\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "  \n",
    "        train_metrics = compute_metrics(train_preds, train_labels)\n",
    "        train_metrics['loss'] = total_loss / len(train_loader)\n",
    "      \n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "      \n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(\"Training Metrics:\")\n",
    "        for metric, value in train_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        for metric, value in val_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        if val_metrics['f1'] > best_val_metrics['f1']:\n",
    "            best_val_metrics = val_metrics\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return best_model_state, best_val_metrics\n",
    "\n",
    "def main():\n",
    "    login(token=\"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\")\n",
    "    device = setup_environment()\n",
    "    model_name = 'bert-base-uncased'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/newdata_cleaned.csv\")\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "    \n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    \n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "    \n",
    "    train_dataset = EmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = EmailDataset(val_df, tokenizer, max_length=512)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)  \n",
    "    num_epochs = 6\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    best_model_state, best_metrics = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/binary_classification_bert_revised\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    \n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 16,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"best_metrics\": {k: float(v) if k != 'confusion_matrix' else v.tolist() \n",
    "                        for k, v in best_metrics.items()}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f4c6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n",
      "GPU Memory: 23.68 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98252c546314760a8f70cd3ac35999e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at dreamgen/WizardLM-2-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 6.2113\n",
      "Epoch 1, Step 10: Loss = 6.6387\n",
      "Epoch 1, Step 20: Loss = 1.4686\n",
      "Epoch 1, Step 30: Loss = 1.3207\n",
      "Epoch 1, Step 40: Loss = 0.0483\n",
      "Epoch 1, Step 50: Loss = 3.2610\n",
      "Epoch 1, Step 60: Loss = 7.1458\n",
      "Epoch 1, Step 70: Loss = 2.7937\n",
      "Epoch 1, Step 80: Loss = 3.8336\n",
      "Epoch 1, Step 90: Loss = 3.3470\n",
      "Epoch 1, Step 100: Loss = 1.0829\n",
      "Epoch 1, Step 110: Loss = 0.1463\n",
      "Epoch 1, Step 120: Loss = 1.9070\n",
      "Epoch 1, Step 130: Loss = 2.9357\n",
      "Epoch 1, Step 140: Loss = 6.8344\n",
      "Epoch 1, Step 150: Loss = 0.7980\n",
      "Epoch 1, Step 160: Loss = 0.9489\n",
      "Epoch 1, Step 170: Loss = 2.0826\n",
      "Epoch 1, Step 180: Loss = 2.0940\n",
      "Epoch 1, Step 190: Loss = 2.6393\n",
      "Epoch 1, Step 200: Loss = 2.1722\n",
      "Epoch 1, Step 210: Loss = 1.4988\n",
      "Epoch 1, Step 220: Loss = 1.4168\n",
      "Epoch 1, Step 230: Loss = 3.3555\n",
      "Epoch 1, Step 240: Loss = 3.6142\n",
      "Epoch 1, Step 250: Loss = 0.5334\n",
      "Epoch 1, Step 260: Loss = 3.3424\n",
      "Epoch 1, Step 270: Loss = 4.0149\n",
      "Epoch 1, Step 280: Loss = 0.5563\n",
      "Epoch 1, Step 290: Loss = 3.8309\n",
      "Epoch 1, Step 300: Loss = 2.4833\n",
      "Epoch 1, Step 310: Loss = 4.9534\n",
      "Epoch 1, Step 320: Loss = 0.6263\n",
      "Epoch 1, Step 330: Loss = 2.5746\n",
      "Epoch 1, Step 340: Loss = 2.6644\n",
      "Epoch 1, Step 350: Loss = 2.5853\n",
      "Epoch 1, Step 360: Loss = 2.5661\n",
      "Epoch 1, Step 370: Loss = 3.9907\n",
      "Epoch 1, Step 380: Loss = 3.4930\n",
      "Epoch 1, Step 390: Loss = 6.0764\n",
      "Epoch 1, Step 400: Loss = 1.0750\n",
      "Epoch 1, Step 410: Loss = 2.6585\n",
      "Epoch 1, Step 420: Loss = 5.0350\n",
      "Epoch 1, Step 430: Loss = 0.9968\n",
      "Epoch 1, Step 440: Loss = 5.0670\n",
      "Epoch 1, Step 450: Loss = 3.6390\n",
      "Epoch 1, Step 460: Loss = 1.0339\n",
      "Epoch 1, Step 470: Loss = 2.9977\n",
      "Epoch 1, Step 480: Loss = 5.4593\n",
      "Epoch 1, Step 490: Loss = 2.2930\n",
      "Epoch 1, Step 500: Loss = 1.0161\n",
      "Epoch 1, Step 510: Loss = 4.9202\n",
      "Epoch 1, Step 520: Loss = 5.7645\n",
      "Epoch 1, Step 530: Loss = 5.8024\n",
      "Epoch 1, Step 540: Loss = 1.9712\n",
      "Epoch 1, Step 550: Loss = 2.7639\n",
      "Epoch 1, Step 560: Loss = 5.8395\n",
      "Epoch 1, Step 570: Loss = 2.7820\n",
      "Epoch 1, Step 580: Loss = 0.6948\n",
      "Epoch 1, Step 590: Loss = 2.5992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.5548\n",
      "precision: 0.5755\n",
      "recall: 0.4175\n",
      "f1: 0.4839\n",
      "loss: 2.9538\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.6050\n",
      "precision: 0.6230\n",
      "recall: 0.5317\n",
      "f1: 0.5737\n",
      "loss: 2.6958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 0: Loss = 1.9413\n",
      "Epoch 2, Step 10: Loss = 2.4812\n",
      "Epoch 2, Step 20: Loss = 1.5239\n",
      "Epoch 2, Step 30: Loss = 2.6381\n",
      "Epoch 2, Step 40: Loss = 5.3037\n",
      "Epoch 2, Step 50: Loss = 1.7171\n",
      "Epoch 2, Step 60: Loss = 3.0399\n",
      "Epoch 2, Step 70: Loss = 3.6979\n",
      "Epoch 2, Step 80: Loss = 3.2975\n",
      "Epoch 2, Step 90: Loss = 2.7928\n",
      "Epoch 2, Step 100: Loss = 3.3085\n",
      "Epoch 2, Step 110: Loss = 3.3500\n",
      "Epoch 2, Step 120: Loss = 4.1650\n",
      "Epoch 2, Step 130: Loss = 3.3132\n",
      "Epoch 2, Step 140: Loss = 0.9803\n",
      "Epoch 2, Step 150: Loss = 0.6280\n",
      "Epoch 2, Step 160: Loss = 1.2280\n",
      "Epoch 2, Step 170: Loss = 2.7566\n",
      "Epoch 2, Step 180: Loss = 0.4447\n",
      "Epoch 2, Step 190: Loss = 5.9158\n",
      "Epoch 2, Step 200: Loss = 0.8544\n",
      "Epoch 2, Step 210: Loss = 1.9219\n",
      "Epoch 2, Step 220: Loss = 3.8035\n",
      "Epoch 2, Step 230: Loss = 2.1817\n",
      "Epoch 2, Step 240: Loss = 0.5151\n",
      "Epoch 2, Step 250: Loss = 4.6344\n",
      "Epoch 2, Step 260: Loss = 1.8239\n",
      "Epoch 2, Step 270: Loss = 3.9730\n",
      "Epoch 2, Step 280: Loss = 3.0259\n",
      "Epoch 2, Step 290: Loss = 4.8448\n",
      "Epoch 2, Step 300: Loss = 5.3029\n",
      "Epoch 2, Step 310: Loss = 3.9383\n",
      "Epoch 2, Step 320: Loss = 0.8824\n",
      "Epoch 2, Step 330: Loss = 4.6782\n",
      "Epoch 2, Step 340: Loss = 2.2003\n",
      "Epoch 2, Step 350: Loss = 0.3518\n",
      "Epoch 2, Step 360: Loss = 0.1932\n",
      "Epoch 2, Step 370: Loss = 2.7316\n",
      "Epoch 2, Step 380: Loss = 0.1778\n",
      "Epoch 2, Step 390: Loss = 1.7486\n",
      "Epoch 2, Step 400: Loss = 1.9218\n",
      "Epoch 2, Step 410: Loss = 4.0648\n",
      "Epoch 2, Step 420: Loss = 0.8918\n",
      "Epoch 2, Step 430: Loss = 1.0697\n",
      "Epoch 2, Step 440: Loss = 2.8713\n",
      "Epoch 2, Step 450: Loss = 0.7475\n",
      "Epoch 2, Step 460: Loss = 1.8865\n",
      "Epoch 2, Step 470: Loss = 4.0079\n",
      "Epoch 2, Step 480: Loss = 0.4150\n",
      "Epoch 2, Step 490: Loss = 4.3537\n",
      "Epoch 2, Step 500: Loss = 5.0403\n",
      "Epoch 2, Step 510: Loss = 1.0816\n",
      "Epoch 2, Step 520: Loss = 0.5941\n",
      "Epoch 2, Step 530: Loss = 5.7565\n",
      "Epoch 2, Step 540: Loss = 1.2342\n",
      "Epoch 2, Step 550: Loss = 5.2188\n",
      "Epoch 2, Step 560: Loss = 2.3590\n",
      "Epoch 2, Step 570: Loss = 4.1841\n",
      "Epoch 2, Step 580: Loss = 3.3070\n",
      "Epoch 2, Step 590: Loss = 1.9117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.6229\n",
      "precision: 0.6348\n",
      "recall: 0.5787\n",
      "f1: 0.6055\n",
      "loss: 2.2871\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.6642\n",
      "precision: 0.6768\n",
      "recall: 0.6283\n",
      "f1: 0.6517\n",
      "loss: 2.1893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 0: Loss = 1.4000\n",
      "Epoch 3, Step 10: Loss = 0.1392\n",
      "Epoch 3, Step 20: Loss = 3.3958\n",
      "Epoch 3, Step 30: Loss = 2.2521\n",
      "Epoch 3, Step 40: Loss = 1.0122\n",
      "Epoch 3, Step 50: Loss = 1.2499\n",
      "Epoch 3, Step 60: Loss = 2.2842\n",
      "Epoch 3, Step 70: Loss = 2.8229\n",
      "Epoch 3, Step 80: Loss = 5.1533\n",
      "Epoch 3, Step 90: Loss = 1.5236\n",
      "Epoch 3, Step 100: Loss = 3.1277\n",
      "Epoch 3, Step 110: Loss = 1.3050\n",
      "Epoch 3, Step 120: Loss = 2.6654\n",
      "Epoch 3, Step 130: Loss = 0.7327\n",
      "Epoch 3, Step 140: Loss = 7.8349\n",
      "Epoch 3, Step 150: Loss = 2.9700\n",
      "Epoch 3, Step 160: Loss = 1.6368\n",
      "Epoch 3, Step 170: Loss = 5.3865\n",
      "Epoch 3, Step 180: Loss = 0.8436\n",
      "Epoch 3, Step 190: Loss = 4.2251\n",
      "Epoch 3, Step 200: Loss = 1.3719\n",
      "Epoch 3, Step 210: Loss = 2.9848\n",
      "Epoch 3, Step 220: Loss = 0.0521\n",
      "Epoch 3, Step 230: Loss = 3.2790\n",
      "Epoch 3, Step 240: Loss = 1.0863\n",
      "Epoch 3, Step 250: Loss = 2.7086\n",
      "Epoch 3, Step 260: Loss = 2.1116\n",
      "Epoch 3, Step 270: Loss = 3.3270\n",
      "Epoch 3, Step 280: Loss = 1.3022\n",
      "Epoch 3, Step 290: Loss = 2.3300\n",
      "Epoch 3, Step 300: Loss = 0.8773\n",
      "Epoch 3, Step 310: Loss = 0.5326\n",
      "Epoch 3, Step 320: Loss = 1.1502\n",
      "Epoch 3, Step 330: Loss = 1.9069\n",
      "Epoch 3, Step 340: Loss = 4.9899\n",
      "Epoch 3, Step 350: Loss = 0.6127\n",
      "Epoch 3, Step 360: Loss = 0.7617\n",
      "Epoch 3, Step 370: Loss = 0.6580\n",
      "Epoch 3, Step 380: Loss = 0.9136\n",
      "Epoch 3, Step 390: Loss = 1.1789\n",
      "Epoch 3, Step 400: Loss = 2.1337\n",
      "Epoch 3, Step 410: Loss = 0.8317\n",
      "Epoch 3, Step 420: Loss = 1.8633\n",
      "Epoch 3, Step 430: Loss = 3.0191\n",
      "Epoch 3, Step 440: Loss = 0.6455\n",
      "Epoch 3, Step 450: Loss = 0.2406\n",
      "Epoch 3, Step 460: Loss = 3.1960\n",
      "Epoch 3, Step 470: Loss = 5.9301\n",
      "Epoch 3, Step 480: Loss = 2.0866\n",
      "Epoch 3, Step 490: Loss = 4.0654\n",
      "Epoch 3, Step 500: Loss = 0.4082\n",
      "Epoch 3, Step 510: Loss = 0.4686\n",
      "Epoch 3, Step 520: Loss = 3.7442\n",
      "Epoch 3, Step 530: Loss = 3.9272\n",
      "Epoch 3, Step 540: Loss = 0.5227\n",
      "Epoch 3, Step 550: Loss = 4.2525\n",
      "Epoch 3, Step 560: Loss = 2.0472\n",
      "Epoch 3, Step 570: Loss = 2.3941\n",
      "Epoch 3, Step 580: Loss = 0.3903\n",
      "Epoch 3, Step 590: Loss = 4.1538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.6642\n",
      "precision: 0.6761\n",
      "recall: 0.6304\n",
      "f1: 0.6524\n",
      "loss: 1.9498\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.6900\n",
      "precision: 0.7050\n",
      "recall: 0.6533\n",
      "f1: 0.6782\n",
      "loss: 1.9778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 0: Loss = 1.5301\n",
      "Epoch 4, Step 10: Loss = 3.0882\n",
      "Epoch 4, Step 20: Loss = 0.4124\n",
      "Epoch 4, Step 30: Loss = 1.5541\n",
      "Epoch 4, Step 40: Loss = 2.1682\n",
      "Epoch 4, Step 50: Loss = 0.5299\n",
      "Epoch 4, Step 60: Loss = 2.0470\n",
      "Epoch 4, Step 70: Loss = 1.5550\n",
      "Epoch 4, Step 80: Loss = 1.0974\n",
      "Epoch 4, Step 90: Loss = 1.3561\n",
      "Epoch 4, Step 100: Loss = 2.6563\n",
      "Epoch 4, Step 110: Loss = 3.5315\n",
      "Epoch 4, Step 120: Loss = 1.8248\n",
      "Epoch 4, Step 130: Loss = 0.1975\n",
      "Epoch 4, Step 140: Loss = 2.5466\n",
      "Epoch 4, Step 150: Loss = 0.4991\n",
      "Epoch 4, Step 160: Loss = 2.0889\n",
      "Epoch 4, Step 170: Loss = 2.7611\n",
      "Epoch 4, Step 180: Loss = 4.0551\n",
      "Epoch 4, Step 190: Loss = 3.1772\n",
      "Epoch 4, Step 200: Loss = 2.6787\n",
      "Epoch 4, Step 210: Loss = 2.3296\n",
      "Epoch 4, Step 220: Loss = 1.3420\n",
      "Epoch 4, Step 230: Loss = 3.0670\n",
      "Epoch 4, Step 240: Loss = 4.6900\n",
      "Epoch 4, Step 250: Loss = 5.7090\n",
      "Epoch 4, Step 260: Loss = 0.9868\n",
      "Epoch 4, Step 270: Loss = 0.5616\n",
      "Epoch 4, Step 280: Loss = 0.1240\n",
      "Epoch 4, Step 290: Loss = 0.3602\n",
      "Epoch 4, Step 300: Loss = 1.4027\n",
      "Epoch 4, Step 310: Loss = 2.8088\n",
      "Epoch 4, Step 320: Loss = 3.2643\n",
      "Epoch 4, Step 330: Loss = 2.6349\n",
      "Epoch 4, Step 340: Loss = 4.3105\n",
      "Epoch 4, Step 350: Loss = 3.3705\n",
      "Epoch 4, Step 360: Loss = 1.4640\n",
      "Epoch 4, Step 370: Loss = 0.2838\n",
      "Epoch 4, Step 380: Loss = 0.1442\n",
      "Epoch 4, Step 390: Loss = 0.0424\n",
      "Epoch 4, Step 400: Loss = 1.6172\n",
      "Epoch 4, Step 410: Loss = 2.3196\n",
      "Epoch 4, Step 420: Loss = 2.9875\n",
      "Epoch 4, Step 430: Loss = 4.1098\n",
      "Epoch 4, Step 440: Loss = 1.5313\n",
      "Epoch 4, Step 450: Loss = 3.5887\n",
      "Epoch 4, Step 460: Loss = 0.5592\n",
      "Epoch 4, Step 470: Loss = 2.8422\n",
      "Epoch 4, Step 480: Loss = 0.2204\n",
      "Epoch 4, Step 490: Loss = 0.0871\n",
      "Epoch 4, Step 500: Loss = 4.3645\n",
      "Epoch 4, Step 510: Loss = 0.3516\n",
      "Epoch 4, Step 520: Loss = 1.9703\n",
      "Epoch 4, Step 530: Loss = 1.7950\n",
      "Epoch 4, Step 540: Loss = 3.5801\n",
      "Epoch 4, Step 550: Loss = 4.0422\n",
      "Epoch 4, Step 560: Loss = 1.5959\n",
      "Epoch 4, Step 570: Loss = 0.4429\n",
      "Epoch 4, Step 580: Loss = 2.0297\n",
      "Epoch 4, Step 590: Loss = 1.6155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.6860\n",
      "precision: 0.6975\n",
      "recall: 0.6571\n",
      "f1: 0.6767\n",
      "loss: 1.7973\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7042\n",
      "precision: 0.7199\n",
      "recall: 0.6683\n",
      "f1: 0.6932\n",
      "loss: 1.8820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 1.3089\n",
      "Epoch 5, Step 10: Loss = 0.9341\n",
      "Epoch 5, Step 20: Loss = 2.7797\n",
      "Epoch 5, Step 30: Loss = 0.8017\n",
      "Epoch 5, Step 40: Loss = 1.3581\n",
      "Epoch 5, Step 50: Loss = 1.0660\n",
      "Epoch 5, Step 60: Loss = 3.0621\n",
      "Epoch 5, Step 70: Loss = 2.2621\n",
      "Epoch 5, Step 80: Loss = 5.1549\n",
      "Epoch 5, Step 90: Loss = 0.0405\n",
      "Epoch 5, Step 100: Loss = 1.0704\n",
      "Epoch 5, Step 110: Loss = 3.0277\n",
      "Epoch 5, Step 120: Loss = 0.6154\n",
      "Epoch 5, Step 130: Loss = 1.4377\n",
      "Epoch 5, Step 140: Loss = 1.6356\n",
      "Epoch 5, Step 150: Loss = 0.5097\n",
      "Epoch 5, Step 160: Loss = 3.6896\n",
      "Epoch 5, Step 170: Loss = 1.0269\n",
      "Epoch 5, Step 180: Loss = 4.1624\n",
      "Epoch 5, Step 190: Loss = 1.8021\n",
      "Epoch 5, Step 200: Loss = 4.2829\n",
      "Epoch 5, Step 210: Loss = 0.5094\n",
      "Epoch 5, Step 220: Loss = 3.2533\n",
      "Epoch 5, Step 230: Loss = 5.0907\n",
      "Epoch 5, Step 240: Loss = 3.2730\n",
      "Epoch 5, Step 250: Loss = 4.4665\n",
      "Epoch 5, Step 260: Loss = 2.3833\n",
      "Epoch 5, Step 270: Loss = 0.4159\n",
      "Epoch 5, Step 280: Loss = 1.4485\n",
      "Epoch 5, Step 290: Loss = 1.3861\n",
      "Epoch 5, Step 300: Loss = 2.5238\n",
      "Epoch 5, Step 310: Loss = 2.4005\n",
      "Epoch 5, Step 320: Loss = 1.3781\n",
      "Epoch 5, Step 330: Loss = 0.6868\n",
      "Epoch 5, Step 340: Loss = 2.8613\n",
      "Epoch 5, Step 350: Loss = 1.1254\n",
      "Epoch 5, Step 360: Loss = 2.1675\n",
      "Epoch 5, Step 370: Loss = 3.2656\n",
      "Epoch 5, Step 380: Loss = 2.0195\n",
      "Epoch 5, Step 390: Loss = 0.7615\n",
      "Epoch 5, Step 400: Loss = 1.0546\n",
      "Epoch 5, Step 410: Loss = 2.9576\n",
      "Epoch 5, Step 420: Loss = 2.6068\n",
      "Epoch 5, Step 430: Loss = 0.3306\n",
      "Epoch 5, Step 440: Loss = 1.2905\n",
      "Epoch 5, Step 450: Loss = 3.5223\n",
      "Epoch 5, Step 460: Loss = 1.7403\n",
      "Epoch 5, Step 470: Loss = 0.7467\n",
      "Epoch 5, Step 480: Loss = 1.1911\n",
      "Epoch 5, Step 490: Loss = 3.6706\n",
      "Epoch 5, Step 500: Loss = 2.4032\n",
      "Epoch 5, Step 510: Loss = 0.5760\n",
      "Epoch 5, Step 520: Loss = 1.4805\n",
      "Epoch 5, Step 530: Loss = 0.0355\n",
      "Epoch 5, Step 540: Loss = 1.1025\n",
      "Epoch 5, Step 550: Loss = 2.4960\n",
      "Epoch 5, Step 560: Loss = 1.1344\n",
      "Epoch 5, Step 570: Loss = 0.4983\n",
      "Epoch 5, Step 580: Loss = 1.7808\n",
      "Epoch 5, Step 590: Loss = 3.7365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.6963\n",
      "precision: 0.7077\n",
      "recall: 0.6687\n",
      "f1: 0.6877\n",
      "loss: 1.7224\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7050\n",
      "precision: 0.7204\n",
      "recall: 0.6700\n",
      "f1: 0.6943\n",
      "loss: 1.8372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Step 0: Loss = 2.2669\n",
      "Epoch 6, Step 10: Loss = 0.3102\n",
      "Epoch 6, Step 20: Loss = 2.0786\n",
      "Epoch 6, Step 30: Loss = 2.0154\n",
      "Epoch 6, Step 40: Loss = 4.1097\n",
      "Epoch 6, Step 50: Loss = 2.5131\n",
      "Epoch 6, Step 60: Loss = 0.5913\n",
      "Epoch 6, Step 70: Loss = 1.0862\n",
      "Epoch 6, Step 80: Loss = 1.9338\n",
      "Epoch 6, Step 90: Loss = 1.4814\n",
      "Epoch 6, Step 100: Loss = 1.4745\n",
      "Epoch 6, Step 110: Loss = 2.0235\n",
      "Epoch 6, Step 120: Loss = 1.2674\n",
      "Epoch 6, Step 130: Loss = 1.0243\n",
      "Epoch 6, Step 140: Loss = 0.9619\n",
      "Epoch 6, Step 150: Loss = 2.0210\n",
      "Epoch 6, Step 160: Loss = 1.2441\n",
      "Epoch 6, Step 170: Loss = 3.8975\n",
      "Epoch 6, Step 180: Loss = 0.2593\n",
      "Epoch 6, Step 190: Loss = 0.4106\n",
      "Epoch 6, Step 200: Loss = 1.5869\n",
      "Epoch 6, Step 210: Loss = 0.2347\n",
      "Epoch 6, Step 220: Loss = 1.5681\n",
      "Epoch 6, Step 230: Loss = 2.6996\n",
      "Epoch 6, Step 240: Loss = 2.5990\n",
      "Epoch 6, Step 250: Loss = 0.1486\n",
      "Epoch 6, Step 260: Loss = 2.5933\n",
      "Epoch 6, Step 270: Loss = 2.5614\n",
      "Epoch 6, Step 280: Loss = 1.1181\n",
      "Epoch 6, Step 290: Loss = 2.2565\n",
      "Epoch 6, Step 300: Loss = 1.2707\n",
      "Epoch 6, Step 310: Loss = 1.4937\n",
      "Epoch 6, Step 320: Loss = 1.2773\n",
      "Epoch 6, Step 330: Loss = 1.9934\n",
      "Epoch 6, Step 340: Loss = 0.9114\n",
      "Epoch 6, Step 350: Loss = 4.3699\n",
      "Epoch 6, Step 360: Loss = 2.4030\n",
      "Epoch 6, Step 370: Loss = 1.6715\n",
      "Epoch 6, Step 380: Loss = 0.2633\n",
      "Epoch 6, Step 390: Loss = 1.4398\n",
      "Epoch 6, Step 400: Loss = 1.2691\n",
      "Epoch 6, Step 410: Loss = 6.4261\n",
      "Epoch 6, Step 420: Loss = 0.1023\n",
      "Epoch 6, Step 430: Loss = 0.5602\n",
      "Epoch 6, Step 440: Loss = 1.4770\n",
      "Epoch 6, Step 450: Loss = 1.5897\n",
      "Epoch 6, Step 460: Loss = 2.9112\n",
      "Epoch 6, Step 470: Loss = 3.2873\n",
      "Epoch 6, Step 480: Loss = 0.9058\n",
      "Epoch 6, Step 490: Loss = 0.7386\n",
      "Epoch 6, Step 500: Loss = 2.9032\n",
      "Epoch 6, Step 510: Loss = 4.6619\n",
      "Epoch 6, Step 520: Loss = 1.1620\n",
      "Epoch 6, Step 530: Loss = 4.1421\n",
      "Epoch 6, Step 540: Loss = 0.0949\n",
      "Epoch 6, Step 550: Loss = 1.8415\n",
      "Epoch 6, Step 560: Loss = 3.5937\n",
      "Epoch 6, Step 570: Loss = 0.7717\n",
      "Epoch 6, Step 580: Loss = 1.8924\n",
      "Epoch 6, Step 590: Loss = 0.7406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.6994\n",
      "precision: 0.7135\n",
      "recall: 0.6663\n",
      "f1: 0.6891\n",
      "loss: 1.7002\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7083\n",
      "precision: 0.7232\n",
      "recall: 0.6750\n",
      "f1: 0.6983\n",
      "loss: 1.8131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Step 0: Loss = 1.3895\n",
      "Epoch 7, Step 10: Loss = 1.6943\n",
      "Epoch 7, Step 20: Loss = 0.3569\n",
      "Epoch 7, Step 30: Loss = 1.8910\n",
      "Epoch 7, Step 40: Loss = 2.9507\n",
      "Epoch 7, Step 50: Loss = 1.9359\n",
      "Epoch 7, Step 60: Loss = 1.8504\n",
      "Epoch 7, Step 70: Loss = 0.0070\n",
      "Epoch 7, Step 80: Loss = 1.1214\n",
      "Epoch 7, Step 90: Loss = 2.1966\n",
      "Epoch 7, Step 100: Loss = 2.3299\n",
      "Epoch 7, Step 110: Loss = 0.1551\n",
      "Epoch 7, Step 120: Loss = 1.4898\n",
      "Epoch 7, Step 130: Loss = 0.4344\n",
      "Epoch 7, Step 140: Loss = 2.2253\n",
      "Epoch 7, Step 150: Loss = 1.9971\n",
      "Epoch 7, Step 160: Loss = 1.0763\n",
      "Epoch 7, Step 170: Loss = 1.9201\n",
      "Epoch 7, Step 180: Loss = 1.7144\n",
      "Epoch 7, Step 190: Loss = 3.5872\n",
      "Epoch 7, Step 200: Loss = 0.9302\n",
      "Epoch 7, Step 210: Loss = 3.0224\n",
      "Epoch 7, Step 220: Loss = 1.7220\n",
      "Epoch 7, Step 230: Loss = 1.5429\n",
      "Epoch 7, Step 240: Loss = 4.9280\n",
      "Epoch 7, Step 250: Loss = 0.9017\n",
      "Epoch 7, Step 260: Loss = 0.6735\n",
      "Epoch 7, Step 270: Loss = 0.8544\n",
      "Epoch 7, Step 280: Loss = 1.6476\n",
      "Epoch 7, Step 290: Loss = 3.0776\n",
      "Epoch 7, Step 300: Loss = 1.6919\n",
      "Epoch 7, Step 310: Loss = 2.1404\n",
      "Epoch 7, Step 320: Loss = 0.9050\n",
      "Epoch 7, Step 330: Loss = 3.0562\n",
      "Epoch 7, Step 340: Loss = 3.9004\n",
      "Epoch 7, Step 350: Loss = 1.1645\n",
      "Epoch 7, Step 360: Loss = 2.3947\n",
      "Epoch 7, Step 370: Loss = 1.8372\n",
      "Epoch 7, Step 380: Loss = 0.2537\n",
      "Epoch 7, Step 390: Loss = 0.7131\n",
      "Epoch 7, Step 400: Loss = 1.4967\n",
      "Epoch 7, Step 410: Loss = 0.5043\n",
      "Epoch 7, Step 420: Loss = 1.7553\n",
      "Epoch 7, Step 430: Loss = 0.8514\n",
      "Epoch 7, Step 440: Loss = 1.3185\n",
      "Epoch 7, Step 450: Loss = 1.7180\n",
      "Epoch 7, Step 460: Loss = 0.8734\n",
      "Epoch 7, Step 470: Loss = 1.4554\n",
      "Epoch 7, Step 480: Loss = 0.7975\n",
      "Epoch 7, Step 490: Loss = 0.4561\n",
      "Epoch 7, Step 500: Loss = 0.0561\n",
      "Epoch 7, Step 510: Loss = 2.4417\n",
      "Epoch 7, Step 520: Loss = 2.3415\n",
      "Epoch 7, Step 530: Loss = 1.9211\n",
      "Epoch 7, Step 540: Loss = 5.4970\n",
      "Epoch 7, Step 550: Loss = 2.0768\n",
      "Epoch 7, Step 560: Loss = 3.3040\n",
      "Epoch 7, Step 570: Loss = 2.4005\n",
      "Epoch 7, Step 580: Loss = 2.8952\n",
      "Epoch 7, Step 590: Loss = 0.3816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7010\n",
      "precision: 0.7130\n",
      "recall: 0.6729\n",
      "f1: 0.6924\n",
      "loss: 1.6923\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7092\n",
      "precision: 0.7237\n",
      "recall: 0.6767\n",
      "f1: 0.6994\n",
      "loss: 1.8021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Step 0: Loss = 0.3737\n",
      "Epoch 8, Step 10: Loss = 2.4846\n",
      "Epoch 8, Step 20: Loss = 0.7235\n",
      "Epoch 8, Step 30: Loss = 1.1161\n",
      "Epoch 8, Step 40: Loss = 2.6707\n",
      "Epoch 8, Step 50: Loss = 0.0171\n",
      "Epoch 8, Step 60: Loss = 1.8811\n",
      "Epoch 8, Step 70: Loss = 4.2377\n",
      "Epoch 8, Step 80: Loss = 0.6847\n",
      "Epoch 8, Step 90: Loss = 0.8486\n",
      "Epoch 8, Step 100: Loss = 0.3856\n",
      "Epoch 8, Step 110: Loss = 0.8021\n",
      "Epoch 8, Step 120: Loss = 1.0815\n",
      "Epoch 8, Step 130: Loss = 1.0936\n",
      "Epoch 8, Step 140: Loss = 1.1237\n",
      "Epoch 8, Step 150: Loss = 2.7276\n",
      "Epoch 8, Step 160: Loss = 2.0111\n",
      "Epoch 8, Step 170: Loss = 0.5039\n",
      "Epoch 8, Step 180: Loss = 0.4455\n",
      "Epoch 8, Step 190: Loss = 3.4320\n",
      "Epoch 8, Step 200: Loss = 2.1549\n",
      "Epoch 8, Step 210: Loss = 0.2831\n",
      "Epoch 8, Step 220: Loss = 3.7046\n",
      "Epoch 8, Step 230: Loss = 3.3712\n",
      "Epoch 8, Step 240: Loss = 2.6997\n",
      "Epoch 8, Step 250: Loss = 1.5833\n",
      "Epoch 8, Step 260: Loss = 0.2736\n",
      "Epoch 8, Step 270: Loss = 0.3743\n",
      "Epoch 8, Step 280: Loss = 0.8986\n",
      "Epoch 8, Step 290: Loss = 0.9888\n",
      "Epoch 8, Step 300: Loss = 2.0162\n",
      "Epoch 8, Step 310: Loss = 0.2720\n",
      "Epoch 8, Step 320: Loss = 1.8321\n",
      "Epoch 8, Step 330: Loss = 2.1707\n",
      "Epoch 8, Step 340: Loss = 0.4396\n",
      "Epoch 8, Step 350: Loss = 1.5125\n",
      "Epoch 8, Step 360: Loss = 0.6169\n",
      "Epoch 8, Step 370: Loss = 1.4500\n",
      "Epoch 8, Step 380: Loss = 3.7299\n",
      "Epoch 8, Step 390: Loss = 2.6679\n",
      "Epoch 8, Step 400: Loss = 4.0995\n",
      "Epoch 8, Step 410: Loss = 1.6306\n",
      "Epoch 8, Step 420: Loss = 2.5776\n",
      "Epoch 8, Step 430: Loss = 0.3726\n",
      "Epoch 8, Step 440: Loss = 0.2180\n",
      "Epoch 8, Step 450: Loss = 1.6279\n",
      "Epoch 8, Step 460: Loss = 0.9609\n",
      "Epoch 8, Step 470: Loss = 2.1170\n",
      "Epoch 8, Step 480: Loss = 0.5160\n",
      "Epoch 8, Step 490: Loss = 2.5879\n",
      "Epoch 8, Step 500: Loss = 4.9352\n",
      "Epoch 8, Step 510: Loss = 1.9390\n",
      "Epoch 8, Step 520: Loss = 1.0813\n",
      "Epoch 8, Step 530: Loss = 2.7516\n",
      "Epoch 8, Step 540: Loss = 1.8517\n",
      "Epoch 8, Step 550: Loss = 0.9664\n",
      "Epoch 8, Step 560: Loss = 0.6511\n",
      "Epoch 8, Step 570: Loss = 2.1558\n",
      "Epoch 8, Step 580: Loss = 0.7289\n",
      "Epoch 8, Step 590: Loss = 3.6446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7013\n",
      "precision: 0.7135\n",
      "recall: 0.6725\n",
      "f1: 0.6924\n",
      "loss: 1.6748\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7092\n",
      "precision: 0.7237\n",
      "recall: 0.6767\n",
      "f1: 0.6994\n",
      "loss: 1.7958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 0: Loss = 1.4255\n",
      "Epoch 9, Step 10: Loss = 2.3865\n",
      "Epoch 9, Step 20: Loss = 2.9377\n",
      "Epoch 9, Step 30: Loss = 4.7885\n",
      "Epoch 9, Step 40: Loss = 2.4532\n",
      "Epoch 9, Step 50: Loss = 4.8548\n",
      "Epoch 9, Step 60: Loss = 1.7188\n",
      "Epoch 9, Step 70: Loss = 2.7863\n",
      "Epoch 9, Step 80: Loss = 0.0060\n",
      "Epoch 9, Step 90: Loss = 2.9754\n",
      "Epoch 9, Step 100: Loss = 1.7123\n",
      "Epoch 9, Step 110: Loss = 2.5666\n",
      "Epoch 9, Step 120: Loss = 1.0871\n",
      "Epoch 9, Step 130: Loss = 0.5703\n",
      "Epoch 9, Step 140: Loss = 2.5477\n",
      "Epoch 9, Step 150: Loss = 0.1591\n",
      "Epoch 9, Step 160: Loss = 1.8331\n",
      "Epoch 9, Step 170: Loss = 3.2147\n",
      "Epoch 9, Step 180: Loss = 1.9149\n",
      "Epoch 9, Step 190: Loss = 0.0095\n",
      "Epoch 9, Step 200: Loss = 0.1361\n",
      "Epoch 9, Step 210: Loss = 2.4796\n",
      "Epoch 9, Step 220: Loss = 1.2056\n",
      "Epoch 9, Step 230: Loss = 0.1292\n",
      "Epoch 9, Step 240: Loss = 1.7142\n",
      "Epoch 9, Step 250: Loss = 1.5926\n",
      "Epoch 9, Step 260: Loss = 1.4080\n",
      "Epoch 9, Step 270: Loss = 1.3901\n",
      "Epoch 9, Step 280: Loss = 0.5340\n",
      "Epoch 9, Step 290: Loss = 1.4478\n",
      "Epoch 9, Step 300: Loss = 2.3711\n",
      "Epoch 9, Step 310: Loss = 0.3911\n",
      "Epoch 9, Step 320: Loss = 1.4969\n",
      "Epoch 9, Step 330: Loss = 0.5914\n",
      "Epoch 9, Step 340: Loss = 2.4760\n",
      "Epoch 9, Step 350: Loss = 1.0496\n",
      "Epoch 9, Step 360: Loss = 0.6223\n",
      "Epoch 9, Step 370: Loss = 3.1779\n",
      "Epoch 9, Step 380: Loss = 1.8637\n",
      "Epoch 9, Step 390: Loss = 4.0342\n",
      "Epoch 9, Step 400: Loss = 0.2047\n",
      "Epoch 9, Step 410: Loss = 0.3880\n",
      "Epoch 9, Step 420: Loss = 0.9314\n",
      "Epoch 9, Step 430: Loss = 3.1658\n",
      "Epoch 9, Step 440: Loss = 2.7889\n",
      "Epoch 9, Step 450: Loss = 1.0307\n",
      "Epoch 9, Step 460: Loss = 0.7343\n",
      "Epoch 9, Step 470: Loss = 2.5865\n",
      "Epoch 9, Step 480: Loss = 1.8151\n",
      "Epoch 9, Step 490: Loss = 0.5299\n",
      "Epoch 9, Step 500: Loss = 0.6846\n",
      "Epoch 9, Step 510: Loss = 1.6864\n",
      "Epoch 9, Step 520: Loss = 1.5720\n",
      "Epoch 9, Step 530: Loss = 0.8848\n",
      "Epoch 9, Step 540: Loss = 0.8454\n",
      "Epoch 9, Step 550: Loss = 0.8889\n",
      "Epoch 9, Step 560: Loss = 0.7394\n",
      "Epoch 9, Step 570: Loss = 0.2702\n",
      "Epoch 9, Step 580: Loss = 1.7702\n",
      "Epoch 9, Step 590: Loss = 0.5535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7037\n",
      "precision: 0.7164\n",
      "recall: 0.6746\n",
      "f1: 0.6948\n",
      "loss: 1.6658\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7092\n",
      "precision: 0.7237\n",
      "recall: 0.6767\n",
      "f1: 0.6994\n",
      "loss: 1.7930\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (AutoTokenizer,AutoModelForSequenceClassification,\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "                           BitsAndBytesConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        self.emails_df = emails_df.copy()\n",
    "        self.emails_df['sender'] = self.emails_df['sender'].apply(clean_text)\n",
    "        self.emails_df['subject'] = self.emails_df['subject'].apply(clean_text)\n",
    "        self.emails_df['body'] = self.emails_df['body'].apply(clean_text)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        email = self.emails_df.iloc[idx]\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(email['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    #text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    #text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model_config.use_cache = False\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, output_dir):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "    \n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    return metrics\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9):\n",
    "    best_val_metrics = {'f1': 0}\n",
    "    best_model_state = None\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % 2 == 0:  # Gradient accumulation steps = 2\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(batch['label'].cpu().numpy())\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "  \n",
    "        train_metrics = compute_metrics(train_preds, train_labels)\n",
    "        train_metrics['loss'] = total_loss / len(train_loader)\n",
    "      \n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "      \n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(\"Training Metrics:\")\n",
    "        for metric, value in train_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        for metric, value in val_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_val_metrics['f1']:\n",
    "            best_val_metrics = val_metrics\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return best_model_state, best_val_metrics\n",
    "\n",
    "def main():\n",
    "    login(token=\"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\")\n",
    "    device = setup_environment()\n",
    "    model_name = 'dreamgen/WizardLM-2-7B'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/newdata_cleaned.csv\")\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "    \n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    \n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "    \n",
    "    train_dataset = EmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = EmailDataset(val_df, tokenizer, max_length=512)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    best_model_state, best_metrics = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/binary_classification_wiz_revised\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "   \n",
    "    \n",
    "  \n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"best_metrics\": {k: float(v) if k != 'confusion_matrix' else v.tolist() \n",
    "                        for k, v in best_metrics.items()}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "669ee271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n",
      "GPU Memory: 23.68 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea0182dd434422a98e12fb3a577986c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 11.0144\n",
      "Epoch 1, Step 10: Loss = 4.4789\n",
      "Epoch 1, Step 20: Loss = 6.2724\n",
      "Epoch 1, Step 30: Loss = 5.7053\n",
      "Epoch 1, Step 40: Loss = 3.4180\n",
      "Epoch 1, Step 50: Loss = 6.9233\n",
      "Epoch 1, Step 60: Loss = 6.6618\n",
      "Epoch 1, Step 70: Loss = 4.4406\n",
      "Epoch 1, Step 80: Loss = 8.4976\n",
      "Epoch 1, Step 90: Loss = 4.7993\n",
      "Epoch 1, Step 100: Loss = 3.6008\n",
      "Epoch 1, Step 110: Loss = 4.1995\n",
      "Epoch 1, Step 120: Loss = 5.5875\n",
      "Epoch 1, Step 130: Loss = 2.2347\n",
      "Epoch 1, Step 140: Loss = 2.4018\n",
      "Epoch 1, Step 150: Loss = 4.9808\n",
      "Epoch 1, Step 160: Loss = 3.3161\n",
      "Epoch 1, Step 170: Loss = 5.2373\n",
      "Epoch 1, Step 180: Loss = 1.7471\n",
      "Epoch 1, Step 190: Loss = 11.8584\n",
      "Epoch 1, Step 200: Loss = 6.1159\n",
      "Epoch 1, Step 210: Loss = 3.9472\n",
      "Epoch 1, Step 220: Loss = 4.5249\n",
      "Epoch 1, Step 230: Loss = 2.9620\n",
      "Epoch 1, Step 240: Loss = 6.5698\n",
      "Epoch 1, Step 250: Loss = 4.3863\n",
      "Epoch 1, Step 260: Loss = 10.9907\n",
      "Epoch 1, Step 270: Loss = 3.9604\n",
      "Epoch 1, Step 280: Loss = 5.9834\n",
      "Epoch 1, Step 290: Loss = 6.6325\n",
      "Epoch 1, Step 300: Loss = 6.0473\n",
      "Epoch 1, Step 310: Loss = 6.6348\n",
      "Epoch 1, Step 320: Loss = 5.3896\n",
      "Epoch 1, Step 330: Loss = 0.9398\n",
      "Epoch 1, Step 340: Loss = 5.0894\n",
      "Epoch 1, Step 350: Loss = 2.7460\n",
      "Epoch 1, Step 360: Loss = 4.4464\n",
      "Epoch 1, Step 370: Loss = 3.2895\n",
      "Epoch 1, Step 380: Loss = 9.4342\n",
      "Epoch 1, Step 390: Loss = 5.0468\n",
      "Epoch 1, Step 400: Loss = 4.8040\n",
      "Epoch 1, Step 410: Loss = 7.0733\n",
      "Epoch 1, Step 420: Loss = 3.9225\n",
      "Epoch 1, Step 430: Loss = 4.3915\n",
      "Epoch 1, Step 440: Loss = 2.3993\n",
      "Epoch 1, Step 450: Loss = 0.4672\n",
      "Epoch 1, Step 460: Loss = 3.2664\n",
      "Epoch 1, Step 470: Loss = 3.2373\n",
      "Epoch 1, Step 480: Loss = 4.8432\n",
      "Epoch 1, Step 490: Loss = 3.5789\n",
      "Epoch 1, Step 500: Loss = 4.4595\n",
      "Epoch 1, Step 510: Loss = 5.8770\n",
      "Epoch 1, Step 520: Loss = 3.9045\n",
      "Epoch 1, Step 530: Loss = 2.1190\n",
      "Epoch 1, Step 540: Loss = 2.1631\n",
      "Epoch 1, Step 550: Loss = 3.5123\n",
      "Epoch 1, Step 560: Loss = 1.3548\n",
      "Epoch 1, Step 570: Loss = 5.1119\n",
      "Epoch 1, Step 580: Loss = 5.0958\n",
      "Epoch 1, Step 590: Loss = 1.5086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.5215\n",
      "precision: 0.5122\n",
      "recall: 0.9008\n",
      "f1: 0.6531\n",
      "loss: 4.5172\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.5633\n",
      "precision: 0.5447\n",
      "recall: 0.7717\n",
      "f1: 0.6386\n",
      "loss: 2.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 0: Loss = 6.1672\n",
      "Epoch 2, Step 10: Loss = 2.3898\n",
      "Epoch 2, Step 20: Loss = 3.3454\n",
      "Epoch 2, Step 30: Loss = 3.7459\n",
      "Epoch 2, Step 40: Loss = 4.1902\n",
      "Epoch 2, Step 50: Loss = 0.8775\n",
      "Epoch 2, Step 60: Loss = 1.3822\n",
      "Epoch 2, Step 70: Loss = 2.9451\n",
      "Epoch 2, Step 80: Loss = 2.6739\n",
      "Epoch 2, Step 90: Loss = 4.2030\n",
      "Epoch 2, Step 100: Loss = 3.5155\n",
      "Epoch 2, Step 110: Loss = 0.0948\n",
      "Epoch 2, Step 120: Loss = 2.0932\n",
      "Epoch 2, Step 130: Loss = 1.3959\n",
      "Epoch 2, Step 140: Loss = 1.4338\n",
      "Epoch 2, Step 150: Loss = 1.6439\n",
      "Epoch 2, Step 160: Loss = 0.0248\n",
      "Epoch 2, Step 170: Loss = 2.1752\n",
      "Epoch 2, Step 180: Loss = 0.4570\n",
      "Epoch 2, Step 190: Loss = 1.3965\n",
      "Epoch 2, Step 200: Loss = 0.4177\n",
      "Epoch 2, Step 210: Loss = 1.9337\n",
      "Epoch 2, Step 220: Loss = 0.3697\n",
      "Epoch 2, Step 230: Loss = 3.8065\n",
      "Epoch 2, Step 240: Loss = 0.6871\n",
      "Epoch 2, Step 250: Loss = 5.1951\n",
      "Epoch 2, Step 260: Loss = 1.7572\n",
      "Epoch 2, Step 270: Loss = 3.6431\n",
      "Epoch 2, Step 280: Loss = 1.1843\n",
      "Epoch 2, Step 290: Loss = 0.6126\n",
      "Epoch 2, Step 300: Loss = 2.8901\n",
      "Epoch 2, Step 310: Loss = 3.7516\n",
      "Epoch 2, Step 320: Loss = 1.4901\n",
      "Epoch 2, Step 330: Loss = 1.3191\n",
      "Epoch 2, Step 340: Loss = 1.1332\n",
      "Epoch 2, Step 350: Loss = 3.4969\n",
      "Epoch 2, Step 360: Loss = 1.5883\n",
      "Epoch 2, Step 370: Loss = 1.5289\n",
      "Epoch 2, Step 380: Loss = 2.4414\n",
      "Epoch 2, Step 390: Loss = 1.1097\n",
      "Epoch 2, Step 400: Loss = 2.5866\n",
      "Epoch 2, Step 410: Loss = 0.7642\n",
      "Epoch 2, Step 420: Loss = 1.1791\n",
      "Epoch 2, Step 430: Loss = 0.4521\n",
      "Epoch 2, Step 440: Loss = 1.4786\n",
      "Epoch 2, Step 450: Loss = 0.2493\n",
      "Epoch 2, Step 460: Loss = 3.3298\n",
      "Epoch 2, Step 470: Loss = 3.8040\n",
      "Epoch 2, Step 480: Loss = 0.8866\n",
      "Epoch 2, Step 490: Loss = 2.0204\n",
      "Epoch 2, Step 500: Loss = 1.9143\n",
      "Epoch 2, Step 510: Loss = 1.4016\n",
      "Epoch 2, Step 520: Loss = 2.6464\n",
      "Epoch 2, Step 530: Loss = 1.8772\n",
      "Epoch 2, Step 540: Loss = 1.5394\n",
      "Epoch 2, Step 550: Loss = 2.1044\n",
      "Epoch 2, Step 560: Loss = 1.3694\n",
      "Epoch 2, Step 570: Loss = 0.4886\n",
      "Epoch 2, Step 580: Loss = 0.5970\n",
      "Epoch 2, Step 590: Loss = 0.7810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.6410\n",
      "precision: 0.6156\n",
      "recall: 0.7508\n",
      "f1: 0.6766\n",
      "loss: 1.9832\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.6750\n",
      "precision: 0.6586\n",
      "recall: 0.7267\n",
      "f1: 0.6910\n",
      "loss: 1.6652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 0: Loss = 1.2152\n",
      "Epoch 3, Step 10: Loss = 1.6102\n",
      "Epoch 3, Step 20: Loss = 1.9743\n",
      "Epoch 3, Step 30: Loss = 3.6414\n",
      "Epoch 3, Step 40: Loss = 1.1431\n",
      "Epoch 3, Step 50: Loss = 1.0116\n",
      "Epoch 3, Step 60: Loss = 1.0044\n",
      "Epoch 3, Step 70: Loss = 0.7341\n",
      "Epoch 3, Step 80: Loss = 2.2380\n",
      "Epoch 3, Step 90: Loss = 1.1697\n",
      "Epoch 3, Step 100: Loss = 3.9205\n",
      "Epoch 3, Step 110: Loss = 0.4992\n",
      "Epoch 3, Step 120: Loss = 0.8348\n",
      "Epoch 3, Step 130: Loss = 0.4732\n",
      "Epoch 3, Step 140: Loss = 0.0052\n",
      "Epoch 3, Step 150: Loss = 2.7847\n",
      "Epoch 3, Step 160: Loss = 0.7133\n",
      "Epoch 3, Step 170: Loss = 2.1583\n",
      "Epoch 3, Step 180: Loss = 1.5500\n",
      "Epoch 3, Step 190: Loss = 2.6846\n",
      "Epoch 3, Step 200: Loss = 0.9637\n",
      "Epoch 3, Step 210: Loss = 3.0236\n",
      "Epoch 3, Step 220: Loss = 4.0855\n",
      "Epoch 3, Step 230: Loss = 3.1547\n",
      "Epoch 3, Step 240: Loss = 0.4734\n",
      "Epoch 3, Step 250: Loss = 1.0515\n",
      "Epoch 3, Step 260: Loss = 0.6187\n",
      "Epoch 3, Step 270: Loss = 0.9076\n",
      "Epoch 3, Step 280: Loss = 0.0932\n",
      "Epoch 3, Step 290: Loss = 1.2509\n",
      "Epoch 3, Step 300: Loss = 0.5827\n",
      "Epoch 3, Step 310: Loss = 1.4557\n",
      "Epoch 3, Step 320: Loss = 0.7734\n",
      "Epoch 3, Step 330: Loss = 0.2857\n",
      "Epoch 3, Step 340: Loss = 0.8903\n",
      "Epoch 3, Step 350: Loss = 0.9725\n",
      "Epoch 3, Step 360: Loss = 0.8483\n",
      "Epoch 3, Step 370: Loss = 0.0241\n",
      "Epoch 3, Step 380: Loss = 0.6392\n",
      "Epoch 3, Step 390: Loss = 1.0395\n",
      "Epoch 3, Step 400: Loss = 2.0938\n",
      "Epoch 3, Step 410: Loss = 2.9547\n",
      "Epoch 3, Step 420: Loss = 0.0708\n",
      "Epoch 3, Step 430: Loss = 0.0973\n",
      "Epoch 3, Step 440: Loss = 0.0684\n",
      "Epoch 3, Step 450: Loss = 2.8714\n",
      "Epoch 3, Step 460: Loss = 1.4453\n",
      "Epoch 3, Step 470: Loss = 0.0368\n",
      "Epoch 3, Step 480: Loss = 1.3635\n",
      "Epoch 3, Step 490: Loss = 1.2938\n",
      "Epoch 3, Step 500: Loss = 1.3971\n",
      "Epoch 3, Step 510: Loss = 1.3716\n",
      "Epoch 3, Step 520: Loss = 0.0166\n",
      "Epoch 3, Step 530: Loss = 0.7138\n",
      "Epoch 3, Step 540: Loss = 1.2087\n",
      "Epoch 3, Step 550: Loss = 1.6886\n",
      "Epoch 3, Step 560: Loss = 1.4407\n",
      "Epoch 3, Step 570: Loss = 0.3870\n",
      "Epoch 3, Step 580: Loss = 0.0428\n",
      "Epoch 3, Step 590: Loss = 0.8181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7206\n",
      "precision: 0.7006\n",
      "recall: 0.7704\n",
      "f1: 0.7339\n",
      "loss: 1.3864\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7425\n",
      "precision: 0.7263\n",
      "recall: 0.7783\n",
      "f1: 0.7514\n",
      "loss: 1.3354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 0: Loss = 0.6833\n",
      "Epoch 4, Step 10: Loss = 0.3143\n",
      "Epoch 4, Step 20: Loss = 1.9596\n",
      "Epoch 4, Step 30: Loss = 0.8291\n",
      "Epoch 4, Step 40: Loss = 0.2503\n",
      "Epoch 4, Step 50: Loss = 0.5606\n",
      "Epoch 4, Step 60: Loss = 0.0359\n",
      "Epoch 4, Step 70: Loss = 2.5164\n",
      "Epoch 4, Step 80: Loss = 0.9383\n",
      "Epoch 4, Step 90: Loss = 1.1410\n",
      "Epoch 4, Step 100: Loss = 1.3534\n",
      "Epoch 4, Step 110: Loss = 4.7007\n",
      "Epoch 4, Step 120: Loss = 0.6299\n",
      "Epoch 4, Step 130: Loss = 0.0877\n",
      "Epoch 4, Step 140: Loss = 0.4970\n",
      "Epoch 4, Step 150: Loss = 0.3356\n",
      "Epoch 4, Step 160: Loss = 1.5285\n",
      "Epoch 4, Step 170: Loss = 1.8175\n",
      "Epoch 4, Step 180: Loss = 1.5497\n",
      "Epoch 4, Step 190: Loss = 0.1805\n",
      "Epoch 4, Step 200: Loss = 2.0935\n",
      "Epoch 4, Step 210: Loss = 0.6519\n",
      "Epoch 4, Step 220: Loss = 0.0374\n",
      "Epoch 4, Step 230: Loss = 1.8114\n",
      "Epoch 4, Step 240: Loss = 1.3333\n",
      "Epoch 4, Step 250: Loss = 0.6642\n",
      "Epoch 4, Step 260: Loss = 0.2094\n",
      "Epoch 4, Step 270: Loss = 0.1145\n",
      "Epoch 4, Step 280: Loss = 0.5310\n",
      "Epoch 4, Step 290: Loss = 0.0798\n",
      "Epoch 4, Step 300: Loss = 1.4104\n",
      "Epoch 4, Step 310: Loss = 2.5177\n",
      "Epoch 4, Step 320: Loss = 1.5616\n",
      "Epoch 4, Step 330: Loss = 0.2031\n",
      "Epoch 4, Step 340: Loss = 0.9518\n",
      "Epoch 4, Step 350: Loss = 1.3166\n",
      "Epoch 4, Step 360: Loss = 0.5262\n",
      "Epoch 4, Step 370: Loss = 0.0758\n",
      "Epoch 4, Step 380: Loss = 0.6465\n",
      "Epoch 4, Step 390: Loss = 0.8992\n",
      "Epoch 4, Step 400: Loss = 0.8546\n",
      "Epoch 4, Step 410: Loss = 2.4722\n",
      "Epoch 4, Step 420: Loss = 0.7891\n",
      "Epoch 4, Step 430: Loss = 1.9153\n",
      "Epoch 4, Step 440: Loss = 2.2139\n",
      "Epoch 4, Step 450: Loss = 0.4552\n",
      "Epoch 4, Step 460: Loss = 3.7571\n",
      "Epoch 4, Step 470: Loss = 0.0902\n",
      "Epoch 4, Step 480: Loss = 0.7406\n",
      "Epoch 4, Step 490: Loss = 0.2845\n",
      "Epoch 4, Step 500: Loss = 1.6668\n",
      "Epoch 4, Step 510: Loss = 0.8806\n",
      "Epoch 4, Step 520: Loss = 2.6958\n",
      "Epoch 4, Step 530: Loss = 0.4669\n",
      "Epoch 4, Step 540: Loss = 1.5051\n",
      "Epoch 4, Step 550: Loss = 1.6282\n",
      "Epoch 4, Step 560: Loss = 1.5851\n",
      "Epoch 4, Step 570: Loss = 1.4735\n",
      "Epoch 4, Step 580: Loss = 1.4755\n",
      "Epoch 4, Step 590: Loss = 2.0654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7546\n",
      "precision: 0.7346\n",
      "recall: 0.7971\n",
      "f1: 0.7646\n",
      "loss: 1.1673\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7675\n",
      "precision: 0.7481\n",
      "recall: 0.8067\n",
      "f1: 0.7763\n",
      "loss: 1.1974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.3458\n",
      "Epoch 5, Step 10: Loss = 0.0769\n",
      "Epoch 5, Step 20: Loss = 2.7053\n",
      "Epoch 5, Step 30: Loss = 0.0545\n",
      "Epoch 5, Step 40: Loss = 2.2910\n",
      "Epoch 5, Step 50: Loss = 1.0668\n",
      "Epoch 5, Step 60: Loss = 1.0030\n",
      "Epoch 5, Step 70: Loss = 0.8226\n",
      "Epoch 5, Step 80: Loss = 2.3529\n",
      "Epoch 5, Step 90: Loss = 1.3237\n",
      "Epoch 5, Step 100: Loss = 1.1862\n",
      "Epoch 5, Step 110: Loss = 1.6630\n",
      "Epoch 5, Step 120: Loss = 0.8432\n",
      "Epoch 5, Step 130: Loss = 0.2741\n",
      "Epoch 5, Step 140: Loss = 1.1029\n",
      "Epoch 5, Step 150: Loss = 0.7915\n",
      "Epoch 5, Step 160: Loss = 0.9836\n",
      "Epoch 5, Step 170: Loss = 1.5876\n",
      "Epoch 5, Step 180: Loss = 0.2822\n",
      "Epoch 5, Step 190: Loss = 0.7002\n",
      "Epoch 5, Step 200: Loss = 1.0172\n",
      "Epoch 5, Step 210: Loss = 1.6254\n",
      "Epoch 5, Step 220: Loss = 2.1918\n",
      "Epoch 5, Step 230: Loss = 3.0301\n",
      "Epoch 5, Step 240: Loss = 0.3027\n",
      "Epoch 5, Step 250: Loss = 4.5181\n",
      "Epoch 5, Step 260: Loss = 0.3487\n",
      "Epoch 5, Step 270: Loss = 0.5168\n",
      "Epoch 5, Step 280: Loss = 0.4114\n",
      "Epoch 5, Step 290: Loss = 1.5120\n",
      "Epoch 5, Step 300: Loss = 0.8786\n",
      "Epoch 5, Step 310: Loss = 1.3445\n",
      "Epoch 5, Step 320: Loss = 0.2519\n",
      "Epoch 5, Step 330: Loss = 1.3844\n",
      "Epoch 5, Step 340: Loss = 2.2303\n",
      "Epoch 5, Step 350: Loss = 1.5646\n",
      "Epoch 5, Step 360: Loss = 1.3947\n",
      "Epoch 5, Step 370: Loss = 0.3886\n",
      "Epoch 5, Step 380: Loss = 0.0428\n",
      "Epoch 5, Step 390: Loss = 1.6293\n",
      "Epoch 5, Step 400: Loss = 0.8449\n",
      "Epoch 5, Step 410: Loss = 0.7182\n",
      "Epoch 5, Step 420: Loss = 2.4333\n",
      "Epoch 5, Step 430: Loss = 0.0725\n",
      "Epoch 5, Step 440: Loss = 2.2209\n",
      "Epoch 5, Step 450: Loss = 1.1792\n",
      "Epoch 5, Step 460: Loss = 3.1232\n",
      "Epoch 5, Step 470: Loss = 1.4826\n",
      "Epoch 5, Step 480: Loss = 0.2068\n",
      "Epoch 5, Step 490: Loss = 0.1641\n",
      "Epoch 5, Step 500: Loss = 1.8020\n",
      "Epoch 5, Step 510: Loss = 1.5581\n",
      "Epoch 5, Step 520: Loss = 0.6167\n",
      "Epoch 5, Step 530: Loss = 1.7727\n",
      "Epoch 5, Step 540: Loss = 0.2253\n",
      "Epoch 5, Step 550: Loss = 0.0296\n",
      "Epoch 5, Step 560: Loss = 1.2800\n",
      "Epoch 5, Step 570: Loss = 2.2029\n",
      "Epoch 5, Step 580: Loss = 1.6347\n",
      "Epoch 5, Step 590: Loss = 0.2296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7735\n",
      "precision: 0.7514\n",
      "recall: 0.8175\n",
      "f1: 0.7831\n",
      "loss: 1.0762\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7783\n",
      "precision: 0.7554\n",
      "recall: 0.8233\n",
      "f1: 0.7879\n",
      "loss: 1.1333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Step 0: Loss = 1.1575\n",
      "Epoch 6, Step 10: Loss = 2.1090\n",
      "Epoch 6, Step 20: Loss = 2.5501\n",
      "Epoch 6, Step 30: Loss = 0.5987\n",
      "Epoch 6, Step 40: Loss = 1.1132\n",
      "Epoch 6, Step 50: Loss = 1.1178\n",
      "Epoch 6, Step 60: Loss = 1.2708\n",
      "Epoch 6, Step 70: Loss = 1.1846\n",
      "Epoch 6, Step 80: Loss = 0.1683\n",
      "Epoch 6, Step 90: Loss = 2.0991\n",
      "Epoch 6, Step 100: Loss = 1.2198\n",
      "Epoch 6, Step 110: Loss = 1.1815\n",
      "Epoch 6, Step 120: Loss = 1.6097\n",
      "Epoch 6, Step 130: Loss = 0.7489\n",
      "Epoch 6, Step 140: Loss = 1.3726\n",
      "Epoch 6, Step 150: Loss = 0.7692\n",
      "Epoch 6, Step 160: Loss = 0.2334\n",
      "Epoch 6, Step 170: Loss = 0.4133\n",
      "Epoch 6, Step 180: Loss = 0.1772\n",
      "Epoch 6, Step 190: Loss = 0.3404\n",
      "Epoch 6, Step 200: Loss = 1.9419\n",
      "Epoch 6, Step 210: Loss = 0.0083\n",
      "Epoch 6, Step 220: Loss = 0.2102\n",
      "Epoch 6, Step 230: Loss = 1.1992\n",
      "Epoch 6, Step 240: Loss = 1.5220\n",
      "Epoch 6, Step 250: Loss = 0.4550\n",
      "Epoch 6, Step 260: Loss = 2.2495\n",
      "Epoch 6, Step 270: Loss = 0.3898\n",
      "Epoch 6, Step 280: Loss = 0.8162\n",
      "Epoch 6, Step 290: Loss = 0.2518\n",
      "Epoch 6, Step 300: Loss = 3.5442\n",
      "Epoch 6, Step 310: Loss = 1.7444\n",
      "Epoch 6, Step 320: Loss = 2.2270\n",
      "Epoch 6, Step 330: Loss = 0.2700\n",
      "Epoch 6, Step 340: Loss = 2.3053\n",
      "Epoch 6, Step 350: Loss = 2.4024\n",
      "Epoch 6, Step 360: Loss = 0.3109\n",
      "Epoch 6, Step 370: Loss = 0.2755\n",
      "Epoch 6, Step 380: Loss = 3.1752\n",
      "Epoch 6, Step 390: Loss = 0.1031\n",
      "Epoch 6, Step 400: Loss = 0.8308\n",
      "Epoch 6, Step 410: Loss = 1.7212\n",
      "Epoch 6, Step 420: Loss = 1.0348\n",
      "Epoch 6, Step 430: Loss = 0.0133\n",
      "Epoch 6, Step 440: Loss = 0.1746\n",
      "Epoch 6, Step 450: Loss = 1.0907\n",
      "Epoch 6, Step 460: Loss = 0.0071\n",
      "Epoch 6, Step 470: Loss = 1.3486\n",
      "Epoch 6, Step 480: Loss = 0.0349\n",
      "Epoch 6, Step 490: Loss = 0.6054\n",
      "Epoch 6, Step 500: Loss = 2.4488\n",
      "Epoch 6, Step 510: Loss = 0.8786\n",
      "Epoch 6, Step 520: Loss = 1.3741\n",
      "Epoch 6, Step 530: Loss = 0.0356\n",
      "Epoch 6, Step 540: Loss = 1.2863\n",
      "Epoch 6, Step 550: Loss = 3.2521\n",
      "Epoch 6, Step 560: Loss = 2.1327\n",
      "Epoch 6, Step 570: Loss = 0.4131\n",
      "Epoch 6, Step 580: Loss = 2.7530\n",
      "Epoch 6, Step 590: Loss = 0.0947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7817\n",
      "precision: 0.7610\n",
      "recall: 0.8213\n",
      "f1: 0.7900\n",
      "loss: 1.0251\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7817\n",
      "precision: 0.7584\n",
      "recall: 0.8267\n",
      "f1: 0.7911\n",
      "loss: 1.1041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Step 0: Loss = 1.5632\n",
      "Epoch 7, Step 10: Loss = 0.5512\n",
      "Epoch 7, Step 20: Loss = 0.2140\n",
      "Epoch 7, Step 30: Loss = 0.1682\n",
      "Epoch 7, Step 40: Loss = 1.4108\n",
      "Epoch 7, Step 50: Loss = 1.1833\n",
      "Epoch 7, Step 60: Loss = 0.5808\n",
      "Epoch 7, Step 70: Loss = 0.2798\n",
      "Epoch 7, Step 80: Loss = 0.5960\n",
      "Epoch 7, Step 90: Loss = 1.3553\n",
      "Epoch 7, Step 100: Loss = 1.3510\n",
      "Epoch 7, Step 110: Loss = 1.1720\n",
      "Epoch 7, Step 120: Loss = 0.0883\n",
      "Epoch 7, Step 130: Loss = 0.0924\n",
      "Epoch 7, Step 140: Loss = 1.3891\n",
      "Epoch 7, Step 150: Loss = 3.0635\n",
      "Epoch 7, Step 160: Loss = 0.1187\n",
      "Epoch 7, Step 170: Loss = 3.0975\n",
      "Epoch 7, Step 180: Loss = 0.3527\n",
      "Epoch 7, Step 190: Loss = 0.6549\n",
      "Epoch 7, Step 200: Loss = 1.4347\n",
      "Epoch 7, Step 210: Loss = 1.5404\n",
      "Epoch 7, Step 220: Loss = 1.3929\n",
      "Epoch 7, Step 230: Loss = 0.1271\n",
      "Epoch 7, Step 240: Loss = 0.5751\n",
      "Epoch 7, Step 250: Loss = 1.0511\n",
      "Epoch 7, Step 260: Loss = 0.1827\n",
      "Epoch 7, Step 270: Loss = 0.5635\n",
      "Epoch 7, Step 280: Loss = 0.6240\n",
      "Epoch 7, Step 290: Loss = 0.6908\n",
      "Epoch 7, Step 300: Loss = 0.5036\n",
      "Epoch 7, Step 310: Loss = 0.6253\n",
      "Epoch 7, Step 320: Loss = 1.1236\n",
      "Epoch 7, Step 330: Loss = 0.3213\n",
      "Epoch 7, Step 340: Loss = 1.8249\n",
      "Epoch 7, Step 350: Loss = 2.0761\n",
      "Epoch 7, Step 360: Loss = 1.0984\n",
      "Epoch 7, Step 370: Loss = 0.8734\n",
      "Epoch 7, Step 380: Loss = 1.8237\n",
      "Epoch 7, Step 390: Loss = 3.9023\n",
      "Epoch 7, Step 400: Loss = 0.0879\n",
      "Epoch 7, Step 410: Loss = 1.7619\n",
      "Epoch 7, Step 420: Loss = 2.7076\n",
      "Epoch 7, Step 430: Loss = 0.4611\n",
      "Epoch 7, Step 440: Loss = 0.3985\n",
      "Epoch 7, Step 450: Loss = 1.0644\n",
      "Epoch 7, Step 460: Loss = 2.3381\n",
      "Epoch 7, Step 470: Loss = 0.3547\n",
      "Epoch 7, Step 480: Loss = 1.2484\n",
      "Epoch 7, Step 490: Loss = 0.6281\n",
      "Epoch 7, Step 500: Loss = 0.3717\n",
      "Epoch 7, Step 510: Loss = 0.2304\n",
      "Epoch 7, Step 520: Loss = 0.0672\n",
      "Epoch 7, Step 530: Loss = 0.6438\n",
      "Epoch 7, Step 540: Loss = 1.0334\n",
      "Epoch 7, Step 550: Loss = 1.1264\n",
      "Epoch 7, Step 560: Loss = 0.2392\n",
      "Epoch 7, Step 570: Loss = 2.1668\n",
      "Epoch 7, Step 580: Loss = 2.1695\n",
      "Epoch 7, Step 590: Loss = 0.7353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7881\n",
      "precision: 0.7661\n",
      "recall: 0.8296\n",
      "f1: 0.7966\n",
      "loss: 0.9965\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7858\n",
      "precision: 0.7626\n",
      "recall: 0.8300\n",
      "f1: 0.7949\n",
      "loss: 1.0835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Step 0: Loss = 2.0506\n",
      "Epoch 8, Step 10: Loss = 2.2039\n",
      "Epoch 8, Step 20: Loss = 0.1054\n",
      "Epoch 8, Step 30: Loss = 0.9077\n",
      "Epoch 8, Step 40: Loss = 0.2612\n",
      "Epoch 8, Step 50: Loss = 0.7970\n",
      "Epoch 8, Step 60: Loss = 0.0249\n",
      "Epoch 8, Step 70: Loss = 0.4630\n",
      "Epoch 8, Step 80: Loss = 1.4290\n",
      "Epoch 8, Step 90: Loss = 1.6756\n",
      "Epoch 8, Step 100: Loss = 0.6592\n",
      "Epoch 8, Step 110: Loss = 0.8804\n",
      "Epoch 8, Step 120: Loss = 0.6285\n",
      "Epoch 8, Step 130: Loss = 0.0035\n",
      "Epoch 8, Step 140: Loss = 0.1164\n",
      "Epoch 8, Step 150: Loss = 0.8034\n",
      "Epoch 8, Step 160: Loss = 0.0108\n",
      "Epoch 8, Step 170: Loss = 0.9579\n",
      "Epoch 8, Step 180: Loss = 2.3557\n",
      "Epoch 8, Step 190: Loss = 0.4627\n",
      "Epoch 8, Step 200: Loss = 0.3111\n",
      "Epoch 8, Step 210: Loss = 0.8470\n",
      "Epoch 8, Step 220: Loss = 0.5150\n",
      "Epoch 8, Step 230: Loss = 0.8725\n",
      "Epoch 8, Step 240: Loss = 1.3001\n",
      "Epoch 8, Step 250: Loss = 0.1751\n",
      "Epoch 8, Step 260: Loss = 2.4933\n",
      "Epoch 8, Step 270: Loss = 1.5086\n",
      "Epoch 8, Step 280: Loss = 0.0083\n",
      "Epoch 8, Step 290: Loss = 1.1694\n",
      "Epoch 8, Step 300: Loss = 0.2154\n",
      "Epoch 8, Step 310: Loss = 0.4227\n",
      "Epoch 8, Step 320: Loss = 4.1252\n",
      "Epoch 8, Step 330: Loss = 0.9074\n",
      "Epoch 8, Step 340: Loss = 0.9109\n",
      "Epoch 8, Step 350: Loss = 1.8768\n",
      "Epoch 8, Step 360: Loss = 1.7276\n",
      "Epoch 8, Step 370: Loss = 1.5457\n",
      "Epoch 8, Step 380: Loss = 0.0055\n",
      "Epoch 8, Step 390: Loss = 0.5467\n",
      "Epoch 8, Step 400: Loss = 0.0716\n",
      "Epoch 8, Step 410: Loss = 0.0991\n",
      "Epoch 8, Step 420: Loss = 0.4506\n",
      "Epoch 8, Step 430: Loss = 0.8032\n",
      "Epoch 8, Step 440: Loss = 0.3909\n",
      "Epoch 8, Step 450: Loss = 1.1716\n",
      "Epoch 8, Step 460: Loss = 0.9308\n",
      "Epoch 8, Step 470: Loss = 1.5661\n",
      "Epoch 8, Step 480: Loss = 1.1385\n",
      "Epoch 8, Step 490: Loss = 0.2768\n",
      "Epoch 8, Step 500: Loss = 0.3017\n",
      "Epoch 8, Step 510: Loss = 1.9607\n",
      "Epoch 8, Step 520: Loss = 0.2842\n",
      "Epoch 8, Step 530: Loss = 0.2305\n",
      "Epoch 8, Step 540: Loss = 0.2896\n",
      "Epoch 8, Step 550: Loss = 0.0827\n",
      "Epoch 8, Step 560: Loss = 0.4281\n",
      "Epoch 8, Step 570: Loss = 0.4512\n",
      "Epoch 8, Step 580: Loss = 1.1238\n",
      "Epoch 8, Step 590: Loss = 0.6246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7908\n",
      "precision: 0.7697\n",
      "recall: 0.8300\n",
      "f1: 0.7987\n",
      "loss: 0.9842\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7892\n",
      "precision: 0.7657\n",
      "recall: 0.8333\n",
      "f1: 0.7981\n",
      "loss: 1.0733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 0: Loss = 2.8891\n",
      "Epoch 9, Step 10: Loss = 0.8393\n",
      "Epoch 9, Step 20: Loss = 0.7205\n",
      "Epoch 9, Step 30: Loss = 0.0155\n",
      "Epoch 9, Step 40: Loss = 0.0215\n",
      "Epoch 9, Step 50: Loss = 1.7510\n",
      "Epoch 9, Step 60: Loss = 0.2488\n",
      "Epoch 9, Step 70: Loss = 0.9211\n",
      "Epoch 9, Step 80: Loss = 1.5501\n",
      "Epoch 9, Step 90: Loss = 0.3928\n",
      "Epoch 9, Step 100: Loss = 0.2062\n",
      "Epoch 9, Step 110: Loss = 1.1353\n",
      "Epoch 9, Step 120: Loss = 0.0013\n",
      "Epoch 9, Step 130: Loss = 2.2504\n",
      "Epoch 9, Step 140: Loss = 0.2503\n",
      "Epoch 9, Step 150: Loss = 0.6431\n",
      "Epoch 9, Step 160: Loss = 3.1333\n",
      "Epoch 9, Step 170: Loss = 0.5245\n",
      "Epoch 9, Step 180: Loss = 0.3638\n",
      "Epoch 9, Step 190: Loss = 1.7643\n",
      "Epoch 9, Step 200: Loss = 2.6654\n",
      "Epoch 9, Step 210: Loss = 0.7576\n",
      "Epoch 9, Step 220: Loss = 0.0055\n",
      "Epoch 9, Step 230: Loss = 0.7528\n",
      "Epoch 9, Step 240: Loss = 2.2117\n",
      "Epoch 9, Step 250: Loss = 2.4503\n",
      "Epoch 9, Step 260: Loss = 0.0094\n",
      "Epoch 9, Step 270: Loss = 1.3970\n",
      "Epoch 9, Step 280: Loss = 0.2414\n",
      "Epoch 9, Step 290: Loss = 0.0255\n",
      "Epoch 9, Step 300: Loss = 1.9849\n",
      "Epoch 9, Step 310: Loss = 2.6769\n",
      "Epoch 9, Step 320: Loss = 0.0092\n",
      "Epoch 9, Step 330: Loss = 1.0767\n",
      "Epoch 9, Step 340: Loss = 1.6108\n",
      "Epoch 9, Step 350: Loss = 2.0778\n",
      "Epoch 9, Step 360: Loss = 1.8097\n",
      "Epoch 9, Step 370: Loss = 0.3875\n",
      "Epoch 9, Step 380: Loss = 1.2778\n",
      "Epoch 9, Step 390: Loss = 1.0071\n",
      "Epoch 9, Step 400: Loss = 1.2262\n",
      "Epoch 9, Step 410: Loss = 0.0984\n",
      "Epoch 9, Step 420: Loss = 0.4293\n",
      "Epoch 9, Step 430: Loss = 0.8203\n",
      "Epoch 9, Step 440: Loss = 1.4176\n",
      "Epoch 9, Step 450: Loss = 1.0476\n",
      "Epoch 9, Step 460: Loss = 0.0328\n",
      "Epoch 9, Step 470: Loss = 1.1878\n",
      "Epoch 9, Step 480: Loss = 0.0010\n",
      "Epoch 9, Step 490: Loss = 3.2004\n",
      "Epoch 9, Step 500: Loss = 2.1335\n",
      "Epoch 9, Step 510: Loss = 0.6312\n",
      "Epoch 9, Step 520: Loss = 1.5563\n",
      "Epoch 9, Step 530: Loss = 0.0517\n",
      "Epoch 9, Step 540: Loss = 0.1306\n",
      "Epoch 9, Step 550: Loss = 2.1664\n",
      "Epoch 9, Step 560: Loss = 0.1895\n",
      "Epoch 9, Step 570: Loss = 0.7636\n",
      "Epoch 9, Step 580: Loss = 0.3273\n",
      "Epoch 9, Step 590: Loss = 1.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7898\n",
      "precision: 0.7688\n",
      "recall: 0.8287\n",
      "f1: 0.7977\n",
      "loss: 0.9757\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7900\n",
      "precision: 0.7661\n",
      "recall: 0.8350\n",
      "f1: 0.7990\n",
      "loss: 1.0674\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "gc = None\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    BitsAndBytesConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        self.emails_df = emails_df.copy()\n",
    "        self.emails_df['sender'] = self.emails_df['sender'].apply(clean_text)\n",
    "        self.emails_df['subject'] = self.emails_df['subject'].apply(clean_text)\n",
    "        self.emails_df['body'] = self.emails_df['body'].apply(clean_text)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        email = self.emails_df.iloc[idx]\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(email['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    # initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # model configuration\n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "    # 8-bit quantization setup\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "    # LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, output_dir):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9):\n",
    "    best_val_metrics = {'f1': 0}\n",
    "    best_model_state = None\n",
    "    model = model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds, train_labels = [], []\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            if (step + 1) % 2 == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(batch['label'].cpu().numpy())\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "            torch.cuda.empty_cache()\n",
    "        train_metrics = compute_metrics(train_preds, train_labels)\n",
    "        train_metrics['loss'] = total_loss / len(train_loader)\n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print_metrics = lambda m: [print(f\"{k}: {v:.4f}\") for k, v in m.items() if k != 'confusion_matrix']\n",
    "        print(\"Training Metrics:\")\n",
    "        print_metrics(train_metrics)\n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        print_metrics(val_metrics)\n",
    "        if val_metrics['f1'] > best_val_metrics['f1']:\n",
    "            best_val_metrics = val_metrics\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    return best_model_state, best_val_metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    login(token=\"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\")\n",
    "    device = setup_environment()\n",
    "    # switch to Mistral-7B model\n",
    "    model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/newdata_cleaned.csv\")\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    train_df, val_df = train_test_split(\n",
    "        emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42\n",
    "    )\n",
    "    train_dataset = EmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = EmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    best_model_state, best_metrics = train_model(\n",
    "        model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs\n",
    "    )\n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/binary_classification_mistral\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # save PEFT-wrapped model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    # save training config and metrics\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"best_metrics\": {k: float(v) if k != 'confusion_matrix' else v.tolist() for k, v in best_metrics.items()}\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fcb167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n",
      "GPU Memory: 23.68 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd922c4b6c249cba99029a4d073b07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/32.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98431eeaf82d499f82d17df1222f2b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62cf078c77f84ca1bbcc115285d07a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d72dd9ff9247faa654b3b5e267f19d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e260bda58c304dc9b0848a3b16b373a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5086e465668e44ff8d3e5ab95a419255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa31dd1b4ba489f8eca6b9152e5cbff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50dc078fde94febb21ef96fd118ae17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 2.0400\n",
      "Epoch 1, Step 10: Loss = 1.5032\n",
      "Epoch 1, Step 20: Loss = 0.9281\n",
      "Epoch 1, Step 30: Loss = 1.2490\n",
      "Epoch 1, Step 40: Loss = 0.7756\n",
      "Epoch 1, Step 50: Loss = 1.3464\n",
      "Epoch 1, Step 60: Loss = 0.5596\n",
      "Epoch 1, Step 70: Loss = 1.7058\n",
      "Epoch 1, Step 80: Loss = 0.7351\n",
      "Epoch 1, Step 90: Loss = 1.9586\n",
      "Epoch 1, Step 100: Loss = 1.6562\n",
      "Epoch 1, Step 110: Loss = 1.1429\n",
      "Epoch 1, Step 120: Loss = 1.5777\n",
      "Epoch 1, Step 130: Loss = 1.8174\n",
      "Epoch 1, Step 140: Loss = 0.2428\n",
      "Epoch 1, Step 150: Loss = 1.7237\n",
      "Epoch 1, Step 160: Loss = 1.1876\n",
      "Epoch 1, Step 170: Loss = 0.9066\n",
      "Epoch 1, Step 180: Loss = 1.5048\n",
      "Epoch 1, Step 190: Loss = 1.2130\n",
      "Epoch 1, Step 200: Loss = 0.6908\n",
      "Epoch 1, Step 210: Loss = 1.5208\n",
      "Epoch 1, Step 220: Loss = 2.0937\n",
      "Epoch 1, Step 230: Loss = 1.3716\n",
      "Epoch 1, Step 240: Loss = 1.7857\n",
      "Epoch 1, Step 250: Loss = 0.9519\n",
      "Epoch 1, Step 260: Loss = 1.4492\n",
      "Epoch 1, Step 270: Loss = 0.9419\n",
      "Epoch 1, Step 280: Loss = 0.6507\n",
      "Epoch 1, Step 290: Loss = 1.1684\n",
      "Epoch 1, Step 300: Loss = 1.4732\n",
      "Epoch 1, Step 310: Loss = 0.1271\n",
      "Epoch 1, Step 320: Loss = 0.4304\n",
      "Epoch 1, Step 330: Loss = 1.3233\n",
      "Epoch 1, Step 340: Loss = 1.5186\n",
      "Epoch 1, Step 350: Loss = 1.1108\n",
      "Epoch 1, Step 360: Loss = 0.3409\n",
      "Epoch 1, Step 370: Loss = 0.7178\n",
      "Epoch 1, Step 380: Loss = 0.7187\n",
      "Epoch 1, Step 390: Loss = 1.6211\n",
      "Epoch 1, Step 400: Loss = 0.9733\n",
      "Epoch 1, Step 410: Loss = 1.4747\n",
      "Epoch 1, Step 420: Loss = 1.1647\n",
      "Epoch 1, Step 430: Loss = 0.7224\n",
      "Epoch 1, Step 440: Loss = 1.8796\n",
      "Epoch 1, Step 450: Loss = 1.2296\n",
      "Epoch 1, Step 460: Loss = 1.0662\n",
      "Epoch 1, Step 470: Loss = 0.5833\n",
      "Epoch 1, Step 480: Loss = 0.6211\n",
      "Epoch 1, Step 490: Loss = 0.7685\n",
      "Epoch 1, Step 500: Loss = 1.0777\n",
      "Epoch 1, Step 510: Loss = 1.0894\n",
      "Epoch 1, Step 520: Loss = 1.0140\n",
      "Epoch 1, Step 530: Loss = 0.5635\n",
      "Epoch 1, Step 540: Loss = 0.2265\n",
      "Epoch 1, Step 550: Loss = 0.6137\n",
      "Epoch 1, Step 560: Loss = 0.8529\n",
      "Epoch 1, Step 570: Loss = 0.5322\n",
      "Epoch 1, Step 580: Loss = 0.7153\n",
      "Epoch 1, Step 590: Loss = 0.7293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.5267\n",
      "precision: 0.5146\n",
      "recall: 0.9379\n",
      "f1: 0.6646\n",
      "loss: 1.2231\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.6100\n",
      "precision: 0.5846\n",
      "recall: 0.7600\n",
      "f1: 0.6609\n",
      "loss: 0.6945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 0: Loss = 0.4935\n",
      "Epoch 2, Step 10: Loss = 0.8578\n",
      "Epoch 2, Step 20: Loss = 0.5711\n",
      "Epoch 2, Step 30: Loss = 1.0760\n",
      "Epoch 2, Step 40: Loss = 0.8506\n",
      "Epoch 2, Step 50: Loss = 1.0648\n",
      "Epoch 2, Step 60: Loss = 0.4281\n",
      "Epoch 2, Step 70: Loss = 0.5850\n",
      "Epoch 2, Step 80: Loss = 1.1257\n",
      "Epoch 2, Step 90: Loss = 0.8209\n",
      "Epoch 2, Step 100: Loss = 0.9596\n",
      "Epoch 2, Step 110: Loss = 0.4166\n",
      "Epoch 2, Step 120: Loss = 0.3897\n",
      "Epoch 2, Step 130: Loss = 0.4037\n",
      "Epoch 2, Step 140: Loss = 0.8420\n",
      "Epoch 2, Step 150: Loss = 0.3076\n",
      "Epoch 2, Step 160: Loss = 0.7378\n",
      "Epoch 2, Step 170: Loss = 0.6178\n",
      "Epoch 2, Step 180: Loss = 0.6433\n",
      "Epoch 2, Step 190: Loss = 0.5357\n",
      "Epoch 2, Step 200: Loss = 0.3213\n",
      "Epoch 2, Step 210: Loss = 0.4607\n",
      "Epoch 2, Step 220: Loss = 0.4637\n",
      "Epoch 2, Step 230: Loss = 0.4731\n",
      "Epoch 2, Step 240: Loss = 0.5359\n",
      "Epoch 2, Step 250: Loss = 0.8210\n",
      "Epoch 2, Step 260: Loss = 0.6590\n",
      "Epoch 2, Step 270: Loss = 0.5159\n",
      "Epoch 2, Step 280: Loss = 0.6984\n",
      "Epoch 2, Step 290: Loss = 0.3334\n",
      "Epoch 2, Step 300: Loss = 0.5084\n",
      "Epoch 2, Step 310: Loss = 0.3439\n",
      "Epoch 2, Step 320: Loss = 0.4149\n",
      "Epoch 2, Step 330: Loss = 0.4503\n",
      "Epoch 2, Step 340: Loss = 0.4243\n",
      "Epoch 2, Step 350: Loss = 0.5353\n",
      "Epoch 2, Step 360: Loss = 0.5362\n",
      "Epoch 2, Step 370: Loss = 0.1994\n",
      "Epoch 2, Step 380: Loss = 0.5415\n",
      "Epoch 2, Step 390: Loss = 0.5334\n",
      "Epoch 2, Step 400: Loss = 0.4565\n",
      "Epoch 2, Step 410: Loss = 0.3528\n",
      "Epoch 2, Step 420: Loss = 0.6554\n",
      "Epoch 2, Step 430: Loss = 0.5156\n",
      "Epoch 2, Step 440: Loss = 0.4426\n",
      "Epoch 2, Step 450: Loss = 0.4742\n",
      "Epoch 2, Step 460: Loss = 0.6270\n",
      "Epoch 2, Step 470: Loss = 0.3104\n",
      "Epoch 2, Step 480: Loss = 0.3541\n",
      "Epoch 2, Step 490: Loss = 0.7390\n",
      "Epoch 2, Step 500: Loss = 0.3111\n",
      "Epoch 2, Step 510: Loss = 0.5171\n",
      "Epoch 2, Step 520: Loss = 0.4959\n",
      "Epoch 2, Step 530: Loss = 0.5085\n",
      "Epoch 2, Step 540: Loss = 0.3737\n",
      "Epoch 2, Step 550: Loss = 0.3120\n",
      "Epoch 2, Step 560: Loss = 0.2228\n",
      "Epoch 2, Step 570: Loss = 0.1782\n",
      "Epoch 2, Step 580: Loss = 0.6537\n",
      "Epoch 2, Step 590: Loss = 0.2889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.7423\n",
      "precision: 0.7182\n",
      "recall: 0.7975\n",
      "f1: 0.7558\n",
      "loss: 0.5395\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8192\n",
      "precision: 0.7888\n",
      "recall: 0.8717\n",
      "f1: 0.8282\n",
      "loss: 0.4339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 0: Loss = 0.5836\n",
      "Epoch 3, Step 10: Loss = 0.5113\n",
      "Epoch 3, Step 20: Loss = 0.3375\n",
      "Epoch 3, Step 30: Loss = 0.4506\n",
      "Epoch 3, Step 40: Loss = 0.7465\n",
      "Epoch 3, Step 50: Loss = 0.5448\n",
      "Epoch 3, Step 60: Loss = 0.4044\n",
      "Epoch 3, Step 70: Loss = 0.2668\n",
      "Epoch 3, Step 80: Loss = 0.3763\n",
      "Epoch 3, Step 90: Loss = 0.4105\n",
      "Epoch 3, Step 100: Loss = 0.2770\n",
      "Epoch 3, Step 110: Loss = 0.3383\n",
      "Epoch 3, Step 120: Loss = 0.3678\n",
      "Epoch 3, Step 130: Loss = 0.4097\n",
      "Epoch 3, Step 140: Loss = 0.3559\n",
      "Epoch 3, Step 150: Loss = 0.5972\n",
      "Epoch 3, Step 160: Loss = 0.5034\n",
      "Epoch 3, Step 170: Loss = 0.2258\n",
      "Epoch 3, Step 180: Loss = 0.4395\n",
      "Epoch 3, Step 190: Loss = 0.5918\n",
      "Epoch 3, Step 200: Loss = 0.5012\n",
      "Epoch 3, Step 210: Loss = 0.5838\n",
      "Epoch 3, Step 220: Loss = 0.3095\n",
      "Epoch 3, Step 230: Loss = 0.3524\n",
      "Epoch 3, Step 240: Loss = 0.3641\n",
      "Epoch 3, Step 250: Loss = 0.2782\n",
      "Epoch 3, Step 260: Loss = 0.2705\n",
      "Epoch 3, Step 270: Loss = 0.4984\n",
      "Epoch 3, Step 280: Loss = 0.1895\n",
      "Epoch 3, Step 290: Loss = 1.0596\n",
      "Epoch 3, Step 300: Loss = 0.2854\n",
      "Epoch 3, Step 310: Loss = 0.2017\n",
      "Epoch 3, Step 320: Loss = 0.3784\n",
      "Epoch 3, Step 330: Loss = 0.4918\n",
      "Epoch 3, Step 340: Loss = 0.4998\n",
      "Epoch 3, Step 350: Loss = 0.2167\n",
      "Epoch 3, Step 360: Loss = 0.4200\n",
      "Epoch 3, Step 370: Loss = 0.2072\n",
      "Epoch 3, Step 380: Loss = 0.2331\n",
      "Epoch 3, Step 390: Loss = 0.2219\n",
      "Epoch 3, Step 400: Loss = 0.3667\n",
      "Epoch 3, Step 410: Loss = 0.3405\n",
      "Epoch 3, Step 420: Loss = 0.2895\n",
      "Epoch 3, Step 430: Loss = 0.4268\n",
      "Epoch 3, Step 440: Loss = 0.5635\n",
      "Epoch 3, Step 450: Loss = 0.3278\n",
      "Epoch 3, Step 460: Loss = 0.6870\n",
      "Epoch 3, Step 470: Loss = 0.2547\n",
      "Epoch 3, Step 480: Loss = 0.2697\n",
      "Epoch 3, Step 490: Loss = 0.4688\n",
      "Epoch 3, Step 500: Loss = 0.2932\n",
      "Epoch 3, Step 510: Loss = 0.3379\n",
      "Epoch 3, Step 520: Loss = 0.3475\n",
      "Epoch 3, Step 530: Loss = 0.3598\n",
      "Epoch 3, Step 540: Loss = 0.2564\n",
      "Epoch 3, Step 550: Loss = 0.1917\n",
      "Epoch 3, Step 560: Loss = 0.1660\n",
      "Epoch 3, Step 570: Loss = 0.3166\n",
      "Epoch 3, Step 580: Loss = 0.2370\n",
      "Epoch 3, Step 590: Loss = 0.4484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8433\n",
      "precision: 0.8259\n",
      "recall: 0.8700\n",
      "f1: 0.8474\n",
      "loss: 0.4007\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8650\n",
      "precision: 0.8521\n",
      "recall: 0.8833\n",
      "f1: 0.8674\n",
      "loss: 0.3610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 0: Loss = 0.7704\n",
      "Epoch 4, Step 10: Loss = 0.2577\n",
      "Epoch 4, Step 20: Loss = 0.7176\n",
      "Epoch 4, Step 30: Loss = 0.4735\n",
      "Epoch 4, Step 40: Loss = 0.5380\n",
      "Epoch 4, Step 50: Loss = 1.1122\n",
      "Epoch 4, Step 60: Loss = 0.1949\n",
      "Epoch 4, Step 70: Loss = 0.4304\n",
      "Epoch 4, Step 80: Loss = 0.2104\n",
      "Epoch 4, Step 90: Loss = 0.5541\n",
      "Epoch 4, Step 100: Loss = 0.6340\n",
      "Epoch 4, Step 110: Loss = 0.2387\n",
      "Epoch 4, Step 120: Loss = 0.3883\n",
      "Epoch 4, Step 130: Loss = 0.5401\n",
      "Epoch 4, Step 140: Loss = 0.5307\n",
      "Epoch 4, Step 150: Loss = 0.3679\n",
      "Epoch 4, Step 160: Loss = 0.2383\n",
      "Epoch 4, Step 170: Loss = 0.4136\n",
      "Epoch 4, Step 180: Loss = 0.5416\n",
      "Epoch 4, Step 190: Loss = 0.1973\n",
      "Epoch 4, Step 200: Loss = 0.4288\n",
      "Epoch 4, Step 210: Loss = 0.2718\n",
      "Epoch 4, Step 220: Loss = 0.4971\n",
      "Epoch 4, Step 230: Loss = 0.3559\n",
      "Epoch 4, Step 240: Loss = 0.2384\n",
      "Epoch 4, Step 250: Loss = 0.5520\n",
      "Epoch 4, Step 260: Loss = 0.1441\n",
      "Epoch 4, Step 270: Loss = 0.6296\n",
      "Epoch 4, Step 280: Loss = 0.6485\n",
      "Epoch 4, Step 290: Loss = 0.5729\n",
      "Epoch 4, Step 300: Loss = 0.2639\n",
      "Epoch 4, Step 310: Loss = 0.1199\n",
      "Epoch 4, Step 320: Loss = 0.2445\n",
      "Epoch 4, Step 330: Loss = 0.3669\n",
      "Epoch 4, Step 340: Loss = 0.3192\n",
      "Epoch 4, Step 350: Loss = 0.2228\n",
      "Epoch 4, Step 360: Loss = 0.7814\n",
      "Epoch 4, Step 370: Loss = 0.3681\n",
      "Epoch 4, Step 380: Loss = 0.2576\n",
      "Epoch 4, Step 390: Loss = 0.2511\n",
      "Epoch 4, Step 400: Loss = 0.2204\n",
      "Epoch 4, Step 410: Loss = 0.2904\n",
      "Epoch 4, Step 420: Loss = 0.3699\n",
      "Epoch 4, Step 430: Loss = 0.1968\n",
      "Epoch 4, Step 440: Loss = 0.6994\n",
      "Epoch 4, Step 450: Loss = 0.1842\n",
      "Epoch 4, Step 460: Loss = 0.2411\n",
      "Epoch 4, Step 470: Loss = 0.2022\n",
      "Epoch 4, Step 480: Loss = 0.5380\n",
      "Epoch 4, Step 490: Loss = 0.1881\n",
      "Epoch 4, Step 500: Loss = 0.4207\n",
      "Epoch 4, Step 510: Loss = 0.6531\n",
      "Epoch 4, Step 520: Loss = 0.3569\n",
      "Epoch 4, Step 530: Loss = 0.4223\n",
      "Epoch 4, Step 540: Loss = 0.4005\n",
      "Epoch 4, Step 550: Loss = 0.3395\n",
      "Epoch 4, Step 560: Loss = 0.3175\n",
      "Epoch 4, Step 570: Loss = 0.1568\n",
      "Epoch 4, Step 580: Loss = 0.2457\n",
      "Epoch 4, Step 590: Loss = 0.4731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8690\n",
      "precision: 0.8575\n",
      "recall: 0.8850\n",
      "f1: 0.8710\n",
      "loss: 0.3581\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8758\n",
      "precision: 0.8667\n",
      "recall: 0.8883\n",
      "f1: 0.8774\n",
      "loss: 0.3381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.3967\n",
      "Epoch 5, Step 10: Loss = 0.1533\n",
      "Epoch 5, Step 20: Loss = 0.3057\n",
      "Epoch 5, Step 30: Loss = 0.2526\n",
      "Epoch 5, Step 40: Loss = 0.3793\n",
      "Epoch 5, Step 50: Loss = 0.2019\n",
      "Epoch 5, Step 60: Loss = 0.1681\n",
      "Epoch 5, Step 70: Loss = 0.5748\n",
      "Epoch 5, Step 80: Loss = 0.6317\n",
      "Epoch 5, Step 90: Loss = 0.4194\n",
      "Epoch 5, Step 100: Loss = 0.2551\n",
      "Epoch 5, Step 110: Loss = 0.2289\n",
      "Epoch 5, Step 120: Loss = 0.2292\n",
      "Epoch 5, Step 130: Loss = 0.3456\n",
      "Epoch 5, Step 140: Loss = 0.6530\n",
      "Epoch 5, Step 150: Loss = 0.4519\n",
      "Epoch 5, Step 160: Loss = 0.2738\n",
      "Epoch 5, Step 170: Loss = 0.3340\n",
      "Epoch 5, Step 180: Loss = 0.2291\n",
      "Epoch 5, Step 190: Loss = 0.2352\n",
      "Epoch 5, Step 200: Loss = 0.2451\n",
      "Epoch 5, Step 210: Loss = 0.3215\n",
      "Epoch 5, Step 220: Loss = 0.2511\n",
      "Epoch 5, Step 230: Loss = 0.2990\n",
      "Epoch 5, Step 240: Loss = 0.1773\n",
      "Epoch 5, Step 250: Loss = 0.2663\n",
      "Epoch 5, Step 260: Loss = 0.1894\n",
      "Epoch 5, Step 270: Loss = 0.2492\n",
      "Epoch 5, Step 280: Loss = 0.4137\n",
      "Epoch 5, Step 290: Loss = 0.3438\n",
      "Epoch 5, Step 300: Loss = 0.5820\n",
      "Epoch 5, Step 310: Loss = 0.1836\n",
      "Epoch 5, Step 320: Loss = 0.2024\n",
      "Epoch 5, Step 330: Loss = 0.3464\n",
      "Epoch 5, Step 340: Loss = 0.2882\n",
      "Epoch 5, Step 350: Loss = 0.5526\n",
      "Epoch 5, Step 360: Loss = 0.3415\n",
      "Epoch 5, Step 370: Loss = 0.2950\n",
      "Epoch 5, Step 380: Loss = 0.2104\n",
      "Epoch 5, Step 390: Loss = 0.3825\n",
      "Epoch 5, Step 400: Loss = 0.2667\n",
      "Epoch 5, Step 410: Loss = 0.1244\n",
      "Epoch 5, Step 420: Loss = 0.4549\n",
      "Epoch 5, Step 430: Loss = 0.1449\n",
      "Epoch 5, Step 440: Loss = 0.3620\n",
      "Epoch 5, Step 450: Loss = 0.2244\n",
      "Epoch 5, Step 460: Loss = 0.1621\n",
      "Epoch 5, Step 470: Loss = 0.2707\n",
      "Epoch 5, Step 480: Loss = 0.2090\n",
      "Epoch 5, Step 490: Loss = 0.2485\n",
      "Epoch 5, Step 500: Loss = 0.2443\n",
      "Epoch 5, Step 510: Loss = 0.4208\n",
      "Epoch 5, Step 520: Loss = 0.3302\n",
      "Epoch 5, Step 530: Loss = 0.2973\n",
      "Epoch 5, Step 540: Loss = 0.3246\n",
      "Epoch 5, Step 550: Loss = 0.2508\n",
      "Epoch 5, Step 560: Loss = 0.1522\n",
      "Epoch 5, Step 570: Loss = 0.1926\n",
      "Epoch 5, Step 580: Loss = 0.3802\n",
      "Epoch 5, Step 590: Loss = 0.2619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8779\n",
      "precision: 0.8687\n",
      "recall: 0.8904\n",
      "f1: 0.8794\n",
      "loss: 0.3415\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8808\n",
      "precision: 0.8715\n",
      "recall: 0.8933\n",
      "f1: 0.8823\n",
      "loss: 0.3270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Step 0: Loss = 0.3083\n",
      "Epoch 6, Step 10: Loss = 0.3131\n",
      "Epoch 6, Step 20: Loss = 0.2576\n",
      "Epoch 6, Step 30: Loss = 0.2864\n",
      "Epoch 6, Step 40: Loss = 0.1823\n",
      "Epoch 6, Step 50: Loss = 0.3457\n",
      "Epoch 6, Step 60: Loss = 0.7559\n",
      "Epoch 6, Step 70: Loss = 0.3130\n",
      "Epoch 6, Step 80: Loss = 0.3231\n",
      "Epoch 6, Step 90: Loss = 0.3391\n",
      "Epoch 6, Step 100: Loss = 0.3090\n",
      "Epoch 6, Step 110: Loss = 0.2850\n",
      "Epoch 6, Step 120: Loss = 0.5416\n",
      "Epoch 6, Step 130: Loss = 0.6859\n",
      "Epoch 6, Step 140: Loss = 0.7074\n",
      "Epoch 6, Step 150: Loss = 0.1691\n",
      "Epoch 6, Step 160: Loss = 0.2883\n",
      "Epoch 6, Step 170: Loss = 1.4343\n",
      "Epoch 6, Step 180: Loss = 0.8671\n",
      "Epoch 6, Step 190: Loss = 1.1141\n",
      "Epoch 6, Step 200: Loss = 0.4129\n",
      "Epoch 6, Step 210: Loss = 0.4518\n",
      "Epoch 6, Step 220: Loss = 0.5931\n",
      "Epoch 6, Step 230: Loss = 0.2152\n",
      "Epoch 6, Step 240: Loss = 0.4375\n",
      "Epoch 6, Step 250: Loss = 0.2649\n",
      "Epoch 6, Step 260: Loss = 0.3738\n",
      "Epoch 6, Step 270: Loss = 0.3496\n",
      "Epoch 6, Step 280: Loss = 0.2314\n",
      "Epoch 6, Step 290: Loss = 0.2393\n",
      "Epoch 6, Step 300: Loss = 0.2652\n",
      "Epoch 6, Step 310: Loss = 0.6921\n",
      "Epoch 6, Step 320: Loss = 0.3907\n",
      "Epoch 6, Step 330: Loss = 0.2013\n",
      "Epoch 6, Step 340: Loss = 0.2310\n",
      "Epoch 6, Step 350: Loss = 0.2222\n",
      "Epoch 6, Step 360: Loss = 0.2448\n",
      "Epoch 6, Step 370: Loss = 0.1265\n",
      "Epoch 6, Step 380: Loss = 0.3049\n",
      "Epoch 6, Step 390: Loss = 0.4481\n",
      "Epoch 6, Step 400: Loss = 0.1580\n",
      "Epoch 6, Step 410: Loss = 0.1823\n",
      "Epoch 6, Step 420: Loss = 0.3102\n",
      "Epoch 6, Step 430: Loss = 0.2561\n",
      "Epoch 6, Step 440: Loss = 0.3454\n",
      "Epoch 6, Step 450: Loss = 0.1901\n",
      "Epoch 6, Step 460: Loss = 0.4225\n",
      "Epoch 6, Step 470: Loss = 0.3411\n",
      "Epoch 6, Step 480: Loss = 0.2365\n",
      "Epoch 6, Step 490: Loss = 0.7029\n",
      "Epoch 6, Step 500: Loss = 0.1573\n",
      "Epoch 6, Step 510: Loss = 0.3474\n",
      "Epoch 6, Step 520: Loss = 0.1757\n",
      "Epoch 6, Step 530: Loss = 0.1880\n",
      "Epoch 6, Step 540: Loss = 0.5216\n",
      "Epoch 6, Step 550: Loss = 0.3193\n",
      "Epoch 6, Step 560: Loss = 0.3227\n",
      "Epoch 6, Step 570: Loss = 0.2005\n",
      "Epoch 6, Step 580: Loss = 0.3393\n",
      "Epoch 6, Step 590: Loss = 0.1818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8831\n",
      "precision: 0.8770\n",
      "recall: 0.8912\n",
      "f1: 0.8841\n",
      "loss: 0.3319\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8808\n",
      "precision: 0.8703\n",
      "recall: 0.8950\n",
      "f1: 0.8825\n",
      "loss: 0.3227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Step 0: Loss = 0.2119\n",
      "Epoch 7, Step 10: Loss = 0.2691\n",
      "Epoch 7, Step 20: Loss = 0.1466\n",
      "Epoch 7, Step 30: Loss = 0.1895\n",
      "Epoch 7, Step 40: Loss = 0.5581\n",
      "Epoch 7, Step 50: Loss = 0.4857\n",
      "Epoch 7, Step 60: Loss = 0.3602\n",
      "Epoch 7, Step 70: Loss = 0.2163\n",
      "Epoch 7, Step 80: Loss = 0.3529\n",
      "Epoch 7, Step 90: Loss = 0.3178\n",
      "Epoch 7, Step 100: Loss = 0.1936\n",
      "Epoch 7, Step 110: Loss = 0.2263\n",
      "Epoch 7, Step 120: Loss = 0.7496\n",
      "Epoch 7, Step 130: Loss = 0.2838\n",
      "Epoch 7, Step 140: Loss = 0.1994\n",
      "Epoch 7, Step 150: Loss = 0.3087\n",
      "Epoch 7, Step 160: Loss = 0.4410\n",
      "Epoch 7, Step 170: Loss = 0.1600\n",
      "Epoch 7, Step 180: Loss = 0.1729\n",
      "Epoch 7, Step 190: Loss = 0.3289\n",
      "Epoch 7, Step 200: Loss = 0.4733\n",
      "Epoch 7, Step 210: Loss = 0.3739\n",
      "Epoch 7, Step 220: Loss = 0.2684\n",
      "Epoch 7, Step 230: Loss = 0.2334\n",
      "Epoch 7, Step 240: Loss = 0.7815\n",
      "Epoch 7, Step 250: Loss = 0.3623\n",
      "Epoch 7, Step 260: Loss = 0.1311\n",
      "Epoch 7, Step 270: Loss = 0.3312\n",
      "Epoch 7, Step 280: Loss = 0.1462\n",
      "Epoch 7, Step 290: Loss = 0.2451\n",
      "Epoch 7, Step 300: Loss = 0.2640\n",
      "Epoch 7, Step 310: Loss = 0.4258\n",
      "Epoch 7, Step 320: Loss = 0.3885\n",
      "Epoch 7, Step 330: Loss = 0.0975\n",
      "Epoch 7, Step 340: Loss = 0.5002\n",
      "Epoch 7, Step 350: Loss = 0.3068\n",
      "Epoch 7, Step 360: Loss = 0.2295\n",
      "Epoch 7, Step 370: Loss = 0.4773\n",
      "Epoch 7, Step 380: Loss = 0.2949\n",
      "Epoch 7, Step 390: Loss = 0.3662\n",
      "Epoch 7, Step 400: Loss = 0.4687\n",
      "Epoch 7, Step 410: Loss = 0.2050\n",
      "Epoch 7, Step 420: Loss = 0.2058\n",
      "Epoch 7, Step 430: Loss = 0.2065\n",
      "Epoch 7, Step 440: Loss = 0.2239\n",
      "Epoch 7, Step 450: Loss = 0.4440\n",
      "Epoch 7, Step 460: Loss = 0.2212\n",
      "Epoch 7, Step 470: Loss = 0.3381\n",
      "Epoch 7, Step 480: Loss = 0.3606\n",
      "Epoch 7, Step 490: Loss = 0.2972\n",
      "Epoch 7, Step 500: Loss = 0.6050\n",
      "Epoch 7, Step 510: Loss = 0.2326\n",
      "Epoch 7, Step 520: Loss = 0.1906\n",
      "Epoch 7, Step 530: Loss = 0.2863\n",
      "Epoch 7, Step 540: Loss = 0.1590\n",
      "Epoch 7, Step 550: Loss = 0.2013\n",
      "Epoch 7, Step 560: Loss = 0.1687\n",
      "Epoch 7, Step 570: Loss = 0.2992\n",
      "Epoch 7, Step 580: Loss = 0.2958\n",
      "Epoch 7, Step 590: Loss = 0.1895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8862\n",
      "precision: 0.8759\n",
      "recall: 0.9000\n",
      "f1: 0.8878\n",
      "loss: 0.3289\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8825\n",
      "precision: 0.8708\n",
      "recall: 0.8983\n",
      "f1: 0.8843\n",
      "loss: 0.3201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Step 0: Loss = 0.2203\n",
      "Epoch 8, Step 10: Loss = 0.1895\n",
      "Epoch 8, Step 20: Loss = 1.4238\n",
      "Epoch 8, Step 30: Loss = 0.3891\n",
      "Epoch 8, Step 40: Loss = 0.2578\n",
      "Epoch 8, Step 50: Loss = 0.1392\n",
      "Epoch 8, Step 60: Loss = 0.3378\n",
      "Epoch 8, Step 70: Loss = 0.4827\n",
      "Epoch 8, Step 80: Loss = 0.2176\n",
      "Epoch 8, Step 90: Loss = 0.2140\n",
      "Epoch 8, Step 100: Loss = 0.3228\n",
      "Epoch 8, Step 110: Loss = 0.3936\n",
      "Epoch 8, Step 120: Loss = 0.3117\n",
      "Epoch 8, Step 130: Loss = 0.4519\n",
      "Epoch 8, Step 140: Loss = 0.4622\n",
      "Epoch 8, Step 150: Loss = 0.3693\n",
      "Epoch 8, Step 160: Loss = 0.1556\n",
      "Epoch 8, Step 170: Loss = 0.2538\n",
      "Epoch 8, Step 180: Loss = 0.1868\n",
      "Epoch 8, Step 190: Loss = 0.6475\n",
      "Epoch 8, Step 200: Loss = 0.1653\n",
      "Epoch 8, Step 210: Loss = 0.3542\n",
      "Epoch 8, Step 220: Loss = 0.4491\n",
      "Epoch 8, Step 230: Loss = 0.3337\n",
      "Epoch 8, Step 240: Loss = 0.3051\n",
      "Epoch 8, Step 250: Loss = 0.2622\n",
      "Epoch 8, Step 260: Loss = 0.5999\n",
      "Epoch 8, Step 270: Loss = 0.1852\n",
      "Epoch 8, Step 280: Loss = 0.3570\n",
      "Epoch 8, Step 290: Loss = 0.1599\n",
      "Epoch 8, Step 300: Loss = 0.3639\n",
      "Epoch 8, Step 310: Loss = 0.2131\n",
      "Epoch 8, Step 320: Loss = 0.6505\n",
      "Epoch 8, Step 330: Loss = 0.3338\n",
      "Epoch 8, Step 340: Loss = 0.2556\n",
      "Epoch 8, Step 350: Loss = 0.3717\n",
      "Epoch 8, Step 360: Loss = 0.5553\n",
      "Epoch 8, Step 370: Loss = 0.3196\n",
      "Epoch 8, Step 380: Loss = 0.3448\n",
      "Epoch 8, Step 390: Loss = 0.2707\n",
      "Epoch 8, Step 400: Loss = 0.1642\n",
      "Epoch 8, Step 410: Loss = 0.2481\n",
      "Epoch 8, Step 420: Loss = 0.2741\n",
      "Epoch 8, Step 430: Loss = 0.2115\n",
      "Epoch 8, Step 440: Loss = 0.4609\n",
      "Epoch 8, Step 450: Loss = 0.3266\n",
      "Epoch 8, Step 460: Loss = 0.7933\n",
      "Epoch 8, Step 470: Loss = 0.1323\n",
      "Epoch 8, Step 480: Loss = 0.4167\n",
      "Epoch 8, Step 490: Loss = 0.3081\n",
      "Epoch 8, Step 500: Loss = 0.1890\n",
      "Epoch 8, Step 510: Loss = 0.3308\n",
      "Epoch 8, Step 520: Loss = 0.4582\n",
      "Epoch 8, Step 530: Loss = 0.4146\n",
      "Epoch 8, Step 540: Loss = 0.3770\n",
      "Epoch 8, Step 550: Loss = 0.4718\n",
      "Epoch 8, Step 560: Loss = 0.2963\n",
      "Epoch 8, Step 570: Loss = 1.2513\n",
      "Epoch 8, Step 580: Loss = 0.1688\n",
      "Epoch 8, Step 590: Loss = 0.1934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8879\n",
      "precision: 0.8803\n",
      "recall: 0.8979\n",
      "f1: 0.8890\n",
      "loss: 0.3272\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8817\n",
      "precision: 0.8718\n",
      "recall: 0.8950\n",
      "f1: 0.8832\n",
      "loss: 0.3187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 0: Loss = 0.3322\n",
      "Epoch 9, Step 10: Loss = 0.3156\n",
      "Epoch 9, Step 20: Loss = 0.2705\n",
      "Epoch 9, Step 30: Loss = 0.1074\n",
      "Epoch 9, Step 40: Loss = 0.1773\n",
      "Epoch 9, Step 50: Loss = 0.7843\n",
      "Epoch 9, Step 60: Loss = 0.3809\n",
      "Epoch 9, Step 70: Loss = 0.4093\n",
      "Epoch 9, Step 80: Loss = 0.3910\n",
      "Epoch 9, Step 90: Loss = 0.3058\n",
      "Epoch 9, Step 100: Loss = 0.3301\n",
      "Epoch 9, Step 110: Loss = 0.2834\n",
      "Epoch 9, Step 120: Loss = 0.2857\n",
      "Epoch 9, Step 130: Loss = 0.2809\n",
      "Epoch 9, Step 140: Loss = 0.3030\n",
      "Epoch 9, Step 150: Loss = 0.6060\n",
      "Epoch 9, Step 160: Loss = 0.3376\n",
      "Epoch 9, Step 170: Loss = 0.7047\n",
      "Epoch 9, Step 180: Loss = 0.1599\n",
      "Epoch 9, Step 190: Loss = 0.3878\n",
      "Epoch 9, Step 200: Loss = 0.2093\n",
      "Epoch 9, Step 210: Loss = 0.1912\n",
      "Epoch 9, Step 220: Loss = 0.7185\n",
      "Epoch 9, Step 230: Loss = 0.2558\n",
      "Epoch 9, Step 240: Loss = 0.4205\n",
      "Epoch 9, Step 250: Loss = 0.2906\n",
      "Epoch 9, Step 260: Loss = 0.4654\n",
      "Epoch 9, Step 270: Loss = 0.5212\n",
      "Epoch 9, Step 280: Loss = 0.2625\n",
      "Epoch 9, Step 290: Loss = 0.1758\n",
      "Epoch 9, Step 300: Loss = 0.2981\n",
      "Epoch 9, Step 310: Loss = 0.3532\n",
      "Epoch 9, Step 320: Loss = 0.2733\n",
      "Epoch 9, Step 330: Loss = 0.3199\n",
      "Epoch 9, Step 340: Loss = 0.1958\n",
      "Epoch 9, Step 350: Loss = 0.4147\n",
      "Epoch 9, Step 360: Loss = 0.2593\n",
      "Epoch 9, Step 370: Loss = 0.1822\n",
      "Epoch 9, Step 380: Loss = 0.2240\n",
      "Epoch 9, Step 390: Loss = 0.2177\n",
      "Epoch 9, Step 400: Loss = 0.4365\n",
      "Epoch 9, Step 410: Loss = 0.1950\n",
      "Epoch 9, Step 420: Loss = 0.2309\n",
      "Epoch 9, Step 430: Loss = 0.3662\n",
      "Epoch 9, Step 440: Loss = 0.3882\n",
      "Epoch 9, Step 450: Loss = 0.4285\n",
      "Epoch 9, Step 460: Loss = 0.2379\n",
      "Epoch 9, Step 470: Loss = 0.2965\n",
      "Epoch 9, Step 480: Loss = 0.2442\n",
      "Epoch 9, Step 490: Loss = 0.3559\n",
      "Epoch 9, Step 500: Loss = 1.6193\n",
      "Epoch 9, Step 510: Loss = 0.1531\n",
      "Epoch 9, Step 520: Loss = 0.2085\n",
      "Epoch 9, Step 530: Loss = 0.2883\n",
      "Epoch 9, Step 540: Loss = 0.2816\n",
      "Epoch 9, Step 550: Loss = 0.1969\n",
      "Epoch 9, Step 560: Loss = 0.3226\n",
      "Epoch 9, Step 570: Loss = 0.2183\n",
      "Epoch 9, Step 580: Loss = 0.7557\n",
      "Epoch 9, Step 590: Loss = 1.4464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8900\n",
      "precision: 0.8830\n",
      "recall: 0.8992\n",
      "f1: 0.8910\n",
      "loss: 0.3262\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8825\n",
      "precision: 0.8708\n",
      "recall: 0.8983\n",
      "f1: 0.8843\n",
      "loss: 0.3180\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "gc = None\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    BitsAndBytesConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        self.emails_df = emails_df.copy()\n",
    "        self.emails_df['sender'] = self.emails_df['sender'].apply(clean_text)\n",
    "        self.emails_df['subject'] = self.emails_df['subject'].apply(clean_text)\n",
    "        self.emails_df['body'] = self.emails_df['body'].apply(clean_text)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        email = self.emails_df.iloc[idx]\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(email['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    # initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # model configuration\n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "    # 8-bit quantization setup\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "    # LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, output_dir):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9):\n",
    "    best_val_metrics = {'f1': 0}\n",
    "    best_model_state = None\n",
    "    model = model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds, train_labels = [], []\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            if (step + 1) % 2 == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(batch['label'].cpu().numpy())\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "            torch.cuda.empty_cache()\n",
    "        train_metrics = compute_metrics(train_preds, train_labels)\n",
    "        train_metrics['loss'] = total_loss / len(train_loader)\n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print_metrics = lambda m: [print(f\"{k}: {v:.4f}\") for k, v in m.items() if k != 'confusion_matrix']\n",
    "        print(\"Training Metrics:\")\n",
    "        print_metrics(train_metrics)\n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        print_metrics(val_metrics)\n",
    "        if val_metrics['f1'] > best_val_metrics['f1']:\n",
    "            best_val_metrics = val_metrics\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    return best_model_state, best_val_metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    login(token=\"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\")\n",
    "    device = setup_environment()\n",
    "   \n",
    "    model_name = 'Qwen/Qwen3-8B'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/newdata_cleaned.csv\")\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    train_df, val_df = train_test_split(\n",
    "        emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42\n",
    "    )\n",
    "    train_dataset = EmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = EmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    best_model_state, best_metrics = train_model(\n",
    "        model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs\n",
    "    )\n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/binary_classification_Qwen\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # save PEFT-wrapped model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    # save training config and metrics\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"best_metrics\": {k: float(v) if k != 'confusion_matrix' else v.tolist() for k, v in best_metrics.items()}\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qwen \n",
    "Training Metrics:\n",
    "accuracy: 0.8900\n",
    "precision: 0.8830\n",
    "recall: 0.8992\n",
    "f1: 0.8910\n",
    "loss: 0.3262\n",
    "\n",
    "Validation Metrics:\n",
    "accuracy: 0.8825\n",
    "precision: 0.8708\n",
    "recall: 0.8983\n",
    "f1: 0.8843\n",
    "loss: 0.3180\n",
    "    \n",
    "    \n",
    "    \n",
    "Mistral \n",
    "Epoch 9 Summary:\n",
    "Training Metrics:\n",
    "accuracy: 0.7898\n",
    "precision: 0.7688\n",
    "recall: 0.8287\n",
    "f1: 0.7977\n",
    "loss: 0.9757\n",
    "\n",
    "Validation Metrics:\n",
    "accuracy: 0.7900\n",
    "precision: 0.7661\n",
    "recall: 0.8350\n",
    "f1: 0.7990\n",
    "loss: 1.0674\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Wizard\n",
    "Epoch 9 Summary:\n",
    "Training Metrics:\n",
    "accuracy: 0.7037\n",
    "precision: 0.7164\n",
    "recall: 0.6746\n",
    "f1: 0.6948\n",
    "loss: 1.6658\n",
    "\n",
    "Validation Metrics:\n",
    "accuracy: 0.7092\n",
    "precision: 0.7237\n",
    "recall: 0.6767\n",
    "f1: 0.6994\n",
    "loss: 1.7930\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "BERT\n",
    "Epoch 6 Summary:\n",
    "Training Metrics:\n",
    "accuracy: 0.9810\n",
    "precision: 0.9849\n",
    "recall: 0.9771\n",
    "f1: 0.9810\n",
    "loss: 0.0617\n",
    "\n",
    "Validation Metrics:\n",
    "accuracy: 0.9817\n",
    "precision: 0.9785\n",
    "recall: 0.9850\n",
    "f1: 0.9817\n",
    "loss: 0.0522\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Llama 8B\n",
    "Epoch 9 Summary:\n",
    "Training Metrics:\n",
    "accuracy: 0.8310\n",
    "precision: 0.8312\n",
    "recall: 0.8308\n",
    "f1: 0.8310\n",
    "loss: 0.4024\n",
    "\n",
    "Validation Metrics:\n",
    "accuracy: 0.8358\n",
    "precision: 0.8309\n",
    "recall: 0.8433\n",
    "f1: 0.8371\n",
    "loss: 0.4034\n",
    "    \n",
    "    \n",
    "Llama 7B\n",
    "Epoch 8 Summary:\n",
    "Training Metrics:\n",
    "accuracy: 0.7631\n",
    "precision: 0.7497\n",
    "recall: 0.7900\n",
    "f1: 0.7693\n",
    "loss: 0.5396\n",
    "\n",
    "Validation Metrics:\n",
    "accuracy: 0.7683\n",
    "precision: 0.7439\n",
    "recall: 0.8183\n",
    "f1: 0.7794\n",
    "loss: 0.5283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "gc = None\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    BitsAndBytesConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        self.emails_df = emails_df.copy()\n",
    "        self.emails_df['sender'] = self.emails_df['sender'].apply(clean_text)\n",
    "        self.emails_df['subject'] = self.emails_df['subject'].apply(clean_text)\n",
    "        self.emails_df['body'] = self.emails_df['body'].apply(clean_text)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        email = self.emails_df.iloc[idx]\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(email['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    # initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # model configuration\n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "    # 8-bit quantization setup\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "    # LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, output_dir):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9):\n",
    "    best_val_metrics = {'f1': 0}\n",
    "    best_model_state = None\n",
    "    model = model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds, train_labels = [], []\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            if (step + 1) % 2 == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(batch['label'].cpu().numpy())\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "            torch.cuda.empty_cache()\n",
    "        train_metrics = compute_metrics(train_preds, train_labels)\n",
    "        train_metrics['loss'] = total_loss / len(train_loader)\n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print_metrics = lambda m: [print(f\"{k}: {v:.4f}\") for k, v in m.items() if k != 'confusion_matrix']\n",
    "        print(\"Training Metrics:\")\n",
    "        print_metrics(train_metrics)\n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        print_metrics(val_metrics)\n",
    "        if val_metrics['f1'] > best_val_metrics['f1']:\n",
    "            best_val_metrics = val_metrics\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    return best_model_state, best_val_metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    login(token=\"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\")\n",
    "    device = setup_environment()\n",
    "   \n",
    "    model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/newdata_cleaned.csv\")\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    train_df, val_df = train_test_split(\n",
    "        emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42\n",
    "    )\n",
    "    train_dataset = EmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = EmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    best_model_state, best_metrics = train_model(\n",
    "        model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs\n",
    "    )\n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/binary_classification_DistillDeepSeek\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # save PEFT-wrapped model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    # save training config and metrics\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"best_metrics\": {k: float(v) if k != 'confusion_matrix' else v.tolist() for k, v in best_metrics.items()}\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
