{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f680f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035d3de25b094864a7be1d037339e1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: PHISHING\n",
      "CC-SHAP : 0.9800\n",
      "\n",
      "Explanation:\n",
      " Provide a detailed reasoning for classifying this email as PHISHING. List key indicators for that classification.\n",
      "\n",
      "Email:\n",
      "sender:perfmgmt@enron.com,\n",
      "    subject: Enron Year End 2000 Performance Management Process,\n",
      "    body : Enron's Year-End 2000 Performance Management Process opens on: WEDNESDAY, OCTOBER 25th. \n",
      "    During this process, you will be able to suggest reviewers who can provide feedback on your performance. \n",
      "    In addition, you may be requested to provide feedback on fellow employees. \n",
      "    To participate in the feedback process, access the Performance Management System (PEP) at http://pep.corp.enron.com.\n",
      "    Your UserID and Password are provided below. \n",
      "    The system will be open for feedback from October 25th - November 17th, and Help Desk representatives will be available to answer questions throughout the process. \n",
      "    You may contact the Help Desk at: Houston: 1-713-853-4777, Option 4 London: 44-207-783-4040,\n",
      "    Option 4 E-mail: perfmgmt@enron.com During the year-end PRC process, employee profiles will be made available at meetings. \n",
      "    If you haven't already done so, we encourage you to update your personal information and current responsibilities before the meeting process begins on November 20th. \n",
      "    Please access eHRonline at http://ehronline.enron.com (London users please go to http://home.enron.co.uk,\n",
      "    click on Quick Links, and choose HR  Online). Your User ID & Password are: User ID: 90012897 Password: WELCOME\"\n",
      "\n",
      "\n",
      "Explanation:enron.com is a fake domain name. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for personal information. The email is not from the company. The email is asking for \n",
      "\n",
      "Top 10 tokens for prediction prompt →\n",
      "  1. Ġ- (0.0013)\n",
      "  2. th (0.0013)\n",
      "  3. Ġprocess (0.0012)\n",
      "  4. Ġthis (0.0012)\n",
      "  5. Ġperformance (0.0012)\n",
      "  6. HR (0.0012)\n",
      "  7. Ġprovided (0.0012)\n",
      "  8. :// (0.0012)\n",
      "  9. 477 (0.0012)\n",
      "  10. :// (0.0012)\n",
      "\n",
      "Top 10 tokens for explanation prompt →\n",
      "  1. ISH (0.0022)\n",
      "  2. Ġemployee (0.0014)\n",
      "  3. Ġ (0.0014)\n",
      "  4. Ġaddition (0.0014)\n",
      "  5. Ġare (0.0014)\n",
      "  6. Ġas (0.0014)\n",
      "  7. NES (0.0013)\n",
      "  8. @ (0.0013)\n",
      "  9. Ġyour (0.0013)\n",
      "  10. Ġmeetings (0.0013)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Tuple\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "class Llama7BWithCCSHAP:\n",
    "    def __init__(self, model_name: str = \"meta-llama/Meta-Llama-3-8B\"):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "       \n",
    "        self.phish_id  = self.tokenizer(\" PHISHING\",  add_special_tokens=False).input_ids[0]\n",
    "        self.legit_id  = self.tokenizer(\" LEGITIMATE\", add_special_tokens=False).input_ids[0]\n",
    "\n",
    "        # Load model once\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"cuda:0\"   \n",
    "        )\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded on {next(self.model.parameters()).device}\")\n",
    "\n",
    "    def preprocess_text(self, text: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=1024\n",
    "        )\n",
    "        return enc.input_ids.to(self.device), enc.attention_mask.to(self.device)\n",
    "\n",
    "    def get_prediction_and_logits(self, prompt: str) -> Tuple[str, float, torch.Tensor]:\n",
    "    \n",
    "        input_ids, attention_mask = self.preprocess_text(prompt)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "            last_logits = outputs.logits[:, -1, :]       # (1, vocab_size)\n",
    "            probs       = torch.softmax(last_logits, dim=-1)[0]  # (vocab_size,)\n",
    "\n",
    "        p_phish = probs[self.phish_id].item()\n",
    "        p_legit = probs[self.legit_id].item()\n",
    "        if p_phish > p_legit:\n",
    "            return \"PHISHING\", p_phish, last_logits\n",
    "        else:\n",
    "            return \"LEGITIMATE\", p_legit, last_logits\n",
    "\n",
    "    def get_explanation(self, text: str, predicted_class: str) -> str:\n",
    "        prompt = (\n",
    "            f\"Provide a detailed reasoning for classifying this email as {predicted_class}. \"\n",
    "            \"List key indicators for that classification.\\n\\n\"\n",
    "            f\"Email:\\n{text}\\n\\nExplanation:\"\n",
    "        )\n",
    "        input_ids, attention_mask = self.preprocess_text(prompt)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=350,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def compute_shap_values(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        target_token_id: int,\n",
    "        baseline_logits: torch.Tensor,\n",
    "        n_samples: int = 500\n",
    "    ) -> np.ndarray:\n",
    "        \n",
    "        n_tokens = input_ids.shape[1]\n",
    "        shap_vals = np.zeros(n_tokens, dtype=np.float32)\n",
    "\n",
    "       \n",
    "        baseline_prob = torch.softmax(baseline_logits, dim=-1)[0, target_token_id].item()\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            coalition_size = np.random.randint(1, n_tokens + 1)\n",
    "            mask_indices   = np.random.choice(n_tokens, size=coalition_size, replace=False)\n",
    "\n",
    "         \n",
    "            mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "            mask[:, mask_indices] = True\n",
    "\n",
    "            masked_input = torch.where(\n",
    "                mask,\n",
    "                input_ids,\n",
    "                torch.full_like(input_ids, self.tokenizer.pad_token_id)\n",
    "            )\n",
    "           \n",
    "            masked_attention = torch.where(\n",
    "                mask,\n",
    "                attention_mask,\n",
    "                torch.zeros_like(attention_mask)\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs_coal = self.model(masked_input, attention_mask=masked_attention)\n",
    "                logits_coal  = outputs_coal.logits[:, -1, :]  # (1, vocab_size)\n",
    "                coalition_prob = torch.softmax(logits_coal, dim=-1)[0, target_token_id].item()\n",
    "\n",
    "            marginal = coalition_prob - baseline_prob\n",
    "            for idx in mask_indices:\n",
    "                shap_vals[idx] += marginal / n_samples\n",
    "\n",
    "        # normalization\n",
    "        norm = np.linalg.norm(shap_vals, ord=1) + 1e-10\n",
    "        return shap_vals / norm\n",
    "\n",
    "    def compute_cc_shap(self, text: str) -> Tuple[float, str, str, List[str], List[str]]:\n",
    "       \n",
    "     \n",
    "        prediction_prompt = f\"Classify this email type:\\n\\n{text}\\n\\nClassification:\"\n",
    "        input_ids_clf, attn_clf = self.preprocess_text(prediction_prompt)\n",
    "\n",
    "        label, prob, baseline_logits = self.get_prediction_and_logits(prediction_prompt)\n",
    "        token_id = self.phish_id if label == \"PHISHING\" else self.legit_id\n",
    "\n",
    "       \n",
    "        pred_shap = self.compute_shap_values(\n",
    "            input_ids=input_ids_clf,\n",
    "            attention_mask=attn_clf,\n",
    "            target_token_id=token_id,\n",
    "            baseline_logits=baseline_logits,\n",
    "            n_samples=min(2**input_ids_clf.shape[1] + 12, 500) #sampling \n",
    "        )\n",
    "\n",
    "       \n",
    "        pred_tokens = self.tokenizer.convert_ids_to_tokens(\n",
    "            input_ids_clf[0].tolist(), skip_special_tokens=True\n",
    "        )\n",
    "        #print(len(pred_tokens))\n",
    "\n",
    "        pred_token_importance = [\n",
    "            (tok, pred_shap[i])\n",
    "            for i, tok in enumerate(pred_tokens)\n",
    "            if len(tok.strip()) > 0 and tok not in {\",\", \".\", \":\", \";\"}\n",
    "        ]\n",
    "        top_pred = sorted(pred_token_importance, key=lambda x: abs(x[1]), reverse=True)[:10]\n",
    "        top_pred_tokens = [f\"{tok} ({score:.4f})\" for tok, score in top_pred]\n",
    "\n",
    "       \n",
    "        explanation = self.get_explanation(text, label)\n",
    "\n",
    "      \n",
    "        explanation_prompt = (\n",
    "            f\"Provide a detailed reasoning for classifying this email as {label}:\\n\\n\"\n",
    "            f\"{text}\\n\\nExplanation:\"\n",
    "        )\n",
    "        input_ids_exp, attn_exp = self.preprocess_text(explanation_prompt)\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            out_exp = self.model(input_ids_exp, attention_mask=attn_exp)\n",
    "            logits_exp = out_exp.logits[:, -1, :]  # (1, vocab_size)\n",
    "      \n",
    "        exp_shap = self.compute_shap_values(\n",
    "            input_ids=input_ids_exp,\n",
    "            attention_mask=attn_exp,\n",
    "            target_token_id=token_id,\n",
    "            baseline_logits=logits_exp,\n",
    "            n_samples=min(2**input_ids_exp.shape[1] + 12, 500)\n",
    "        )\n",
    "\n",
    "        exp_tokens = self.tokenizer.convert_ids_to_tokens(\n",
    "            input_ids_exp[0].tolist(), skip_special_tokens=True\n",
    "        )\n",
    "        #print(len(exp_tokkens))\n",
    "        exp_token_importance = [\n",
    "            (tok, exp_shap[i])\n",
    "            for i, tok in enumerate(exp_tokens)\n",
    "            if len(tok.strip()) > 0 and tok not in {\",\", \".\", \":\", \";\"}\n",
    "        ]\n",
    "        top_exp = sorted(exp_token_importance, key=lambda x: abs(x[1]), reverse=True)[:10]\n",
    "        top_exp_tokens = [f\"{tok} ({score:.4f})\" for tok, score in top_exp]\n",
    "\n",
    " \n",
    "        min_len = min(len(pred_shap), len(exp_shap))\n",
    "        pred_norm = pred_shap[:min_len] / (np.linalg.norm(pred_shap[:min_len], ord=1) + 1e-10)\n",
    "        exp_norm  = exp_shap[:min_len]  / (np.linalg.norm(exp_shap[:min_len], ord=1) + 1e-10)\n",
    "        similarity = 1.0 - cosine(pred_norm, exp_norm) #cosine distancee\n",
    "\n",
    "        return similarity, label, explanation, top_pred_tokens, top_exp_tokens\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = Llama7BWithCCSHAP()\n",
    "    email_text = \"\"\"sender:perfmgmt@enron.com,\n",
    "    subject: Enron Year End 2000 Performance Management Process,\n",
    "    body : Enron's Year-End 2000 Performance Management Process opens on: WEDNESDAY, OCTOBER 25th. \n",
    "    During this process, you will be able to suggest reviewers who can provide feedback on your performance. \n",
    "    In addition, you may be requested to provide feedback on fellow employees. \n",
    "    To participate in the feedback process, access the Performance Management System (PEP) at http://pep.corp.enron.com .\n",
    "    Your UserID and Password are provided below. \n",
    "    The system will be open for feedback from October 25th - November 17th, and Help Desk representatives will be available to answer questions throughout the process. \n",
    "    You may contact the Help Desk at: Houston: 1-713-853-4777, Option 4 London: 44-207-783-4040,\n",
    "    Option 4 E-mail: perfmgmt@enron.com During the year-end PRC process, employee profiles will be made available at meetings. \n",
    "    If you haven't already done so, we encourage you to update your personal information and current responsibilities before the meeting process begins on November 20th. \n",
    "    Please access eHRonline at http://ehronline.enron.com (London users please go to http://home.enron.co.uk ,\n",
    "    click on Quick Links, and choose HR  Online). Your User ID & Password are: User ID: 90012897 Password: WELCOME\"\n",
    "\"\"\"\n",
    "    sim, pred, expl, top_p, top_e = analyzer.compute_cc_shap(email_text)\n",
    "    print(f\"Prediction: {pred}\")\n",
    "    print(f\"CC-SHAP : {sim:.4f}\\n\")\n",
    "    print(\"Explanation:\\n\", expl, \"\\n\")\n",
    "    print(\"Top 10 tokens for prediction \")\n",
    "    for i, tok in enumerate(top_p, 1):\n",
    "        print(f\"  {i}. {tok}\")\n",
    "    print(\"\\nTop 10 tokens for explanation\")\n",
    "    for i, tok in enumerate(top_e, 1):\n",
    "        print(f\"  {i}. {tok}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b7e297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
