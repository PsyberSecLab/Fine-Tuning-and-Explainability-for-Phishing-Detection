{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38eb5deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b492825b70e436fb5d9ef761b063e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 0.8101\n",
      "Epoch 1, Step 10: Loss = 0.6461\n",
      "Epoch 1, Step 20: Loss = 0.5980\n",
      "Epoch 1, Step 30: Loss = 0.7918\n",
      "Epoch 1, Step 40: Loss = 0.8680\n",
      "Epoch 1, Step 50: Loss = 0.5674\n",
      "Epoch 1, Step 60: Loss = 0.5396\n",
      "Epoch 1, Step 70: Loss = 0.7503\n",
      "Epoch 1, Step 80: Loss = 0.7560\n",
      "Epoch 1, Step 90: Loss = 0.5677\n",
      "Epoch 1, Step 100: Loss = 0.8650\n",
      "Epoch 1, Step 110: Loss = 0.8832\n",
      "Epoch 1, Step 120: Loss = 0.5864\n",
      "Epoch 1, Step 130: Loss = 0.9844\n",
      "Epoch 1, Step 140: Loss = 0.4921\n",
      "Epoch 1, Step 150: Loss = 1.0096\n",
      "Epoch 1, Step 160: Loss = 0.5531\n",
      "Epoch 1, Step 170: Loss = 0.5686\n",
      "Epoch 1, Step 180: Loss = 0.9962\n",
      "Epoch 1, Step 190: Loss = 0.9772\n",
      "Epoch 1, Step 200: Loss = 0.7517\n",
      "Epoch 1, Step 210: Loss = 0.5499\n",
      "Epoch 1, Step 220: Loss = 0.5828\n",
      "Epoch 1, Step 230: Loss = 0.6110\n",
      "Epoch 1, Step 240: Loss = 0.5795\n",
      "Epoch 1, Step 250: Loss = 0.8249\n",
      "Epoch 1, Step 260: Loss = 0.5340\n",
      "Epoch 1, Step 270: Loss = 0.6975\n",
      "Epoch 1, Step 280: Loss = 0.4896\n",
      "Epoch 1, Step 290: Loss = 0.6943\n",
      "Epoch 1, Step 300: Loss = 0.7048\n",
      "Epoch 1, Step 310: Loss = 0.9780\n",
      "Epoch 1, Step 320: Loss = 0.5245\n",
      "Epoch 1, Step 330: Loss = 0.6012\n",
      "Epoch 1, Step 340: Loss = 0.6199\n",
      "Epoch 1, Step 350: Loss = 0.8191\n",
      "Epoch 1, Step 360: Loss = 0.7020\n",
      "Epoch 1, Step 370: Loss = 0.6537\n",
      "Epoch 1, Step 380: Loss = 0.6298\n",
      "Epoch 1, Step 390: Loss = 0.7064\n",
      "Epoch 1, Step 400: Loss = 0.4281\n",
      "Epoch 1, Step 410: Loss = 0.7104\n",
      "Epoch 1, Step 420: Loss = 0.6980\n",
      "Epoch 1, Step 430: Loss = 0.5861\n",
      "Epoch 1, Step 440: Loss = 0.8869\n",
      "Epoch 1, Step 450: Loss = 0.7817\n",
      "Epoch 1, Step 460: Loss = 0.7027\n",
      "Epoch 1, Step 470: Loss = 0.6194\n",
      "Epoch 1, Step 480: Loss = 0.5027\n",
      "Epoch 1, Step 490: Loss = 0.6903\n",
      "Epoch 1, Step 500: Loss = 0.6740\n",
      "Epoch 1, Step 510: Loss = 0.5646\n",
      "Epoch 1, Step 520: Loss = 0.3883\n",
      "Epoch 1, Step 530: Loss = 0.4492\n",
      "Epoch 1, Step 540: Loss = 0.6749\n",
      "Epoch 1, Step 550: Loss = 0.9467\n",
      "Epoch 1, Step 560: Loss = 0.7162\n",
      "Epoch 1, Step 570: Loss = 0.6123\n",
      "Epoch 1, Step 580: Loss = 0.5333\n",
      "Epoch 1, Step 590: Loss = 0.5768\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Average Training Loss: 1.3545\n",
      "Validation Metrics:\n",
      "Val_loss: 1.2940\n",
      "Epoch 2, Step 0: Loss = 0.5200\n",
      "Epoch 2, Step 10: Loss = 0.5473\n",
      "Epoch 2, Step 20: Loss = 0.5176\n",
      "Epoch 2, Step 30: Loss = 0.5507\n",
      "Epoch 2, Step 40: Loss = 0.7617\n",
      "Epoch 2, Step 50: Loss = 0.5756\n",
      "Epoch 2, Step 60: Loss = 0.6211\n",
      "Epoch 2, Step 70: Loss = 0.5298\n",
      "Epoch 2, Step 80: Loss = 0.8291\n",
      "Epoch 2, Step 90: Loss = 0.6355\n",
      "Epoch 2, Step 100: Loss = 0.5306\n",
      "Epoch 2, Step 110: Loss = 0.5518\n",
      "Epoch 2, Step 120: Loss = 0.6748\n",
      "Epoch 2, Step 130: Loss = 0.5794\n",
      "Epoch 2, Step 140: Loss = 0.5409\n",
      "Epoch 2, Step 150: Loss = 0.6561\n",
      "Epoch 2, Step 160: Loss = 0.4379\n",
      "Epoch 2, Step 170: Loss = 0.3144\n",
      "Epoch 2, Step 180: Loss = 0.4658\n",
      "Epoch 2, Step 190: Loss = 0.5565\n",
      "Epoch 2, Step 200: Loss = 0.7224\n",
      "Epoch 2, Step 210: Loss = 0.2938\n",
      "Epoch 2, Step 220: Loss = 0.3189\n",
      "Epoch 2, Step 230: Loss = 0.6537\n",
      "Epoch 2, Step 240: Loss = 0.3647\n",
      "Epoch 2, Step 250: Loss = 0.5693\n",
      "Epoch 2, Step 260: Loss = 0.3227\n",
      "Epoch 2, Step 270: Loss = 0.6377\n",
      "Epoch 2, Step 280: Loss = 0.4912\n",
      "Epoch 2, Step 290: Loss = 0.3045\n",
      "Epoch 2, Step 300: Loss = 0.7573\n",
      "Epoch 2, Step 310: Loss = 0.5297\n",
      "Epoch 2, Step 320: Loss = 0.4734\n",
      "Epoch 2, Step 330: Loss = 0.4470\n",
      "Epoch 2, Step 340: Loss = 0.5127\n",
      "Epoch 2, Step 350: Loss = 0.3046\n",
      "Epoch 2, Step 360: Loss = 0.3697\n",
      "Epoch 2, Step 370: Loss = 0.2748\n",
      "Epoch 2, Step 380: Loss = 0.4401\n",
      "Epoch 2, Step 390: Loss = 0.3180\n",
      "Epoch 2, Step 400: Loss = 0.5119\n",
      "Epoch 2, Step 410: Loss = 0.5445\n",
      "Epoch 2, Step 420: Loss = 0.3211\n",
      "Epoch 2, Step 430: Loss = 0.3704\n",
      "Epoch 2, Step 440: Loss = 0.7003\n",
      "Epoch 2, Step 450: Loss = 0.2956\n",
      "Epoch 2, Step 460: Loss = 0.5609\n",
      "Epoch 2, Step 470: Loss = 0.7347\n",
      "Epoch 2, Step 480: Loss = 0.4775\n",
      "Epoch 2, Step 490: Loss = 0.3840\n",
      "Epoch 2, Step 500: Loss = 0.2426\n",
      "Epoch 2, Step 510: Loss = 0.2637\n",
      "Epoch 2, Step 520: Loss = 0.3231\n",
      "Epoch 2, Step 530: Loss = 0.3029\n",
      "Epoch 2, Step 540: Loss = 0.3935\n",
      "Epoch 2, Step 550: Loss = 0.4477\n",
      "Epoch 2, Step 560: Loss = 0.4899\n",
      "Epoch 2, Step 570: Loss = 0.1478\n",
      "Epoch 2, Step 580: Loss = 0.4393\n",
      "Epoch 2, Step 590: Loss = 0.5491\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Average Training Loss: 0.9887\n",
      "Validation Metrics:\n",
      "Val_loss: 0.6875\n",
      "Epoch 3, Step 0: Loss = 0.3351\n",
      "Epoch 3, Step 10: Loss = 0.4169\n",
      "Epoch 3, Step 20: Loss = 0.3929\n",
      "Epoch 3, Step 30: Loss = 0.3059\n",
      "Epoch 3, Step 40: Loss = 0.1299\n",
      "Epoch 3, Step 50: Loss = 0.2247\n",
      "Epoch 3, Step 60: Loss = 0.0469\n",
      "Epoch 3, Step 70: Loss = 0.2125\n",
      "Epoch 3, Step 80: Loss = 0.4480\n",
      "Epoch 3, Step 90: Loss = 0.4366\n",
      "Epoch 3, Step 100: Loss = 0.5304\n",
      "Epoch 3, Step 110: Loss = 0.4630\n",
      "Epoch 3, Step 120: Loss = 0.2377\n",
      "Epoch 3, Step 130: Loss = 0.5122\n",
      "Epoch 3, Step 140: Loss = 0.1125\n",
      "Epoch 3, Step 150: Loss = 0.0963\n",
      "Epoch 3, Step 160: Loss = 0.0542\n",
      "Epoch 3, Step 170: Loss = 0.3767\n",
      "Epoch 3, Step 180: Loss = 0.4194\n",
      "Epoch 3, Step 190: Loss = 0.5794\n",
      "Epoch 3, Step 200: Loss = 0.5661\n",
      "Epoch 3, Step 210: Loss = 0.0812\n",
      "Epoch 3, Step 220: Loss = 0.4539\n",
      "Epoch 3, Step 230: Loss = 0.2339\n",
      "Epoch 3, Step 240: Loss = 0.1532\n",
      "Epoch 3, Step 250: Loss = 0.1041\n",
      "Epoch 3, Step 260: Loss = 0.0437\n",
      "Epoch 3, Step 270: Loss = 0.3324\n",
      "Epoch 3, Step 280: Loss = 0.5209\n",
      "Epoch 3, Step 290: Loss = 0.3761\n",
      "Epoch 3, Step 300: Loss = 0.2052\n",
      "Epoch 3, Step 310: Loss = 0.1447\n",
      "Epoch 3, Step 320: Loss = 0.2139\n",
      "Epoch 3, Step 330: Loss = 0.2865\n",
      "Epoch 3, Step 340: Loss = 0.3292\n",
      "Epoch 3, Step 350: Loss = 0.1441\n",
      "Epoch 3, Step 360: Loss = 0.0647\n",
      "Epoch 3, Step 370: Loss = 0.2112\n",
      "Epoch 3, Step 380: Loss = 0.1886\n",
      "Epoch 3, Step 390: Loss = 0.1623\n",
      "Epoch 3, Step 400: Loss = 0.1021\n",
      "Epoch 3, Step 410: Loss = 0.3140\n",
      "Epoch 3, Step 420: Loss = 0.1921\n",
      "Epoch 3, Step 430: Loss = 0.4484\n",
      "Epoch 3, Step 440: Loss = 0.2375\n",
      "Epoch 3, Step 450: Loss = 0.5838\n",
      "Epoch 3, Step 460: Loss = 0.2786\n",
      "Epoch 3, Step 470: Loss = 0.2966\n",
      "Epoch 3, Step 480: Loss = 0.1354\n",
      "Epoch 3, Step 490: Loss = 0.2695\n",
      "Epoch 3, Step 500: Loss = 0.1463\n",
      "Epoch 3, Step 510: Loss = 0.2223\n",
      "Epoch 3, Step 520: Loss = 0.2503\n",
      "Epoch 3, Step 530: Loss = 0.3056\n",
      "Epoch 3, Step 540: Loss = 0.7103\n",
      "Epoch 3, Step 550: Loss = 0.3898\n",
      "Epoch 3, Step 560: Loss = 0.2754\n",
      "Epoch 3, Step 570: Loss = 0.2330\n",
      "Epoch 3, Step 580: Loss = 0.1392\n",
      "Epoch 3, Step 590: Loss = 0.2162\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Average Training Loss: 0.5647\n",
      "Validation Metrics:\n",
      "Val_loss: 0.4864\n",
      "Epoch 4, Step 0: Loss = 0.2281\n",
      "Epoch 4, Step 10: Loss = 0.4929\n",
      "Epoch 4, Step 20: Loss = 0.2199\n",
      "Epoch 4, Step 30: Loss = 0.1737\n",
      "Epoch 4, Step 40: Loss = 0.1765\n",
      "Epoch 4, Step 50: Loss = 0.0793\n",
      "Epoch 4, Step 60: Loss = 0.1525\n",
      "Epoch 4, Step 70: Loss = 0.8055\n",
      "Epoch 4, Step 80: Loss = 0.2226\n",
      "Epoch 4, Step 90: Loss = 0.0515\n",
      "Epoch 4, Step 100: Loss = 0.2092\n",
      "Epoch 4, Step 110: Loss = 0.3019\n",
      "Epoch 4, Step 120: Loss = 0.4339\n",
      "Epoch 4, Step 130: Loss = 0.1658\n",
      "Epoch 4, Step 140: Loss = 0.3117\n",
      "Epoch 4, Step 150: Loss = 0.1898\n",
      "Epoch 4, Step 160: Loss = 0.6808\n",
      "Epoch 4, Step 170: Loss = 0.3778\n",
      "Epoch 4, Step 180: Loss = 0.1641\n",
      "Epoch 4, Step 190: Loss = 0.1951\n",
      "Epoch 4, Step 200: Loss = 0.0449\n",
      "Epoch 4, Step 210: Loss = 0.0748\n",
      "Epoch 4, Step 220: Loss = 0.3734\n",
      "Epoch 4, Step 230: Loss = 0.1583\n",
      "Epoch 4, Step 240: Loss = 0.5419\n",
      "Epoch 4, Step 250: Loss = 0.4000\n",
      "Epoch 4, Step 260: Loss = 0.2826\n",
      "Epoch 4, Step 270: Loss = 0.1328\n",
      "Epoch 4, Step 280: Loss = 0.3189\n",
      "Epoch 4, Step 290: Loss = 0.0094\n",
      "Epoch 4, Step 300: Loss = 0.3049\n",
      "Epoch 4, Step 310: Loss = 0.1164\n",
      "Epoch 4, Step 320: Loss = 0.2518\n",
      "Epoch 4, Step 330: Loss = 0.1640\n",
      "Epoch 4, Step 340: Loss = 0.2216\n",
      "Epoch 4, Step 350: Loss = 0.1966\n",
      "Epoch 4, Step 360: Loss = 0.4481\n",
      "Epoch 4, Step 370: Loss = 0.2362\n",
      "Epoch 4, Step 380: Loss = 0.1330\n",
      "Epoch 4, Step 390: Loss = 0.2717\n",
      "Epoch 4, Step 400: Loss = 0.1372\n",
      "Epoch 4, Step 410: Loss = 0.3134\n",
      "Epoch 4, Step 420: Loss = 0.2629\n",
      "Epoch 4, Step 430: Loss = 0.2057\n",
      "Epoch 4, Step 440: Loss = 0.0696\n",
      "Epoch 4, Step 450: Loss = 0.2246\n",
      "Epoch 4, Step 460: Loss = 0.0499\n",
      "Epoch 4, Step 470: Loss = 0.4332\n",
      "Epoch 4, Step 480: Loss = 0.0628\n",
      "Epoch 4, Step 490: Loss = 0.1760\n",
      "Epoch 4, Step 500: Loss = 0.4864\n",
      "Epoch 4, Step 510: Loss = 0.0023\n",
      "Epoch 4, Step 520: Loss = 0.1868\n",
      "Epoch 4, Step 530: Loss = 0.3367\n",
      "Epoch 4, Step 540: Loss = 0.0498\n",
      "Epoch 4, Step 550: Loss = 0.1195\n",
      "Epoch 4, Step 560: Loss = 0.0456\n",
      "Epoch 4, Step 570: Loss = 0.1768\n",
      "Epoch 4, Step 580: Loss = 0.1838\n",
      "Epoch 4, Step 590: Loss = 0.0025\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Average Training Loss: 0.4314\n",
      "Validation Metrics:\n",
      "Val_loss: 0.4209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.2649\n",
      "Epoch 5, Step 10: Loss = 0.1631\n",
      "Epoch 5, Step 20: Loss = 0.0628\n",
      "Epoch 5, Step 30: Loss = 0.0623\n",
      "Epoch 5, Step 40: Loss = 0.3664\n",
      "Epoch 5, Step 50: Loss = 0.3920\n",
      "Epoch 5, Step 60: Loss = 0.0342\n",
      "Epoch 5, Step 70: Loss = 0.0493\n",
      "Epoch 5, Step 80: Loss = 0.1032\n",
      "Epoch 5, Step 90: Loss = 0.1389\n",
      "Epoch 5, Step 100: Loss = 0.1382\n",
      "Epoch 5, Step 110: Loss = 0.2513\n",
      "Epoch 5, Step 120: Loss = 0.4962\n",
      "Epoch 5, Step 130: Loss = 0.3531\n",
      "Epoch 5, Step 140: Loss = 0.2061\n",
      "Epoch 5, Step 150: Loss = 0.1699\n",
      "Epoch 5, Step 160: Loss = 0.1876\n",
      "Epoch 5, Step 170: Loss = 0.1907\n",
      "Epoch 5, Step 180: Loss = 0.1449\n",
      "Epoch 5, Step 190: Loss = 0.2178\n",
      "Epoch 5, Step 200: Loss = 0.0777\n",
      "Epoch 5, Step 210: Loss = 0.2252\n",
      "Epoch 5, Step 220: Loss = 0.3895\n",
      "Epoch 5, Step 230: Loss = 0.3966\n",
      "Epoch 5, Step 240: Loss = 0.2991\n",
      "Epoch 5, Step 250: Loss = 0.0000\n",
      "Epoch 5, Step 260: Loss = 0.3903\n",
      "Epoch 5, Step 270: Loss = 0.1708\n",
      "Epoch 5, Step 280: Loss = 0.5249\n",
      "Epoch 5, Step 290: Loss = 0.2350\n",
      "Epoch 5, Step 300: Loss = 0.0199\n",
      "Epoch 5, Step 310: Loss = 0.0818\n",
      "Epoch 5, Step 320: Loss = 0.3774\n",
      "Epoch 5, Step 330: Loss = 0.2455\n",
      "Epoch 5, Step 340: Loss = 0.1380\n",
      "Epoch 5, Step 350: Loss = 0.2345\n",
      "Epoch 5, Step 360: Loss = 0.0266\n",
      "Epoch 5, Step 370: Loss = 0.0539\n",
      "Epoch 5, Step 380: Loss = 0.1930\n",
      "Epoch 5, Step 390: Loss = 0.2790\n",
      "Epoch 5, Step 400: Loss = 0.0000\n",
      "Epoch 5, Step 410: Loss = 0.3498\n",
      "Epoch 5, Step 420: Loss = 0.0639\n",
      "Epoch 5, Step 430: Loss = 0.1307\n",
      "Epoch 5, Step 440: Loss = 0.5892\n",
      "Epoch 5, Step 450: Loss = 0.1440\n",
      "Epoch 5, Step 460: Loss = 0.1985\n",
      "Epoch 5, Step 470: Loss = 0.1575\n",
      "Epoch 5, Step 480: Loss = 0.0000\n",
      "Epoch 5, Step 490: Loss = 0.1572\n",
      "Epoch 5, Step 500: Loss = 0.1105\n",
      "Epoch 5, Step 510: Loss = 0.1675\n",
      "Epoch 5, Step 520: Loss = 0.3282\n",
      "Epoch 5, Step 530: Loss = 0.0967\n",
      "Epoch 5, Step 540: Loss = 0.0737\n",
      "Epoch 5, Step 550: Loss = 0.0443\n",
      "Epoch 5, Step 560: Loss = 0.2375\n",
      "Epoch 5, Step 570: Loss = 0.1340\n",
      "Epoch 5, Step 580: Loss = 0.0231\n",
      "Epoch 5, Step 590: Loss = 0.0729\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Average Training Loss: 0.3878\n",
      "Validation Metrics:\n",
      "Val_loss: 0.3555\n",
      "Epoch 6, Step 0: Loss = 0.2112\n",
      "Epoch 6, Step 10: Loss = 0.1628\n",
      "Epoch 6, Step 20: Loss = 0.1584\n",
      "Epoch 6, Step 30: Loss = 0.0994\n",
      "Epoch 6, Step 40: Loss = 0.2869\n",
      "Epoch 6, Step 50: Loss = 0.2889\n",
      "Epoch 6, Step 60: Loss = 0.0855\n",
      "Epoch 6, Step 70: Loss = 0.0667\n",
      "Epoch 6, Step 80: Loss = 0.1561\n",
      "Epoch 6, Step 90: Loss = 0.2368\n",
      "Epoch 6, Step 100: Loss = 0.1828\n",
      "Epoch 6, Step 110: Loss = 0.1270\n",
      "Epoch 6, Step 120: Loss = 0.0126\n",
      "Epoch 6, Step 130: Loss = 0.0724\n",
      "Epoch 6, Step 140: Loss = 0.0688\n",
      "Epoch 6, Step 150: Loss = 0.5823\n",
      "Epoch 6, Step 160: Loss = 0.1292\n",
      "Epoch 6, Step 170: Loss = 0.0292\n",
      "Epoch 6, Step 180: Loss = 0.1429\n",
      "Epoch 6, Step 190: Loss = 0.0160\n",
      "Epoch 6, Step 200: Loss = 0.2923\n",
      "Epoch 6, Step 210: Loss = 0.2217\n",
      "Epoch 6, Step 220: Loss = 0.0977\n",
      "Epoch 6, Step 230: Loss = 0.0921\n",
      "Epoch 6, Step 240: Loss = 0.0823\n",
      "Epoch 6, Step 250: Loss = 0.3477\n",
      "Epoch 6, Step 260: Loss = 0.1651\n",
      "Epoch 6, Step 270: Loss = 0.2002\n",
      "Epoch 6, Step 280: Loss = 0.4605\n",
      "Epoch 6, Step 290: Loss = 0.5912\n",
      "Epoch 6, Step 300: Loss = 0.0286\n",
      "Epoch 6, Step 310: Loss = 0.1310\n",
      "Epoch 6, Step 320: Loss = 0.1821\n",
      "Epoch 6, Step 330: Loss = 0.0850\n",
      "Epoch 6, Step 340: Loss = 0.1001\n",
      "Epoch 6, Step 350: Loss = 0.0423\n",
      "Epoch 6, Step 360: Loss = 0.0832\n",
      "Epoch 6, Step 370: Loss = 0.1401\n",
      "Epoch 6, Step 380: Loss = 0.1674\n",
      "Epoch 6, Step 390: Loss = 0.1726\n",
      "Epoch 6, Step 400: Loss = 0.1692\n",
      "Epoch 6, Step 410: Loss = 0.0457\n",
      "Epoch 6, Step 420: Loss = 0.3365\n",
      "Epoch 6, Step 430: Loss = 0.2706\n",
      "Epoch 6, Step 440: Loss = 0.1961\n",
      "Epoch 6, Step 450: Loss = 0.0141\n",
      "Epoch 6, Step 460: Loss = 0.1159\n",
      "Epoch 6, Step 470: Loss = 0.1837\n",
      "Epoch 6, Step 480: Loss = 0.0120\n",
      "Epoch 6, Step 490: Loss = 0.0310\n",
      "Epoch 6, Step 500: Loss = 0.0961\n",
      "Epoch 6, Step 510: Loss = 0.0094\n",
      "Epoch 6, Step 520: Loss = 0.2949\n",
      "Epoch 6, Step 530: Loss = 0.0576\n",
      "Epoch 6, Step 540: Loss = 0.0254\n",
      "Epoch 6, Step 550: Loss = 0.1893\n",
      "Epoch 6, Step 560: Loss = 0.1211\n",
      "Epoch 6, Step 570: Loss = 0.1419\n",
      "Epoch 6, Step 580: Loss = 0.1949\n",
      "Epoch 6, Step 590: Loss = 0.1136\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Average Training Loss: 0.3471\n",
      "Validation Metrics:\n",
      "Val_loss: 0.3589\n",
      "Epoch 7, Step 0: Loss = 0.4200\n",
      "Epoch 7, Step 10: Loss = 0.0580\n",
      "Epoch 7, Step 20: Loss = 0.0957\n",
      "Epoch 7, Step 30: Loss = 0.4781\n",
      "Epoch 7, Step 40: Loss = 0.0619\n",
      "Epoch 7, Step 50: Loss = 0.3945\n",
      "Epoch 7, Step 60: Loss = 0.0297\n",
      "Epoch 7, Step 70: Loss = 0.0833\n",
      "Epoch 7, Step 80: Loss = 0.1260\n",
      "Epoch 7, Step 90: Loss = 0.0299\n",
      "Epoch 7, Step 100: Loss = 0.3667\n",
      "Epoch 7, Step 110: Loss = 0.2345\n",
      "Epoch 7, Step 120: Loss = 0.0787\n",
      "Epoch 7, Step 130: Loss = 0.2941\n",
      "Epoch 7, Step 140: Loss = 0.0870\n",
      "Epoch 7, Step 150: Loss = 0.0658\n",
      "Epoch 7, Step 160: Loss = 0.4820\n",
      "Epoch 7, Step 170: Loss = 0.3172\n",
      "Epoch 7, Step 180: Loss = 0.1502\n",
      "Epoch 7, Step 190: Loss = 0.0876\n",
      "Epoch 7, Step 200: Loss = 0.0783\n",
      "Epoch 7, Step 210: Loss = 0.1388\n",
      "Epoch 7, Step 220: Loss = 0.0388\n",
      "Epoch 7, Step 230: Loss = 0.1106\n",
      "Epoch 7, Step 240: Loss = 0.2368\n",
      "Epoch 7, Step 250: Loss = 0.1675\n",
      "Epoch 7, Step 260: Loss = 0.2336\n",
      "Epoch 7, Step 270: Loss = 0.3047\n",
      "Epoch 7, Step 280: Loss = 0.5463\n",
      "Epoch 7, Step 290: Loss = 0.2405\n",
      "Epoch 7, Step 300: Loss = 0.1971\n",
      "Epoch 7, Step 310: Loss = 0.2215\n",
      "Epoch 7, Step 320: Loss = 0.0360\n",
      "Epoch 7, Step 330: Loss = 0.2065\n",
      "Epoch 7, Step 340: Loss = 0.0081\n",
      "Epoch 7, Step 350: Loss = 0.2622\n",
      "Epoch 7, Step 360: Loss = 0.0932\n",
      "Epoch 7, Step 370: Loss = 0.1564\n",
      "Epoch 7, Step 380: Loss = 0.2822\n",
      "Epoch 7, Step 390: Loss = 0.0369\n",
      "Epoch 7, Step 400: Loss = 0.0857\n",
      "Epoch 7, Step 410: Loss = 0.1312\n",
      "Epoch 7, Step 420: Loss = 0.0260\n",
      "Epoch 7, Step 430: Loss = 0.1234\n",
      "Epoch 7, Step 440: Loss = 0.1979\n",
      "Epoch 7, Step 450: Loss = 0.0066\n",
      "Epoch 7, Step 460: Loss = 0.0000\n",
      "Epoch 7, Step 470: Loss = 0.2092\n",
      "Epoch 7, Step 480: Loss = 0.1020\n",
      "Epoch 7, Step 490: Loss = 0.2927\n",
      "Epoch 7, Step 500: Loss = 0.5457\n",
      "Epoch 7, Step 510: Loss = 0.2080\n",
      "Epoch 7, Step 520: Loss = 0.0459\n",
      "Epoch 7, Step 530: Loss = 0.1886\n",
      "Epoch 7, Step 540: Loss = 0.1338\n",
      "Epoch 7, Step 550: Loss = 0.2924\n",
      "Epoch 7, Step 560: Loss = 0.1997\n",
      "Epoch 7, Step 570: Loss = 0.0687\n",
      "Epoch 7, Step 580: Loss = 0.1529\n",
      "Epoch 7, Step 590: Loss = 0.5093\n",
      "\n",
      "Epoch 7 Summary:\n",
      "Average Training Loss: 0.3223\n",
      "Validation Metrics:\n",
      "Val_loss: 0.3377\n",
      "Epoch 8, Step 0: Loss = 0.0732\n",
      "Epoch 8, Step 10: Loss = 0.1348\n",
      "Epoch 8, Step 20: Loss = 0.1427\n",
      "Epoch 8, Step 30: Loss = 0.1179\n",
      "Epoch 8, Step 40: Loss = 0.0941\n",
      "Epoch 8, Step 50: Loss = 0.0416\n",
      "Epoch 8, Step 60: Loss = 0.2431\n",
      "Epoch 8, Step 70: Loss = 0.1967\n",
      "Epoch 8, Step 80: Loss = 0.1902\n",
      "Epoch 8, Step 90: Loss = 0.1745\n",
      "Epoch 8, Step 100: Loss = 0.7677\n",
      "Epoch 8, Step 110: Loss = 0.3005\n",
      "Epoch 8, Step 120: Loss = 0.1540\n",
      "Epoch 8, Step 130: Loss = 0.1595\n",
      "Epoch 8, Step 140: Loss = 0.1037\n",
      "Epoch 8, Step 150: Loss = 0.0076\n",
      "Epoch 8, Step 160: Loss = 0.3878\n",
      "Epoch 8, Step 170: Loss = 0.0943\n",
      "Epoch 8, Step 180: Loss = 0.0177\n",
      "Epoch 8, Step 190: Loss = 0.4611\n",
      "Epoch 8, Step 200: Loss = 0.2315\n",
      "Epoch 8, Step 210: Loss = 0.2707\n",
      "Epoch 8, Step 220: Loss = 0.3510\n",
      "Epoch 8, Step 230: Loss = 0.0045\n",
      "Epoch 8, Step 240: Loss = 0.2663\n",
      "Epoch 8, Step 250: Loss = 0.2259\n",
      "Epoch 8, Step 260: Loss = 0.2034\n",
      "Epoch 8, Step 270: Loss = 0.1327\n",
      "Epoch 8, Step 280: Loss = 0.1111\n",
      "Epoch 8, Step 290: Loss = 0.0000\n",
      "Epoch 8, Step 300: Loss = 0.1176\n",
      "Epoch 8, Step 310: Loss = 0.3716\n",
      "Epoch 8, Step 320: Loss = 0.1017\n",
      "Epoch 8, Step 330: Loss = 0.0480\n",
      "Epoch 8, Step 340: Loss = 0.4367\n",
      "Epoch 8, Step 350: Loss = 0.0180\n",
      "Epoch 8, Step 360: Loss = 0.0373\n",
      "Epoch 8, Step 370: Loss = 0.3060\n",
      "Epoch 8, Step 380: Loss = 0.1793\n",
      "Epoch 8, Step 390: Loss = 0.4178\n",
      "Epoch 8, Step 400: Loss = 0.0686\n",
      "Epoch 8, Step 410: Loss = 0.2993\n",
      "Epoch 8, Step 420: Loss = 0.0447\n",
      "Epoch 8, Step 430: Loss = 0.3812\n",
      "Epoch 8, Step 440: Loss = 0.2750\n",
      "Epoch 8, Step 450: Loss = 0.0243\n",
      "Epoch 8, Step 460: Loss = 0.0394\n",
      "Epoch 8, Step 470: Loss = 0.3453\n",
      "Epoch 8, Step 480: Loss = 0.0237\n",
      "Epoch 8, Step 490: Loss = 0.0906\n",
      "Epoch 8, Step 500: Loss = 0.4796\n",
      "Epoch 8, Step 510: Loss = 0.0238\n",
      "Epoch 8, Step 520: Loss = 0.0437\n",
      "Epoch 8, Step 530: Loss = 0.1022\n",
      "Epoch 8, Step 540: Loss = 0.0168\n",
      "Epoch 8, Step 550: Loss = 0.0871\n",
      "Epoch 8, Step 560: Loss = 0.0903\n",
      "Epoch 8, Step 570: Loss = 0.2267\n",
      "Epoch 8, Step 580: Loss = 0.1768\n",
      "Epoch 8, Step 590: Loss = 0.0577\n",
      "\n",
      "Epoch 8 Summary:\n",
      "Average Training Loss: 0.3010\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 0: Loss = 0.0148\n",
      "Epoch 9, Step 10: Loss = 0.3380\n",
      "Epoch 9, Step 20: Loss = 0.0865\n",
      "Epoch 9, Step 30: Loss = 0.0017\n",
      "Epoch 9, Step 40: Loss = 0.0353\n",
      "Epoch 9, Step 50: Loss = 0.1241\n",
      "Epoch 9, Step 60: Loss = 0.5079\n",
      "Epoch 9, Step 70: Loss = 0.1415\n",
      "Epoch 9, Step 80: Loss = 0.1370\n",
      "Epoch 9, Step 90: Loss = 0.1156\n",
      "Epoch 9, Step 100: Loss = 0.3721\n",
      "Epoch 9, Step 110: Loss = 0.0000\n",
      "Epoch 9, Step 120: Loss = 0.0958\n",
      "Epoch 9, Step 130: Loss = 0.0447\n",
      "Epoch 9, Step 140: Loss = 0.3230\n",
      "Epoch 9, Step 150: Loss = 0.1635\n",
      "Epoch 9, Step 160: Loss = 0.1678\n",
      "Epoch 9, Step 170: Loss = 0.0000\n",
      "Epoch 9, Step 180: Loss = 0.0177\n",
      "Epoch 9, Step 190: Loss = 0.1063\n",
      "Epoch 9, Step 200: Loss = 0.1825\n",
      "Epoch 9, Step 210: Loss = 0.0155\n",
      "Epoch 9, Step 220: Loss = 0.0597\n",
      "Epoch 9, Step 230: Loss = 0.3630\n",
      "Epoch 9, Step 240: Loss = 0.3226\n",
      "Epoch 9, Step 250: Loss = 0.1469\n",
      "Epoch 9, Step 260: Loss = 0.0143\n",
      "Epoch 9, Step 270: Loss = 0.0395\n",
      "Epoch 9, Step 280: Loss = 0.4286\n",
      "Epoch 9, Step 290: Loss = 0.3904\n",
      "Epoch 9, Step 300: Loss = 0.0775\n",
      "Epoch 9, Step 310: Loss = 0.2237\n",
      "Epoch 9, Step 320: Loss = 0.0086\n",
      "Epoch 9, Step 330: Loss = 0.0176\n",
      "Epoch 9, Step 340: Loss = 0.1350\n",
      "Epoch 9, Step 350: Loss = 0.1367\n",
      "Epoch 9, Step 360: Loss = 0.2001\n",
      "Epoch 9, Step 370: Loss = 0.0000\n",
      "Epoch 9, Step 380: Loss = 0.1921\n",
      "Epoch 9, Step 390: Loss = 0.5271\n",
      "Epoch 9, Step 400: Loss = 0.1967\n",
      "Epoch 9, Step 410: Loss = 0.0571\n",
      "Epoch 9, Step 420: Loss = 0.0363\n",
      "Epoch 9, Step 430: Loss = 0.1859\n",
      "Epoch 9, Step 440: Loss = 0.0088\n",
      "Epoch 9, Step 450: Loss = 0.1953\n",
      "Epoch 9, Step 460: Loss = 0.0072\n",
      "Epoch 9, Step 470: Loss = 0.5768\n",
      "Epoch 9, Step 480: Loss = 0.1135\n",
      "Epoch 9, Step 490: Loss = 0.0395\n",
      "Epoch 9, Step 500: Loss = 0.0305\n",
      "Epoch 9, Step 510: Loss = 0.0000\n",
      "Epoch 9, Step 520: Loss = 0.2531\n",
      "Epoch 9, Step 530: Loss = 0.0473\n",
      "Epoch 9, Step 540: Loss = 0.0096\n",
      "Epoch 9, Step 550: Loss = 0.2117\n",
      "Epoch 9, Step 560: Loss = 0.0192\n",
      "Epoch 9, Step 570: Loss = 0.2875\n",
      "Epoch 9, Step 580: Loss = 0.0206\n",
      "Epoch 9, Step 590: Loss = 0.1649\n",
      "\n",
      "Epoch 9 Summary:\n",
      "Average Training Loss: 0.2811\n",
      "Validation Metrics:\n",
      "Val_loss: 0.3015\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    torch.cuda.set_device(0)  \n",
    "    return torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataset class with data cleaning\n",
    "class ContrastiveEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        # Clean each text field\n",
    "        emails_df['sender'] = emails_df['sender'].apply(clean_text)\n",
    "        emails_df['subject'] = emails_df['subject'].apply(clean_text)\n",
    "        emails_df['body'] = emails_df['body'].apply(clean_text)\n",
    "        \n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.ham_indices = self.emails_df[self.emails_df['label'] == 0].index.tolist()\n",
    "        self.phish_indices = self.emails_df[self.emails_df['label'] == 1].index.tolist()\n",
    "        if not (self.ham_indices and self.phish_indices):\n",
    "            raise ValueError(\"Dataset must contain examples of both classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def _get_random_email_idx(self, label):\n",
    "        indices = self.ham_indices if label == 0 else self.phish_indices\n",
    "        return np.random.choice(indices)\n",
    "\n",
    "    def _prepare_email_input(self, email):\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        tokenized_output = self.tokenizer(input_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        return tokenized_output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_email = self.emails_df.iloc[idx]\n",
    "        anchor_label = anchor_email['label']\n",
    "        positives, negatives = [], []\n",
    "\n",
    "        for _ in range(3):\n",
    "            pos_idx = self._get_random_email_idx(anchor_label)\n",
    "            neg_idx = self._get_random_email_idx(1 - anchor_label)\n",
    "            positives.append(self._prepare_email_input(self.emails_df.iloc[pos_idx]))\n",
    "            negatives.append(self._prepare_email_input(self.emails_df.iloc[neg_idx]))\n",
    "\n",
    "        anchor_inputs = self._prepare_email_input(anchor_email)\n",
    "        return {\n",
    "            'anchor_input_ids': anchor_inputs['input_ids'].squeeze(),\n",
    "            'anchor_attention_mask': anchor_inputs['attention_mask'].squeeze(),\n",
    "            'positive_input_ids': torch.stack([p['input_ids'].squeeze() for p in positives]),\n",
    "            'positive_attention_mask': torch.stack([p['attention_mask'].squeeze() for p in positives]),\n",
    "            'negative_input_ids': torch.stack([n['input_ids'].squeeze() for n in negatives]),\n",
    "            'negative_attention_mask': torch.stack([n['attention_mask'].squeeze() for n in negatives])\n",
    "        }\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model_config.use_cache = False\n",
    "\n",
    "   \n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        config=model_config, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"SEQ_CLS\")\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9, margin=1.0, accumulation_steps=2):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    model = model.to(device).to(torch.float16)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                # Compute embeddings for anchor, positive, and negative samples in parallel\n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                # Reshape positive and negative embeddings to match the batch structure (batch_size, 3, embedding_dim)\n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "                # Calculate the triplet loss for each triplet in the batch\n",
    "                loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "                loss /= accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        val_metrics = evaluate_model(model, val_loader, device, margin)\n",
    "        print_metrics(epoch, total_loss, len(train_loader), val_metrics)\n",
    "\n",
    "        if val_metrics['val_loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['val_loss']\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model(model, val_loader, device, margin=1.0):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "               \n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                \n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "               \n",
    "                val_loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "    return {'val_loss': avg_val_loss}\n",
    "\n",
    "\n",
    "# Main function\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  \n",
    "        text = \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def print_metrics(epoch, total_loss, num_batches, val_metrics):\n",
    "    \n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "def main():\n",
    "    login(token=\"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\")\n",
    "    device = setup_environment()\n",
    "    model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/newdata_cleaned.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "\n",
    "   \n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "    train_dataset = ContrastiveEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = ContrastiveEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "    best_model_state = train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/contrastive_7B\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59aa5b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1295c4e63fb64fccb3c1f7dcc49860fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 1.0064\n",
      "Epoch 1, Step 10: Loss = 0.6280\n",
      "Epoch 1, Step 20: Loss = 0.7360\n",
      "Epoch 1, Step 30: Loss = 0.8420\n",
      "Epoch 1, Step 40: Loss = 0.5209\n",
      "Epoch 1, Step 50: Loss = 0.6332\n",
      "Epoch 1, Step 60: Loss = 0.6008\n",
      "Epoch 1, Step 70: Loss = 1.0036\n",
      "Epoch 1, Step 80: Loss = 0.5906\n",
      "Epoch 1, Step 90: Loss = 0.8547\n",
      "Epoch 1, Step 100: Loss = 1.0852\n",
      "Epoch 1, Step 110: Loss = 0.9466\n",
      "Epoch 1, Step 120: Loss = 0.5576\n",
      "Epoch 1, Step 130: Loss = 0.7647\n",
      "Epoch 1, Step 140: Loss = 0.8686\n",
      "Epoch 1, Step 150: Loss = 1.3381\n",
      "Epoch 1, Step 160: Loss = 0.5882\n",
      "Epoch 1, Step 170: Loss = 0.7980\n",
      "Epoch 1, Step 180: Loss = 0.6350\n",
      "Epoch 1, Step 190: Loss = 0.7847\n",
      "Epoch 1, Step 200: Loss = 0.5892\n",
      "Epoch 1, Step 210: Loss = 0.7243\n",
      "Epoch 1, Step 220: Loss = 1.1059\n",
      "Epoch 1, Step 230: Loss = 0.8742\n",
      "Epoch 1, Step 240: Loss = 0.6977\n",
      "Epoch 1, Step 250: Loss = 0.6698\n",
      "Epoch 1, Step 260: Loss = 0.5186\n",
      "Epoch 1, Step 270: Loss = 0.8657\n",
      "Epoch 1, Step 280: Loss = 0.9431\n",
      "Epoch 1, Step 290: Loss = 0.4577\n",
      "Epoch 1, Step 300: Loss = 0.7224\n",
      "Epoch 1, Step 310: Loss = 0.8085\n",
      "Epoch 1, Step 320: Loss = 0.9647\n",
      "Epoch 1, Step 330: Loss = 0.6105\n",
      "Epoch 1, Step 340: Loss = 0.4160\n",
      "Epoch 1, Step 350: Loss = 0.8192\n",
      "Epoch 1, Step 360: Loss = 0.6827\n",
      "Epoch 1, Step 370: Loss = 0.7389\n",
      "Epoch 1, Step 380: Loss = 0.5314\n",
      "Epoch 1, Step 390: Loss = 0.6015\n",
      "Epoch 1, Step 400: Loss = 0.9291\n",
      "Epoch 1, Step 410: Loss = 0.7003\n",
      "Epoch 1, Step 420: Loss = 0.8991\n",
      "Epoch 1, Step 430: Loss = 1.0805\n",
      "Epoch 1, Step 440: Loss = 0.7477\n",
      "Epoch 1, Step 450: Loss = 0.9421\n",
      "Epoch 1, Step 460: Loss = 0.8899\n",
      "Epoch 1, Step 470: Loss = 0.6791\n",
      "Epoch 1, Step 480: Loss = 0.6625\n",
      "Epoch 1, Step 490: Loss = 0.4567\n",
      "Epoch 1, Step 500: Loss = 0.6970\n",
      "Epoch 1, Step 510: Loss = 0.8109\n",
      "Epoch 1, Step 520: Loss = 0.7757\n",
      "Epoch 1, Step 530: Loss = 0.3858\n",
      "Epoch 1, Step 540: Loss = 0.7242\n",
      "Epoch 1, Step 550: Loss = 0.4632\n",
      "Epoch 1, Step 560: Loss = 0.4681\n",
      "Epoch 1, Step 570: Loss = 0.6064\n",
      "Epoch 1, Step 580: Loss = 0.5780\n",
      "Epoch 1, Step 590: Loss = 0.3499\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Average Training Loss: 1.4572\n",
      "Validation Metrics:\n",
      "Val_loss: 1.1455\n",
      "Epoch 2, Step 0: Loss = 0.4001\n",
      "Epoch 2, Step 10: Loss = 0.6919\n",
      "Epoch 2, Step 20: Loss = 0.4032\n",
      "Epoch 2, Step 30: Loss = 0.7412\n",
      "Epoch 2, Step 40: Loss = 0.3935\n",
      "Epoch 2, Step 50: Loss = 0.7645\n",
      "Epoch 2, Step 60: Loss = 0.2641\n",
      "Epoch 2, Step 70: Loss = 1.0006\n",
      "Epoch 2, Step 80: Loss = 0.6067\n",
      "Epoch 2, Step 90: Loss = 0.3252\n",
      "Epoch 2, Step 100: Loss = 0.8412\n",
      "Epoch 2, Step 110: Loss = 0.4322\n",
      "Epoch 2, Step 120: Loss = 0.6016\n",
      "Epoch 2, Step 130: Loss = 0.2413\n",
      "Epoch 2, Step 140: Loss = 0.1344\n",
      "Epoch 2, Step 150: Loss = 0.1635\n",
      "Epoch 2, Step 160: Loss = 0.3812\n",
      "Epoch 2, Step 170: Loss = 0.4324\n",
      "Epoch 2, Step 180: Loss = 0.4325\n",
      "Epoch 2, Step 190: Loss = 0.1418\n",
      "Epoch 2, Step 200: Loss = 0.4744\n",
      "Epoch 2, Step 210: Loss = 0.0964\n",
      "Epoch 2, Step 220: Loss = 0.2313\n",
      "Epoch 2, Step 230: Loss = 0.3710\n",
      "Epoch 2, Step 240: Loss = 0.3579\n",
      "Epoch 2, Step 250: Loss = 0.2745\n",
      "Epoch 2, Step 260: Loss = 0.1700\n",
      "Epoch 2, Step 270: Loss = 0.3566\n",
      "Epoch 2, Step 280: Loss = 0.0737\n",
      "Epoch 2, Step 290: Loss = 0.0748\n",
      "Epoch 2, Step 300: Loss = 0.1704\n",
      "Epoch 2, Step 310: Loss = 0.6517\n",
      "Epoch 2, Step 320: Loss = 0.6333\n",
      "Epoch 2, Step 330: Loss = 0.0838\n",
      "Epoch 2, Step 340: Loss = 0.3321\n",
      "Epoch 2, Step 350: Loss = 0.0455\n",
      "Epoch 2, Step 360: Loss = 0.2847\n",
      "Epoch 2, Step 370: Loss = 0.2366\n",
      "Epoch 2, Step 380: Loss = 0.1899\n",
      "Epoch 2, Step 390: Loss = 0.0284\n",
      "Epoch 2, Step 400: Loss = 0.1277\n",
      "Epoch 2, Step 410: Loss = 0.2441\n",
      "Epoch 2, Step 420: Loss = 0.6163\n",
      "Epoch 2, Step 430: Loss = 0.1416\n",
      "Epoch 2, Step 440: Loss = 0.2736\n",
      "Epoch 2, Step 450: Loss = 0.2720\n",
      "Epoch 2, Step 460: Loss = 0.5101\n",
      "Epoch 2, Step 470: Loss = 0.0284\n",
      "Epoch 2, Step 480: Loss = 0.7895\n",
      "Epoch 2, Step 490: Loss = 0.5206\n",
      "Epoch 2, Step 500: Loss = 0.4889\n",
      "Epoch 2, Step 510: Loss = 0.1385\n",
      "Epoch 2, Step 520: Loss = 0.5649\n",
      "Epoch 2, Step 530: Loss = 0.1749\n",
      "Epoch 2, Step 540: Loss = 0.1269\n",
      "Epoch 2, Step 550: Loss = 0.1538\n",
      "Epoch 2, Step 560: Loss = 0.0239\n",
      "Epoch 2, Step 570: Loss = 0.0883\n",
      "Epoch 2, Step 580: Loss = 0.1001\n",
      "Epoch 2, Step 590: Loss = 0.1460\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Average Training Loss: 0.6651\n",
      "Validation Metrics:\n",
      "Val_loss: 0.4513\n",
      "Epoch 3, Step 0: Loss = 0.1529\n",
      "Epoch 3, Step 10: Loss = 0.4003\n",
      "Epoch 3, Step 20: Loss = 0.2917\n",
      "Epoch 3, Step 30: Loss = 0.3829\n",
      "Epoch 3, Step 40: Loss = 0.2863\n",
      "Epoch 3, Step 50: Loss = 0.0355\n",
      "Epoch 3, Step 60: Loss = 0.0632\n",
      "Epoch 3, Step 70: Loss = 0.2628\n",
      "Epoch 3, Step 80: Loss = 0.1772\n",
      "Epoch 3, Step 90: Loss = 0.2198\n",
      "Epoch 3, Step 100: Loss = 0.1039\n",
      "Epoch 3, Step 110: Loss = 0.3555\n",
      "Epoch 3, Step 120: Loss = 0.0000\n",
      "Epoch 3, Step 130: Loss = 0.2587\n",
      "Epoch 3, Step 140: Loss = 0.2937\n",
      "Epoch 3, Step 150: Loss = 0.0366\n",
      "Epoch 3, Step 160: Loss = 0.0327\n",
      "Epoch 3, Step 170: Loss = 0.1576\n",
      "Epoch 3, Step 180: Loss = 0.0748\n",
      "Epoch 3, Step 190: Loss = 0.3777\n",
      "Epoch 3, Step 200: Loss = 0.5293\n",
      "Epoch 3, Step 210: Loss = 0.1551\n",
      "Epoch 3, Step 220: Loss = 0.2549\n",
      "Epoch 3, Step 230: Loss = 0.1377\n",
      "Epoch 3, Step 240: Loss = 0.4235\n",
      "Epoch 3, Step 250: Loss = 0.2453\n",
      "Epoch 3, Step 260: Loss = 0.0596\n",
      "Epoch 3, Step 270: Loss = 0.1760\n",
      "Epoch 3, Step 280: Loss = 0.0137\n",
      "Epoch 3, Step 290: Loss = 0.1225\n",
      "Epoch 3, Step 300: Loss = 0.3287\n",
      "Epoch 3, Step 310: Loss = 0.3863\n",
      "Epoch 3, Step 320: Loss = 0.0453\n",
      "Epoch 3, Step 330: Loss = 0.1598\n",
      "Epoch 3, Step 340: Loss = 0.3162\n",
      "Epoch 3, Step 350: Loss = 0.1829\n",
      "Epoch 3, Step 360: Loss = 0.3353\n",
      "Epoch 3, Step 370: Loss = 0.0000\n",
      "Epoch 3, Step 380: Loss = 0.1455\n",
      "Epoch 3, Step 390: Loss = 0.0666\n",
      "Epoch 3, Step 400: Loss = 0.3114\n",
      "Epoch 3, Step 410: Loss = 0.2240\n",
      "Epoch 3, Step 420: Loss = 0.0507\n",
      "Epoch 3, Step 430: Loss = 0.1614\n",
      "Epoch 3, Step 440: Loss = 0.1252\n",
      "Epoch 3, Step 450: Loss = 0.2540\n",
      "Epoch 3, Step 460: Loss = 0.5389\n",
      "Epoch 3, Step 470: Loss = 0.0679\n",
      "Epoch 3, Step 480: Loss = 0.0112\n",
      "Epoch 3, Step 490: Loss = 0.0063\n",
      "Epoch 3, Step 500: Loss = 0.4440\n",
      "Epoch 3, Step 510: Loss = 0.3407\n",
      "Epoch 3, Step 520: Loss = 0.2822\n",
      "Epoch 3, Step 530: Loss = 0.0000\n",
      "Epoch 3, Step 540: Loss = 0.3694\n",
      "Epoch 3, Step 550: Loss = 0.0552\n",
      "Epoch 3, Step 560: Loss = 0.0002\n",
      "Epoch 3, Step 570: Loss = 0.0444\n",
      "Epoch 3, Step 580: Loss = 0.0201\n",
      "Epoch 3, Step 590: Loss = 0.3827\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Average Training Loss: 0.3766\n",
      "Validation Metrics:\n",
      "Val_loss: 0.3293\n",
      "Epoch 4, Step 0: Loss = 0.1038\n",
      "Epoch 4, Step 10: Loss = 0.2052\n",
      "Epoch 4, Step 20: Loss = 0.0191\n",
      "Epoch 4, Step 30: Loss = 0.0597\n",
      "Epoch 4, Step 40: Loss = 0.0139\n",
      "Epoch 4, Step 50: Loss = 0.3482\n",
      "Epoch 4, Step 60: Loss = 0.4241\n",
      "Epoch 4, Step 70: Loss = 0.2248\n",
      "Epoch 4, Step 80: Loss = 0.0000\n",
      "Epoch 4, Step 90: Loss = 0.4198\n",
      "Epoch 4, Step 100: Loss = 0.8333\n",
      "Epoch 4, Step 110: Loss = 0.3451\n",
      "Epoch 4, Step 120: Loss = 0.0118\n",
      "Epoch 4, Step 130: Loss = 0.1820\n",
      "Epoch 4, Step 140: Loss = 0.2599\n",
      "Epoch 4, Step 150: Loss = 0.1108\n",
      "Epoch 4, Step 160: Loss = 0.2847\n",
      "Epoch 4, Step 170: Loss = 0.2525\n",
      "Epoch 4, Step 180: Loss = 0.1561\n",
      "Epoch 4, Step 190: Loss = 0.0045\n",
      "Epoch 4, Step 200: Loss = 0.2888\n",
      "Epoch 4, Step 210: Loss = 0.0465\n",
      "Epoch 4, Step 220: Loss = 0.0314\n",
      "Epoch 4, Step 230: Loss = 0.0215\n",
      "Epoch 4, Step 240: Loss = 0.3769\n",
      "Epoch 4, Step 250: Loss = 0.0423\n",
      "Epoch 4, Step 260: Loss = 0.1077\n",
      "Epoch 4, Step 270: Loss = 0.0750\n",
      "Epoch 4, Step 280: Loss = 0.1119\n",
      "Epoch 4, Step 290: Loss = 0.1474\n",
      "Epoch 4, Step 300: Loss = 0.0898\n",
      "Epoch 4, Step 310: Loss = 0.3301\n",
      "Epoch 4, Step 320: Loss = 0.4050\n",
      "Epoch 4, Step 330: Loss = 0.1741\n",
      "Epoch 4, Step 340: Loss = 0.1607\n",
      "Epoch 4, Step 350: Loss = 0.0732\n",
      "Epoch 4, Step 360: Loss = 0.0521\n",
      "Epoch 4, Step 370: Loss = 0.4285\n",
      "Epoch 4, Step 380: Loss = 0.1462\n",
      "Epoch 4, Step 390: Loss = 0.3906\n",
      "Epoch 4, Step 400: Loss = 0.1423\n",
      "Epoch 4, Step 410: Loss = 0.0650\n",
      "Epoch 4, Step 420: Loss = 0.0940\n",
      "Epoch 4, Step 430: Loss = 0.4986\n",
      "Epoch 4, Step 440: Loss = 0.0059\n",
      "Epoch 4, Step 450: Loss = 0.0157\n",
      "Epoch 4, Step 460: Loss = 0.0215\n",
      "Epoch 4, Step 470: Loss = 0.0000\n",
      "Epoch 4, Step 480: Loss = 0.1432\n",
      "Epoch 4, Step 490: Loss = 0.0843\n",
      "Epoch 4, Step 500: Loss = 0.0627\n",
      "Epoch 4, Step 510: Loss = 0.1608\n",
      "Epoch 4, Step 520: Loss = 0.0318\n",
      "Epoch 4, Step 530: Loss = 0.0386\n",
      "Epoch 4, Step 540: Loss = 0.1420\n",
      "Epoch 4, Step 550: Loss = 0.0296\n",
      "Epoch 4, Step 560: Loss = 0.1537\n",
      "Epoch 4, Step 570: Loss = 0.0215\n",
      "Epoch 4, Step 580: Loss = 0.0596\n",
      "Epoch 4, Step 590: Loss = 0.0000\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Average Training Loss: 0.3031\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.2186\n",
      "Epoch 5, Step 10: Loss = 0.1178\n",
      "Epoch 5, Step 20: Loss = 0.0007\n",
      "Epoch 5, Step 30: Loss = 0.1030\n",
      "Epoch 5, Step 40: Loss = 0.0000\n",
      "Epoch 5, Step 50: Loss = 0.0885\n",
      "Epoch 5, Step 60: Loss = 0.0000\n",
      "Epoch 5, Step 70: Loss = 0.3810\n",
      "Epoch 5, Step 80: Loss = 0.6118\n",
      "Epoch 5, Step 90: Loss = 0.0936\n",
      "Epoch 5, Step 100: Loss = 0.0000\n",
      "Epoch 5, Step 110: Loss = 0.3295\n",
      "Epoch 5, Step 120: Loss = 0.0522\n",
      "Epoch 5, Step 130: Loss = 0.2232\n",
      "Epoch 5, Step 140: Loss = 0.0556\n",
      "Epoch 5, Step 150: Loss = 0.1454\n",
      "Epoch 5, Step 160: Loss = 0.0000\n",
      "Epoch 5, Step 170: Loss = 0.0923\n",
      "Epoch 5, Step 180: Loss = 0.0719\n",
      "Epoch 5, Step 190: Loss = 0.0530\n",
      "Epoch 5, Step 200: Loss = 0.3196\n",
      "Epoch 5, Step 210: Loss = 0.0000\n",
      "Epoch 5, Step 220: Loss = 0.0411\n",
      "Epoch 5, Step 230: Loss = 0.1253\n",
      "Epoch 5, Step 240: Loss = 0.0050\n",
      "Epoch 5, Step 250: Loss = 0.0454\n",
      "Epoch 5, Step 260: Loss = 0.1058\n",
      "Epoch 5, Step 270: Loss = 0.0665\n",
      "Epoch 5, Step 280: Loss = 0.3438\n",
      "Epoch 5, Step 290: Loss = 0.0649\n",
      "Epoch 5, Step 300: Loss = 0.1452\n",
      "Epoch 5, Step 310: Loss = 0.0178\n",
      "Epoch 5, Step 320: Loss = 0.1093\n",
      "Epoch 5, Step 330: Loss = 0.0131\n",
      "Epoch 5, Step 340: Loss = 0.0000\n",
      "Epoch 5, Step 350: Loss = 0.5890\n",
      "Epoch 5, Step 360: Loss = 0.3138\n",
      "Epoch 5, Step 370: Loss = 0.0000\n",
      "Epoch 5, Step 380: Loss = 0.1770\n",
      "Epoch 5, Step 390: Loss = 0.0108\n",
      "Epoch 5, Step 400: Loss = 0.0391\n",
      "Epoch 5, Step 410: Loss = 0.0748\n",
      "Epoch 5, Step 420: Loss = 0.0000\n",
      "Epoch 5, Step 430: Loss = 0.0000\n",
      "Epoch 5, Step 440: Loss = 0.0000\n",
      "Epoch 5, Step 450: Loss = 0.1625\n",
      "Epoch 5, Step 460: Loss = 0.2320\n",
      "Epoch 5, Step 470: Loss = 0.1619\n",
      "Epoch 5, Step 480: Loss = 0.1916\n",
      "Epoch 5, Step 490: Loss = 0.1634\n",
      "Epoch 5, Step 500: Loss = 0.1321\n",
      "Epoch 5, Step 510: Loss = 0.1831\n",
      "Epoch 5, Step 520: Loss = 0.1368\n",
      "Epoch 5, Step 530: Loss = 0.0356\n",
      "Epoch 5, Step 540: Loss = 0.1934\n",
      "Epoch 5, Step 550: Loss = 0.0000\n",
      "Epoch 5, Step 560: Loss = 0.1399\n",
      "Epoch 5, Step 570: Loss = 0.6370\n",
      "Epoch 5, Step 580: Loss = 0.1969\n",
      "Epoch 5, Step 590: Loss = 0.0000\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Average Training Loss: 0.2662\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2984\n",
      "Epoch 6, Step 0: Loss = 0.4429\n",
      "Epoch 6, Step 10: Loss = 0.0204\n",
      "Epoch 6, Step 20: Loss = 0.0000\n",
      "Epoch 6, Step 30: Loss = 0.0000\n",
      "Epoch 6, Step 40: Loss = 0.0913\n",
      "Epoch 6, Step 50: Loss = 0.0560\n",
      "Epoch 6, Step 60: Loss = 0.0248\n",
      "Epoch 6, Step 70: Loss = 0.0128\n",
      "Epoch 6, Step 80: Loss = 0.0062\n",
      "Epoch 6, Step 90: Loss = 0.0000\n",
      "Epoch 6, Step 100: Loss = 0.0750\n",
      "Epoch 6, Step 110: Loss = 0.0645\n",
      "Epoch 6, Step 120: Loss = 0.2608\n",
      "Epoch 6, Step 130: Loss = 0.1126\n",
      "Epoch 6, Step 140: Loss = 0.7099\n",
      "Epoch 6, Step 150: Loss = 0.2552\n",
      "Epoch 6, Step 160: Loss = 0.0563\n",
      "Epoch 6, Step 170: Loss = 0.0085\n",
      "Epoch 6, Step 180: Loss = 0.0411\n",
      "Epoch 6, Step 190: Loss = 0.0000\n",
      "Epoch 6, Step 200: Loss = 0.0538\n",
      "Epoch 6, Step 210: Loss = 0.0523\n",
      "Epoch 6, Step 220: Loss = 0.0000\n",
      "Epoch 6, Step 230: Loss = 0.0000\n",
      "Epoch 6, Step 240: Loss = 0.0004\n",
      "Epoch 6, Step 250: Loss = 0.0000\n",
      "Epoch 6, Step 260: Loss = 0.1248\n",
      "Epoch 6, Step 270: Loss = 0.0611\n",
      "Epoch 6, Step 280: Loss = 0.1017\n",
      "Epoch 6, Step 290: Loss = 0.3643\n",
      "Epoch 6, Step 300: Loss = 0.0138\n",
      "Epoch 6, Step 310: Loss = 0.1462\n",
      "Epoch 6, Step 320: Loss = 0.0000\n",
      "Epoch 6, Step 330: Loss = 0.0332\n",
      "Epoch 6, Step 340: Loss = 0.0000\n",
      "Epoch 6, Step 350: Loss = 0.2652\n",
      "Epoch 6, Step 360: Loss = 0.1467\n",
      "Epoch 6, Step 370: Loss = 0.0000\n",
      "Epoch 6, Step 380: Loss = 0.0766\n",
      "Epoch 6, Step 390: Loss = 0.1215\n",
      "Epoch 6, Step 400: Loss = 0.1768\n",
      "Epoch 6, Step 410: Loss = 0.1468\n",
      "Epoch 6, Step 420: Loss = 0.4220\n",
      "Epoch 6, Step 430: Loss = 0.2761\n",
      "Epoch 6, Step 440: Loss = 0.5890\n",
      "Epoch 6, Step 450: Loss = 0.0000\n",
      "Epoch 6, Step 460: Loss = 0.1313\n",
      "Epoch 6, Step 470: Loss = 0.0000\n",
      "Epoch 6, Step 480: Loss = 0.3251\n",
      "Epoch 6, Step 490: Loss = 0.1147\n",
      "Epoch 6, Step 500: Loss = 0.1873\n",
      "Epoch 6, Step 510: Loss = 0.0539\n",
      "Epoch 6, Step 520: Loss = 0.1151\n",
      "Epoch 6, Step 530: Loss = 0.3358\n",
      "Epoch 6, Step 540: Loss = 0.2631\n",
      "Epoch 6, Step 550: Loss = 0.0000\n",
      "Epoch 6, Step 560: Loss = 0.2903\n",
      "Epoch 6, Step 570: Loss = 0.1461\n",
      "Epoch 6, Step 580: Loss = 0.2588\n",
      "Epoch 6, Step 590: Loss = 0.2715\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Average Training Loss: 0.2515\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2635\n",
      "Epoch 7, Step 0: Loss = 0.9278\n",
      "Epoch 7, Step 10: Loss = 0.6753\n",
      "Epoch 7, Step 20: Loss = 0.1015\n",
      "Epoch 7, Step 30: Loss = 0.0336\n",
      "Epoch 7, Step 40: Loss = 0.0000\n",
      "Epoch 7, Step 50: Loss = 0.0000\n",
      "Epoch 7, Step 60: Loss = 0.0000\n",
      "Epoch 7, Step 70: Loss = 0.0374\n",
      "Epoch 7, Step 80: Loss = 0.0614\n",
      "Epoch 7, Step 90: Loss = 0.0210\n",
      "Epoch 7, Step 100: Loss = 0.1041\n",
      "Epoch 7, Step 110: Loss = 0.3435\n",
      "Epoch 7, Step 120: Loss = 0.0383\n",
      "Epoch 7, Step 130: Loss = 0.2471\n",
      "Epoch 7, Step 140: Loss = 0.0758\n",
      "Epoch 7, Step 150: Loss = 0.1767\n",
      "Epoch 7, Step 160: Loss = 0.1654\n",
      "Epoch 7, Step 170: Loss = 0.1171\n",
      "Epoch 7, Step 180: Loss = 0.1604\n",
      "Epoch 7, Step 190: Loss = 0.0768\n",
      "Epoch 7, Step 200: Loss = 0.0011\n",
      "Epoch 7, Step 210: Loss = 0.0733\n",
      "Epoch 7, Step 220: Loss = 0.2120\n",
      "Epoch 7, Step 230: Loss = 0.1752\n",
      "Epoch 7, Step 240: Loss = 0.1234\n",
      "Epoch 7, Step 250: Loss = 0.2565\n",
      "Epoch 7, Step 260: Loss = 0.0384\n",
      "Epoch 7, Step 270: Loss = 0.0040\n",
      "Epoch 7, Step 280: Loss = 0.0699\n",
      "Epoch 7, Step 290: Loss = 0.0094\n",
      "Epoch 7, Step 300: Loss = 0.1796\n",
      "Epoch 7, Step 310: Loss = 0.0000\n",
      "Epoch 7, Step 320: Loss = 0.0050\n",
      "Epoch 7, Step 330: Loss = 0.0000\n",
      "Epoch 7, Step 340: Loss = 0.2078\n",
      "Epoch 7, Step 350: Loss = 0.0668\n",
      "Epoch 7, Step 360: Loss = 0.0140\n",
      "Epoch 7, Step 370: Loss = 0.2195\n",
      "Epoch 7, Step 380: Loss = 0.2965\n",
      "Epoch 7, Step 390: Loss = 0.0000\n",
      "Epoch 7, Step 400: Loss = 0.1960\n",
      "Epoch 7, Step 410: Loss = 0.2924\n",
      "Epoch 7, Step 420: Loss = 0.1223\n",
      "Epoch 7, Step 430: Loss = 0.3167\n",
      "Epoch 7, Step 440: Loss = 0.3744\n",
      "Epoch 7, Step 450: Loss = 0.0000\n",
      "Epoch 7, Step 460: Loss = 0.1315\n",
      "Epoch 7, Step 470: Loss = 0.0512\n",
      "Epoch 7, Step 480: Loss = 0.2489\n",
      "Epoch 7, Step 490: Loss = 0.0254\n",
      "Epoch 7, Step 500: Loss = 0.1288\n",
      "Epoch 7, Step 510: Loss = 0.0000\n",
      "Epoch 7, Step 520: Loss = 0.0141\n",
      "Epoch 7, Step 530: Loss = 0.0584\n",
      "Epoch 7, Step 540: Loss = 0.0615\n",
      "Epoch 7, Step 550: Loss = 0.2726\n",
      "Epoch 7, Step 560: Loss = 0.2681\n",
      "Epoch 7, Step 570: Loss = 0.1534\n",
      "Epoch 7, Step 580: Loss = 0.0037\n",
      "Epoch 7, Step 590: Loss = 0.0209\n",
      "\n",
      "Epoch 7 Summary:\n",
      "Average Training Loss: 0.2351\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2609\n",
      "Epoch 8, Step 0: Loss = 0.2749\n",
      "Epoch 8, Step 10: Loss = 0.0580\n",
      "Epoch 8, Step 20: Loss = 0.0216\n",
      "Epoch 8, Step 30: Loss = 0.0957\n",
      "Epoch 8, Step 40: Loss = 0.0118\n",
      "Epoch 8, Step 50: Loss = 0.0000\n",
      "Epoch 8, Step 60: Loss = 0.1495\n",
      "Epoch 8, Step 70: Loss = 0.0290\n",
      "Epoch 8, Step 80: Loss = 0.0493\n",
      "Epoch 8, Step 90: Loss = 0.3594\n",
      "Epoch 8, Step 100: Loss = 0.1772\n",
      "Epoch 8, Step 110: Loss = 0.0000\n",
      "Epoch 8, Step 120: Loss = 0.1263\n",
      "Epoch 8, Step 130: Loss = 0.0403\n",
      "Epoch 8, Step 140: Loss = 0.0189\n",
      "Epoch 8, Step 150: Loss = 0.0387\n",
      "Epoch 8, Step 160: Loss = 0.0634\n",
      "Epoch 8, Step 170: Loss = 0.0319\n",
      "Epoch 8, Step 180: Loss = 0.0000\n",
      "Epoch 8, Step 190: Loss = 0.3493\n",
      "Epoch 8, Step 200: Loss = 0.1469\n",
      "Epoch 8, Step 210: Loss = 0.3210\n",
      "Epoch 8, Step 220: Loss = 0.0114\n",
      "Epoch 8, Step 230: Loss = 0.3730\n",
      "Epoch 8, Step 240: Loss = 0.0755\n",
      "Epoch 8, Step 250: Loss = 0.0000\n",
      "Epoch 8, Step 260: Loss = 0.4131\n",
      "Epoch 8, Step 270: Loss = 0.0000\n",
      "Epoch 8, Step 280: Loss = 0.0000\n",
      "Epoch 8, Step 290: Loss = 0.0469\n",
      "Epoch 8, Step 300: Loss = 0.1328\n",
      "Epoch 8, Step 310: Loss = 0.2117\n",
      "Epoch 8, Step 320: Loss = 0.3578\n",
      "Epoch 8, Step 330: Loss = 0.0000\n",
      "Epoch 8, Step 340: Loss = 0.1527\n",
      "Epoch 8, Step 350: Loss = 0.0278\n",
      "Epoch 8, Step 360: Loss = 0.2596\n",
      "Epoch 8, Step 370: Loss = 0.4194\n",
      "Epoch 8, Step 380: Loss = 0.0499\n",
      "Epoch 8, Step 390: Loss = 0.0803\n",
      "Epoch 8, Step 400: Loss = 0.0354\n",
      "Epoch 8, Step 410: Loss = 0.0062\n",
      "Epoch 8, Step 420: Loss = 0.1027\n",
      "Epoch 8, Step 430: Loss = 0.1741\n",
      "Epoch 8, Step 440: Loss = 0.0695\n",
      "Epoch 8, Step 450: Loss = 0.1792\n",
      "Epoch 8, Step 460: Loss = 0.0249\n",
      "Epoch 8, Step 470: Loss = 0.0565\n",
      "Epoch 8, Step 480: Loss = 0.1460\n",
      "Epoch 8, Step 490: Loss = 0.1021\n",
      "Epoch 8, Step 500: Loss = 0.2626\n",
      "Epoch 8, Step 510: Loss = 0.0000\n",
      "Epoch 8, Step 520: Loss = 0.3834\n",
      "Epoch 8, Step 530: Loss = 0.6665\n",
      "Epoch 8, Step 540: Loss = 0.0248\n",
      "Epoch 8, Step 550: Loss = 0.7142\n",
      "Epoch 8, Step 560: Loss = 0.0045\n",
      "Epoch 8, Step 570: Loss = 0.0000\n",
      "Epoch 8, Step 580: Loss = 0.0000\n",
      "Epoch 8, Step 590: Loss = 0.2673\n",
      "\n",
      "Epoch 8 Summary:\n",
      "Average Training Loss: 0.2291\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 0: Loss = 0.2613\n",
      "Epoch 9, Step 10: Loss = 0.1708\n",
      "Epoch 9, Step 20: Loss = 0.0594\n",
      "Epoch 9, Step 30: Loss = 0.0000\n",
      "Epoch 9, Step 40: Loss = 0.0039\n",
      "Epoch 9, Step 50: Loss = 0.1382\n",
      "Epoch 9, Step 60: Loss = 0.0000\n",
      "Epoch 9, Step 70: Loss = 0.0000\n",
      "Epoch 9, Step 80: Loss = 0.0246\n",
      "Epoch 9, Step 90: Loss = 0.1358\n",
      "Epoch 9, Step 100: Loss = 0.0447\n",
      "Epoch 9, Step 110: Loss = 0.0265\n",
      "Epoch 9, Step 120: Loss = 0.0244\n",
      "Epoch 9, Step 130: Loss = 0.6715\n",
      "Epoch 9, Step 140: Loss = 0.0178\n",
      "Epoch 9, Step 150: Loss = 0.1008\n",
      "Epoch 9, Step 160: Loss = 0.1228\n",
      "Epoch 9, Step 170: Loss = 0.0000\n",
      "Epoch 9, Step 180: Loss = 0.0002\n",
      "Epoch 9, Step 190: Loss = 0.0000\n",
      "Epoch 9, Step 200: Loss = 0.0406\n",
      "Epoch 9, Step 210: Loss = 0.1436\n",
      "Epoch 9, Step 220: Loss = 0.3209\n",
      "Epoch 9, Step 230: Loss = 0.1013\n",
      "Epoch 9, Step 240: Loss = 0.1140\n",
      "Epoch 9, Step 250: Loss = 0.1041\n",
      "Epoch 9, Step 260: Loss = 0.1476\n",
      "Epoch 9, Step 270: Loss = 0.0227\n",
      "Epoch 9, Step 280: Loss = 0.5331\n",
      "Epoch 9, Step 290: Loss = 0.0172\n",
      "Epoch 9, Step 300: Loss = 0.0765\n",
      "Epoch 9, Step 310: Loss = 0.2861\n",
      "Epoch 9, Step 320: Loss = 0.0000\n",
      "Epoch 9, Step 330: Loss = 0.3428\n",
      "Epoch 9, Step 340: Loss = 0.2447\n",
      "Epoch 9, Step 350: Loss = 0.0424\n",
      "Epoch 9, Step 360: Loss = 0.0000\n",
      "Epoch 9, Step 370: Loss = 0.2486\n",
      "Epoch 9, Step 380: Loss = 0.0000\n",
      "Epoch 9, Step 390: Loss = 0.0347\n",
      "Epoch 9, Step 400: Loss = 0.0053\n",
      "Epoch 9, Step 410: Loss = 0.0000\n",
      "Epoch 9, Step 420: Loss = 0.0000\n",
      "Epoch 9, Step 430: Loss = 0.3481\n",
      "Epoch 9, Step 440: Loss = 0.1189\n",
      "Epoch 9, Step 450: Loss = 0.3061\n",
      "Epoch 9, Step 460: Loss = 0.3810\n",
      "Epoch 9, Step 470: Loss = 0.1540\n",
      "Epoch 9, Step 480: Loss = 0.0093\n",
      "Epoch 9, Step 490: Loss = 0.0349\n",
      "Epoch 9, Step 500: Loss = 0.0000\n",
      "Epoch 9, Step 510: Loss = 0.0000\n",
      "Epoch 9, Step 520: Loss = 0.0000\n",
      "Epoch 9, Step 530: Loss = 0.0344\n",
      "Epoch 9, Step 540: Loss = 0.0000\n",
      "Epoch 9, Step 550: Loss = 0.0624\n",
      "Epoch 9, Step 560: Loss = 0.0292\n",
      "Epoch 9, Step 570: Loss = 0.0896\n",
      "Epoch 9, Step 580: Loss = 0.0190\n",
      "Epoch 9, Step 590: Loss = 0.5356\n",
      "\n",
      "Epoch 9 Summary:\n",
      "Average Training Loss: 0.2132\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2361\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    torch.cuda.set_device(0)  \n",
    "    return torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataset class with data cleaning\n",
    "class ContrastiveEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        # Clean each text field\n",
    "        emails_df['sender'] = emails_df['sender'].apply(clean_text)\n",
    "        emails_df['subject'] = emails_df['subject'].apply(clean_text)\n",
    "        emails_df['body'] = emails_df['body'].apply(clean_text)\n",
    "        \n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.ham_indices = self.emails_df[self.emails_df['label'] == 0].index.tolist()\n",
    "        self.phish_indices = self.emails_df[self.emails_df['label'] == 1].index.tolist()\n",
    "        if not (self.ham_indices and self.phish_indices):\n",
    "            raise ValueError(\"Dataset must contain examples of both classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def _get_random_email_idx(self, label):\n",
    "        indices = self.ham_indices if label == 0 else self.phish_indices\n",
    "        return np.random.choice(indices)\n",
    "\n",
    "    def _prepare_email_input(self, email):\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        tokenized_output = self.tokenizer(input_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        return tokenized_output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_email = self.emails_df.iloc[idx]\n",
    "        anchor_label = anchor_email['label']\n",
    "        positives, negatives = [], []\n",
    "\n",
    "        for _ in range(3):\n",
    "            pos_idx = self._get_random_email_idx(anchor_label)\n",
    "            neg_idx = self._get_random_email_idx(1 - anchor_label)\n",
    "            positives.append(self._prepare_email_input(self.emails_df.iloc[pos_idx]))\n",
    "            negatives.append(self._prepare_email_input(self.emails_df.iloc[neg_idx]))\n",
    "\n",
    "        anchor_inputs = self._prepare_email_input(anchor_email)\n",
    "        return {\n",
    "            'anchor_input_ids': anchor_inputs['input_ids'].squeeze(),\n",
    "            'anchor_attention_mask': anchor_inputs['attention_mask'].squeeze(),\n",
    "            'positive_input_ids': torch.stack([p['input_ids'].squeeze() for p in positives]),\n",
    "            'positive_attention_mask': torch.stack([p['attention_mask'].squeeze() for p in positives]),\n",
    "            'negative_input_ids': torch.stack([n['input_ids'].squeeze() for n in negatives]),\n",
    "            'negative_attention_mask': torch.stack([n['attention_mask'].squeeze() for n in negatives])\n",
    "        }\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, LlamaForSequenceClassification\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    # Use the fast tokenizer to avoid the slow/legacy mismatch\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)  \n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Load model config\n",
    "    model_config = AutoConfig.from_pretrained(model_name, num_labels=2, pad_token_id=tokenizer.pad_token_id, use_cache=False)\n",
    "\n",
    "    # Quantize base model\n",
    "    quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base = LlamaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,             # if needed for this checkpoint\n",
    "    )\n",
    "\n",
    "    # Attach LoRA\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "    )\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9, margin=1.0, accumulation_steps=2):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    model = model.to(device).to(torch.float16)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                # Compute embeddings for anchor, positive, and negative samples in parallel\n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                # Reshape positive and negative embeddings to match the batch structure (batch_size, 3, embedding_dim)\n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "                # Calculate the triplet loss for each triplet in the batch\n",
    "                loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "                loss /= accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        val_metrics = evaluate_model(model, val_loader, device, margin)\n",
    "        print_metrics(epoch, total_loss, len(train_loader), val_metrics)\n",
    "\n",
    "        if val_metrics['val_loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['val_loss']\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model(model, val_loader, device, margin=1.0):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "               \n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                \n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "               \n",
    "                val_loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "    return {'val_loss': avg_val_loss}\n",
    "\n",
    "\n",
    "# Main function\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  \n",
    "        text = \"\"\n",
    "    \n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def print_metrics(epoch, total_loss, num_batches, val_metrics):\n",
    "    \n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "def main():\n",
    "    login(token=\"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\")\n",
    "    device = setup_environment()\n",
    "    model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/newdata_cleaned.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "\n",
    "   \n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "    train_dataset = ContrastiveEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = ContrastiveEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "    best_model_state = train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/contrastive_8B\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,s\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a57236",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5943a4d13349dc91cb754764363dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at dreamgen/WizardLM-2-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 1.3188\n",
      "Epoch 1, Step 10: Loss = 1.6830\n",
      "Epoch 1, Step 20: Loss = 1.6642\n",
      "Epoch 1, Step 30: Loss = 1.7365\n",
      "Epoch 1, Step 40: Loss = 1.0800\n",
      "Epoch 1, Step 50: Loss = 2.0796\n",
      "Epoch 1, Step 60: Loss = 0.8108\n",
      "Epoch 1, Step 70: Loss = 1.3426\n",
      "Epoch 1, Step 80: Loss = 0.7914\n",
      "Epoch 1, Step 90: Loss = 2.4679\n",
      "Epoch 1, Step 100: Loss = 0.9290\n",
      "Epoch 1, Step 110: Loss = 2.1301\n",
      "Epoch 1, Step 120: Loss = 1.8296\n",
      "Epoch 1, Step 130: Loss = 1.4450\n",
      "Epoch 1, Step 140: Loss = 2.0599\n",
      "Epoch 1, Step 150: Loss = 2.2248\n",
      "Epoch 1, Step 160: Loss = 1.5988\n",
      "Epoch 1, Step 170: Loss = 0.8125\n",
      "Epoch 1, Step 180: Loss = 1.5921\n",
      "Epoch 1, Step 190: Loss = 1.4128\n",
      "Epoch 1, Step 200: Loss = 1.1420\n",
      "Epoch 1, Step 210: Loss = 1.5280\n",
      "Epoch 1, Step 220: Loss = 0.9515\n",
      "Epoch 1, Step 230: Loss = 2.5399\n",
      "Epoch 1, Step 240: Loss = 1.7433\n",
      "Epoch 1, Step 250: Loss = 1.1936\n",
      "Epoch 1, Step 260: Loss = 1.2059\n",
      "Epoch 1, Step 270: Loss = 1.9741\n",
      "Epoch 1, Step 280: Loss = 1.2583\n",
      "Epoch 1, Step 290: Loss = 1.0895\n",
      "Epoch 1, Step 300: Loss = 1.3619\n",
      "Epoch 1, Step 310: Loss = 1.3141\n",
      "Epoch 1, Step 320: Loss = 2.0105\n",
      "Epoch 1, Step 330: Loss = 0.7712\n",
      "Epoch 1, Step 340: Loss = 1.5302\n",
      "Epoch 1, Step 350: Loss = 0.9100\n",
      "Epoch 1, Step 360: Loss = 1.8092\n",
      "Epoch 1, Step 370: Loss = 1.3031\n",
      "Epoch 1, Step 380: Loss = 1.4593\n",
      "Epoch 1, Step 390: Loss = 0.7904\n",
      "Epoch 1, Step 400: Loss = 1.6873\n",
      "Epoch 1, Step 410: Loss = 1.5347\n",
      "Epoch 1, Step 420: Loss = 1.2698\n",
      "Epoch 1, Step 430: Loss = 1.5693\n",
      "Epoch 1, Step 440: Loss = 0.7982\n",
      "Epoch 1, Step 450: Loss = 2.1171\n",
      "Epoch 1, Step 460: Loss = 1.6430\n",
      "Epoch 1, Step 470: Loss = 1.7252\n",
      "Epoch 1, Step 480: Loss = 1.2760\n",
      "Epoch 1, Step 490: Loss = 2.2455\n",
      "Epoch 1, Step 500: Loss = 1.2870\n",
      "Epoch 1, Step 510: Loss = 2.3814\n",
      "Epoch 1, Step 520: Loss = 2.5520\n",
      "Epoch 1, Step 530: Loss = 2.0004\n",
      "Epoch 1, Step 540: Loss = 1.1671\n",
      "Epoch 1, Step 550: Loss = 1.1765\n",
      "Epoch 1, Step 560: Loss = 1.6178\n",
      "Epoch 1, Step 570: Loss = 1.5620\n",
      "Epoch 1, Step 580: Loss = 1.5593\n",
      "Epoch 1, Step 590: Loss = 0.9759\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Average Training Loss: 3.0170\n",
      "Validation Metrics:\n",
      "Val_loss: 2.7564\n",
      "Epoch 2, Step 0: Loss = 1.3150\n",
      "Epoch 2, Step 10: Loss = 2.1549\n",
      "Epoch 2, Step 20: Loss = 2.1175\n",
      "Epoch 2, Step 30: Loss = 0.9279\n",
      "Epoch 2, Step 40: Loss = 1.2971\n",
      "Epoch 2, Step 50: Loss = 1.9331\n",
      "Epoch 2, Step 60: Loss = 1.0699\n",
      "Epoch 2, Step 70: Loss = 0.6684\n",
      "Epoch 2, Step 80: Loss = 1.8462\n",
      "Epoch 2, Step 90: Loss = 1.5541\n",
      "Epoch 2, Step 100: Loss = 2.1148\n",
      "Epoch 2, Step 110: Loss = 1.3830\n",
      "Epoch 2, Step 120: Loss = 0.9653\n",
      "Epoch 2, Step 130: Loss = 0.7833\n",
      "Epoch 2, Step 140: Loss = 2.5131\n",
      "Epoch 2, Step 150: Loss = 1.7382\n",
      "Epoch 2, Step 160: Loss = 0.9340\n",
      "Epoch 2, Step 170: Loss = 1.2998\n",
      "Epoch 2, Step 180: Loss = 1.3149\n",
      "Epoch 2, Step 190: Loss = 0.5918\n",
      "Epoch 2, Step 200: Loss = 1.3005\n",
      "Epoch 2, Step 210: Loss = 1.8571\n",
      "Epoch 2, Step 220: Loss = 1.9951\n",
      "Epoch 2, Step 230: Loss = 1.7703\n",
      "Epoch 2, Step 240: Loss = 0.9515\n",
      "Epoch 2, Step 250: Loss = 1.8721\n",
      "Epoch 2, Step 260: Loss = 1.2604\n",
      "Epoch 2, Step 270: Loss = 0.9394\n",
      "Epoch 2, Step 280: Loss = 1.7895\n",
      "Epoch 2, Step 290: Loss = 1.5111\n",
      "Epoch 2, Step 300: Loss = 0.4613\n",
      "Epoch 2, Step 310: Loss = 1.4697\n",
      "Epoch 2, Step 320: Loss = 0.4730\n",
      "Epoch 2, Step 330: Loss = 1.6294\n",
      "Epoch 2, Step 340: Loss = 1.2339\n",
      "Epoch 2, Step 350: Loss = 2.0582\n",
      "Epoch 2, Step 360: Loss = 0.3706\n",
      "Epoch 2, Step 370: Loss = 1.1846\n",
      "Epoch 2, Step 380: Loss = 1.7318\n",
      "Epoch 2, Step 390: Loss = 0.4016\n",
      "Epoch 2, Step 400: Loss = 0.8388\n",
      "Epoch 2, Step 410: Loss = 1.2616\n",
      "Epoch 2, Step 420: Loss = 1.2879\n",
      "Epoch 2, Step 430: Loss = 1.0090\n",
      "Epoch 2, Step 440: Loss = 0.9638\n",
      "Epoch 2, Step 450: Loss = 1.2127\n",
      "Epoch 2, Step 460: Loss = 0.2128\n",
      "Epoch 2, Step 470: Loss = 0.6798\n",
      "Epoch 2, Step 480: Loss = 0.2881\n",
      "Epoch 2, Step 490: Loss = 1.8811\n",
      "Epoch 2, Step 500: Loss = 0.9330\n",
      "Epoch 2, Step 510: Loss = 1.0820\n",
      "Epoch 2, Step 520: Loss = 0.1449\n",
      "Epoch 2, Step 530: Loss = 1.2740\n",
      "Epoch 2, Step 540: Loss = 1.5674\n",
      "Epoch 2, Step 550: Loss = 0.6333\n",
      "Epoch 2, Step 560: Loss = 2.0866\n",
      "Epoch 2, Step 570: Loss = 0.9474\n",
      "Epoch 2, Step 580: Loss = 1.5481\n",
      "Epoch 2, Step 590: Loss = 0.7847\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Average Training Loss: 2.4299\n",
      "Validation Metrics:\n",
      "Val_loss: 1.8644\n",
      "Epoch 3, Step 0: Loss = 0.9279\n",
      "Epoch 3, Step 10: Loss = 0.6182\n",
      "Epoch 3, Step 20: Loss = 0.6554\n",
      "Epoch 3, Step 30: Loss = 0.6602\n",
      "Epoch 3, Step 40: Loss = 1.7799\n",
      "Epoch 3, Step 50: Loss = 1.4930\n",
      "Epoch 3, Step 60: Loss = 1.3648\n",
      "Epoch 3, Step 70: Loss = 0.6302\n",
      "Epoch 3, Step 80: Loss = 1.0039\n",
      "Epoch 3, Step 90: Loss = 1.2554\n",
      "Epoch 3, Step 100: Loss = 0.1316\n",
      "Epoch 3, Step 110: Loss = 1.1164\n",
      "Epoch 3, Step 120: Loss = 1.1432\n",
      "Epoch 3, Step 130: Loss = 0.5149\n",
      "Epoch 3, Step 140: Loss = 0.3072\n",
      "Epoch 3, Step 150: Loss = 1.0508\n",
      "Epoch 3, Step 160: Loss = 0.8121\n",
      "Epoch 3, Step 170: Loss = 1.6467\n",
      "Epoch 3, Step 180: Loss = 0.8038\n",
      "Epoch 3, Step 190: Loss = 1.3691\n",
      "Epoch 3, Step 200: Loss = 0.8304\n",
      "Epoch 3, Step 210: Loss = 0.6812\n",
      "Epoch 3, Step 220: Loss = 1.0377\n",
      "Epoch 3, Step 230: Loss = 2.1193\n",
      "Epoch 3, Step 240: Loss = 0.6271\n",
      "Epoch 3, Step 250: Loss = 1.4111\n",
      "Epoch 3, Step 260: Loss = 0.8179\n",
      "Epoch 3, Step 270: Loss = 0.3591\n",
      "Epoch 3, Step 280: Loss = 0.9102\n",
      "Epoch 3, Step 290: Loss = 0.6664\n",
      "Epoch 3, Step 300: Loss = 1.8566\n",
      "Epoch 3, Step 310: Loss = 0.8899\n",
      "Epoch 3, Step 320: Loss = 0.8674\n",
      "Epoch 3, Step 330: Loss = 0.5450\n",
      "Epoch 3, Step 340: Loss = 1.4103\n",
      "Epoch 3, Step 350: Loss = 0.4979\n",
      "Epoch 3, Step 360: Loss = 0.8287\n",
      "Epoch 3, Step 370: Loss = 2.1262\n",
      "Epoch 3, Step 380: Loss = 1.2494\n",
      "Epoch 3, Step 390: Loss = 0.5910\n",
      "Epoch 3, Step 400: Loss = 0.5342\n",
      "Epoch 3, Step 410: Loss = 0.8871\n",
      "Epoch 3, Step 420: Loss = 1.5702\n",
      "Epoch 3, Step 430: Loss = 0.2040\n",
      "Epoch 3, Step 440: Loss = 1.4528\n",
      "Epoch 3, Step 450: Loss = 1.6238\n",
      "Epoch 3, Step 460: Loss = 0.8449\n",
      "Epoch 3, Step 470: Loss = 0.8449\n",
      "Epoch 3, Step 480: Loss = 2.1222\n",
      "Epoch 3, Step 490: Loss = 0.2565\n",
      "Epoch 3, Step 500: Loss = 1.4327\n",
      "Epoch 3, Step 510: Loss = 1.1056\n",
      "Epoch 3, Step 520: Loss = 0.1911\n",
      "Epoch 3, Step 530: Loss = 0.4145\n",
      "Epoch 3, Step 540: Loss = 1.0917\n",
      "Epoch 3, Step 550: Loss = 0.9198\n",
      "Epoch 3, Step 560: Loss = 0.9410\n",
      "Epoch 3, Step 570: Loss = 0.1399\n",
      "Epoch 3, Step 580: Loss = 0.5261\n",
      "Epoch 3, Step 590: Loss = 0.5950\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Average Training Loss: 1.7106\n",
      "Validation Metrics:\n",
      "Val_loss: 1.3019\n",
      "Epoch 4, Step 0: Loss = 0.8671\n",
      "Epoch 4, Step 10: Loss = 1.0249\n",
      "Epoch 4, Step 20: Loss = 0.7957\n",
      "Epoch 4, Step 30: Loss = 1.8587\n",
      "Epoch 4, Step 40: Loss = 0.6352\n",
      "Epoch 4, Step 50: Loss = 0.1827\n",
      "Epoch 4, Step 60: Loss = 2.0430\n",
      "Epoch 4, Step 70: Loss = 1.2433\n",
      "Epoch 4, Step 80: Loss = 0.9916\n",
      "Epoch 4, Step 90: Loss = 0.2850\n",
      "Epoch 4, Step 100: Loss = 1.0568\n",
      "Epoch 4, Step 110: Loss = 0.9336\n",
      "Epoch 4, Step 120: Loss = 0.4919\n",
      "Epoch 4, Step 130: Loss = 1.9798\n",
      "Epoch 4, Step 140: Loss = 0.6367\n",
      "Epoch 4, Step 150: Loss = 0.6008\n",
      "Epoch 4, Step 160: Loss = 0.4661\n",
      "Epoch 4, Step 170: Loss = 1.6759\n",
      "Epoch 4, Step 180: Loss = 0.4618\n",
      "Epoch 4, Step 190: Loss = 0.5213\n",
      "Epoch 4, Step 200: Loss = 1.1532\n",
      "Epoch 4, Step 210: Loss = 0.9481\n",
      "Epoch 4, Step 220: Loss = 1.0754\n",
      "Epoch 4, Step 230: Loss = 0.5673\n",
      "Epoch 4, Step 240: Loss = 0.5397\n",
      "Epoch 4, Step 250: Loss = 0.8273\n",
      "Epoch 4, Step 260: Loss = 0.1547\n",
      "Epoch 4, Step 270: Loss = 1.7043\n",
      "Epoch 4, Step 280: Loss = 0.7811\n",
      "Epoch 4, Step 290: Loss = 1.1104\n",
      "Epoch 4, Step 300: Loss = 0.4502\n",
      "Epoch 4, Step 310: Loss = 0.6830\n",
      "Epoch 4, Step 320: Loss = 1.1306\n",
      "Epoch 4, Step 330: Loss = 0.8141\n",
      "Epoch 4, Step 340: Loss = 1.2222\n",
      "Epoch 4, Step 350: Loss = 0.2294\n",
      "Epoch 4, Step 360: Loss = 0.3082\n",
      "Epoch 4, Step 370: Loss = 0.2115\n",
      "Epoch 4, Step 380: Loss = 1.1931\n",
      "Epoch 4, Step 390: Loss = 0.4226\n",
      "Epoch 4, Step 400: Loss = 0.6606\n",
      "Epoch 4, Step 410: Loss = 0.3697\n",
      "Epoch 4, Step 420: Loss = 0.4328\n",
      "Epoch 4, Step 430: Loss = 0.4594\n",
      "Epoch 4, Step 440: Loss = 0.6950\n",
      "Epoch 4, Step 450: Loss = 0.1369\n",
      "Epoch 4, Step 460: Loss = 0.4078\n",
      "Epoch 4, Step 470: Loss = 0.3095\n",
      "Epoch 4, Step 480: Loss = 0.7589\n",
      "Epoch 4, Step 490: Loss = 0.3411\n",
      "Epoch 4, Step 500: Loss = 0.3526\n",
      "Epoch 4, Step 510: Loss = 0.2851\n",
      "Epoch 4, Step 520: Loss = 0.4836\n",
      "Epoch 4, Step 530: Loss = 0.5562\n",
      "Epoch 4, Step 540: Loss = 0.1367\n",
      "Epoch 4, Step 550: Loss = 0.6650\n",
      "Epoch 4, Step 560: Loss = 0.1263\n",
      "Epoch 4, Step 570: Loss = 0.3661\n",
      "Epoch 4, Step 580: Loss = 0.5305\n",
      "Epoch 4, Step 590: Loss = 1.8345\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Average Training Loss: 1.3749\n",
      "Validation Metrics:\n",
      "Val_loss: 1.0730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.9105\n",
      "Epoch 5, Step 10: Loss = 0.1370\n",
      "Epoch 5, Step 20: Loss = 0.4942\n",
      "Epoch 5, Step 30: Loss = 0.3818\n",
      "Epoch 5, Step 40: Loss = 0.1322\n",
      "Epoch 5, Step 50: Loss = 0.2818\n",
      "Epoch 5, Step 60: Loss = 0.7809\n",
      "Epoch 5, Step 70: Loss = 0.4391\n",
      "Epoch 5, Step 80: Loss = 0.4273\n",
      "Epoch 5, Step 90: Loss = 0.2813\n",
      "Epoch 5, Step 100: Loss = 0.2222\n",
      "Epoch 5, Step 110: Loss = 0.3155\n",
      "Epoch 5, Step 120: Loss = 0.7569\n",
      "Epoch 5, Step 130: Loss = 0.5390\n",
      "Epoch 5, Step 140: Loss = 1.0921\n",
      "Epoch 5, Step 150: Loss = 0.1243\n",
      "Epoch 5, Step 160: Loss = 0.1373\n",
      "Epoch 5, Step 170: Loss = 0.2413\n",
      "Epoch 5, Step 180: Loss = 0.1682\n",
      "Epoch 5, Step 190: Loss = 1.9631\n",
      "Epoch 5, Step 200: Loss = 1.0903\n",
      "Epoch 5, Step 210: Loss = 0.6169\n",
      "Epoch 5, Step 220: Loss = 0.7336\n",
      "Epoch 5, Step 230: Loss = 0.7288\n",
      "Epoch 5, Step 240: Loss = 1.0659\n",
      "Epoch 5, Step 250: Loss = 0.8512\n",
      "Epoch 5, Step 260: Loss = 0.7342\n",
      "Epoch 5, Step 270: Loss = 1.9058\n",
      "Epoch 5, Step 280: Loss = 0.0959\n",
      "Epoch 5, Step 290: Loss = 1.3089\n",
      "Epoch 5, Step 300: Loss = 0.0476\n",
      "Epoch 5, Step 310: Loss = 0.3782\n",
      "Epoch 5, Step 320: Loss = 0.0390\n",
      "Epoch 5, Step 330: Loss = 0.2804\n",
      "Epoch 5, Step 340: Loss = 0.0087\n",
      "Epoch 5, Step 350: Loss = 0.9664\n",
      "Epoch 5, Step 360: Loss = 0.8337\n",
      "Epoch 5, Step 370: Loss = 0.1617\n",
      "Epoch 5, Step 380: Loss = 0.6083\n",
      "Epoch 5, Step 390: Loss = 0.8420\n",
      "Epoch 5, Step 400: Loss = 0.3135\n",
      "Epoch 5, Step 410: Loss = 0.3712\n",
      "Epoch 5, Step 420: Loss = 0.0000\n",
      "Epoch 5, Step 430: Loss = 0.0000\n",
      "Epoch 5, Step 440: Loss = 1.0820\n",
      "Epoch 5, Step 450: Loss = 0.6156\n",
      "Epoch 5, Step 460: Loss = 0.4898\n",
      "Epoch 5, Step 470: Loss = 0.4218\n",
      "Epoch 5, Step 480: Loss = 0.8884\n",
      "Epoch 5, Step 490: Loss = 0.4515\n",
      "Epoch 5, Step 500: Loss = 0.0000\n",
      "Epoch 5, Step 510: Loss = 1.3945\n",
      "Epoch 5, Step 520: Loss = 1.4009\n",
      "Epoch 5, Step 530: Loss = 0.2969\n",
      "Epoch 5, Step 540: Loss = 0.4980\n",
      "Epoch 5, Step 550: Loss = 0.0887\n",
      "Epoch 5, Step 560: Loss = 1.5883\n",
      "Epoch 5, Step 570: Loss = 0.4409\n",
      "Epoch 5, Step 580: Loss = 0.4366\n",
      "Epoch 5, Step 590: Loss = 0.8669\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Average Training Loss: 1.1522\n",
      "Validation Metrics:\n",
      "Val_loss: 0.9347\n",
      "Epoch 6, Step 0: Loss = 0.3340\n",
      "Epoch 6, Step 10: Loss = 0.1451\n",
      "Epoch 6, Step 20: Loss = 0.4811\n",
      "Epoch 6, Step 30: Loss = 1.9794\n",
      "Epoch 6, Step 40: Loss = 0.4115\n",
      "Epoch 6, Step 50: Loss = 0.3634\n",
      "Epoch 6, Step 60: Loss = 1.0126\n",
      "Epoch 6, Step 70: Loss = 0.2796\n",
      "Epoch 6, Step 80: Loss = 0.2345\n",
      "Epoch 6, Step 90: Loss = 0.2748\n",
      "Epoch 6, Step 100: Loss = 0.2668\n",
      "Epoch 6, Step 110: Loss = 0.2198\n",
      "Epoch 6, Step 120: Loss = 0.5071\n",
      "Epoch 6, Step 130: Loss = 0.9358\n",
      "Epoch 6, Step 140: Loss = 0.1501\n",
      "Epoch 6, Step 150: Loss = 0.0935\n",
      "Epoch 6, Step 160: Loss = 0.1232\n",
      "Epoch 6, Step 170: Loss = 0.0000\n",
      "Epoch 6, Step 180: Loss = 0.0761\n",
      "Epoch 6, Step 190: Loss = 0.1345\n",
      "Epoch 6, Step 200: Loss = 0.6762\n",
      "Epoch 6, Step 210: Loss = 0.8118\n",
      "Epoch 6, Step 220: Loss = 0.1745\n",
      "Epoch 6, Step 230: Loss = 0.2400\n",
      "Epoch 6, Step 240: Loss = 0.2622\n",
      "Epoch 6, Step 250: Loss = 0.5399\n",
      "Epoch 6, Step 260: Loss = 0.4626\n",
      "Epoch 6, Step 270: Loss = 0.4069\n",
      "Epoch 6, Step 280: Loss = 0.1629\n",
      "Epoch 6, Step 290: Loss = 0.1506\n",
      "Epoch 6, Step 300: Loss = 0.4879\n",
      "Epoch 6, Step 310: Loss = 0.2533\n",
      "Epoch 6, Step 320: Loss = 1.3398\n",
      "Epoch 6, Step 330: Loss = 0.3707\n",
      "Epoch 6, Step 340: Loss = 1.9978\n",
      "Epoch 6, Step 350: Loss = 0.1078\n",
      "Epoch 6, Step 360: Loss = 0.6212\n",
      "Epoch 6, Step 370: Loss = 1.0017\n",
      "Epoch 6, Step 380: Loss = 0.4440\n",
      "Epoch 6, Step 390: Loss = 0.1391\n",
      "Epoch 6, Step 400: Loss = 0.6074\n",
      "Epoch 6, Step 410: Loss = 2.4726\n",
      "Epoch 6, Step 420: Loss = 0.2339\n",
      "Epoch 6, Step 430: Loss = 0.2272\n",
      "Epoch 6, Step 440: Loss = 0.1417\n",
      "Epoch 6, Step 450: Loss = 1.2197\n",
      "Epoch 6, Step 460: Loss = 0.6649\n",
      "Epoch 6, Step 470: Loss = 0.2136\n",
      "Epoch 6, Step 480: Loss = 0.2964\n",
      "Epoch 6, Step 490: Loss = 1.0137\n",
      "Epoch 6, Step 500: Loss = 1.0887\n",
      "Epoch 6, Step 510: Loss = 0.2630\n",
      "Epoch 6, Step 520: Loss = 0.8122\n",
      "Epoch 6, Step 530: Loss = 0.3595\n",
      "Epoch 6, Step 540: Loss = 0.0482\n",
      "Epoch 6, Step 550: Loss = 0.6869\n",
      "Epoch 6, Step 560: Loss = 1.1306\n",
      "Epoch 6, Step 570: Loss = 0.1722\n",
      "Epoch 6, Step 580: Loss = 2.0711\n",
      "Epoch 6, Step 590: Loss = 0.3177\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Average Training Loss: 0.9985\n",
      "Validation Metrics:\n",
      "Val_loss: 0.8852\n",
      "Epoch 7, Step 0: Loss = 0.1015\n",
      "Epoch 7, Step 10: Loss = 0.1079\n",
      "Epoch 7, Step 20: Loss = 1.0202\n",
      "Epoch 7, Step 30: Loss = 0.6873\n",
      "Epoch 7, Step 40: Loss = 0.3118\n",
      "Epoch 7, Step 50: Loss = 0.0752\n",
      "Epoch 7, Step 60: Loss = 1.2453\n",
      "Epoch 7, Step 70: Loss = 0.9929\n",
      "Epoch 7, Step 80: Loss = 1.3571\n",
      "Epoch 7, Step 90: Loss = 1.6061\n",
      "Epoch 7, Step 100: Loss = 0.1599\n",
      "Epoch 7, Step 110: Loss = 0.5341\n",
      "Epoch 7, Step 120: Loss = 0.4515\n",
      "Epoch 7, Step 130: Loss = 0.2387\n",
      "Epoch 7, Step 140: Loss = 1.4494\n",
      "Epoch 7, Step 150: Loss = 0.3509\n",
      "Epoch 7, Step 160: Loss = 0.2922\n",
      "Epoch 7, Step 170: Loss = 0.6914\n",
      "Epoch 7, Step 180: Loss = 0.0157\n",
      "Epoch 7, Step 190: Loss = 0.1526\n",
      "Epoch 7, Step 200: Loss = 0.2488\n",
      "Epoch 7, Step 210: Loss = 0.2280\n",
      "Epoch 7, Step 220: Loss = 0.6715\n",
      "Epoch 7, Step 230: Loss = 1.3012\n",
      "Epoch 7, Step 240: Loss = 0.3663\n",
      "Epoch 7, Step 250: Loss = 0.3613\n",
      "Epoch 7, Step 260: Loss = 0.0570\n",
      "Epoch 7, Step 270: Loss = 0.1579\n",
      "Epoch 7, Step 280: Loss = 0.7455\n",
      "Epoch 7, Step 290: Loss = 0.3351\n",
      "Epoch 7, Step 300: Loss = 0.2563\n",
      "Epoch 7, Step 310: Loss = 0.2762\n",
      "Epoch 7, Step 320: Loss = 0.3127\n",
      "Epoch 7, Step 330: Loss = 1.2452\n",
      "Epoch 7, Step 340: Loss = 0.9377\n",
      "Epoch 7, Step 350: Loss = 0.8786\n",
      "Epoch 7, Step 360: Loss = 0.2964\n",
      "Epoch 7, Step 370: Loss = 0.4172\n",
      "Epoch 7, Step 380: Loss = 0.4316\n",
      "Epoch 7, Step 390: Loss = 0.2680\n",
      "Epoch 7, Step 400: Loss = 0.4626\n",
      "Epoch 7, Step 410: Loss = 1.0041\n",
      "Epoch 7, Step 420: Loss = 0.4255\n",
      "Epoch 7, Step 430: Loss = 1.0497\n",
      "Epoch 7, Step 440: Loss = 0.2254\n",
      "Epoch 7, Step 450: Loss = 0.2619\n",
      "Epoch 7, Step 460: Loss = 0.4905\n",
      "Epoch 7, Step 470: Loss = 1.3605\n",
      "Epoch 7, Step 480: Loss = 0.6125\n",
      "Epoch 7, Step 490: Loss = 0.7525\n",
      "Epoch 7, Step 500: Loss = 0.6217\n",
      "Epoch 7, Step 510: Loss = 0.2964\n",
      "Epoch 7, Step 520: Loss = 0.4696\n",
      "Epoch 7, Step 530: Loss = 0.9490\n",
      "Epoch 7, Step 540: Loss = 0.4135\n",
      "Epoch 7, Step 550: Loss = 0.8115\n",
      "Epoch 7, Step 560: Loss = 0.1205\n",
      "Epoch 7, Step 570: Loss = 0.5594\n",
      "Epoch 7, Step 580: Loss = 0.8262\n",
      "Epoch 7, Step 590: Loss = 0.1086\n",
      "\n",
      "Epoch 7 Summary:\n",
      "Average Training Loss: 0.9686\n",
      "Validation Metrics:\n",
      "Val_loss: 0.7902\n",
      "Epoch 8, Step 0: Loss = 0.2940\n",
      "Epoch 8, Step 10: Loss = 0.6999\n",
      "Epoch 8, Step 20: Loss = 0.0000\n",
      "Epoch 8, Step 30: Loss = 0.0000\n",
      "Epoch 8, Step 40: Loss = 0.1167\n",
      "Epoch 8, Step 50: Loss = 0.1544\n",
      "Epoch 8, Step 60: Loss = 0.3033\n",
      "Epoch 8, Step 70: Loss = 0.1186\n",
      "Epoch 8, Step 80: Loss = 0.2737\n",
      "Epoch 8, Step 90: Loss = 0.0000\n",
      "Epoch 8, Step 100: Loss = 0.4249\n",
      "Epoch 8, Step 110: Loss = 0.1735\n",
      "Epoch 8, Step 120: Loss = 0.8234\n",
      "Epoch 8, Step 130: Loss = 0.2275\n",
      "Epoch 8, Step 140: Loss = 0.0000\n",
      "Epoch 8, Step 150: Loss = 0.1411\n",
      "Epoch 8, Step 160: Loss = 0.0000\n",
      "Epoch 8, Step 170: Loss = 0.4625\n",
      "Epoch 8, Step 180: Loss = 0.4236\n",
      "Epoch 8, Step 190: Loss = 0.8752\n",
      "Epoch 8, Step 200: Loss = 1.1775\n",
      "Epoch 8, Step 210: Loss = 0.0401\n",
      "Epoch 8, Step 220: Loss = 0.1226\n",
      "Epoch 8, Step 230: Loss = 0.2783\n",
      "Epoch 8, Step 240: Loss = 0.7593\n",
      "Epoch 8, Step 250: Loss = 0.5296\n",
      "Epoch 8, Step 260: Loss = 0.0801\n",
      "Epoch 8, Step 270: Loss = 0.1959\n",
      "Epoch 8, Step 280: Loss = 0.0913\n",
      "Epoch 8, Step 290: Loss = 0.1712\n",
      "Epoch 8, Step 300: Loss = 0.6080\n",
      "Epoch 8, Step 310: Loss = 0.7660\n",
      "Epoch 8, Step 320: Loss = 0.0625\n",
      "Epoch 8, Step 330: Loss = 0.3728\n",
      "Epoch 8, Step 340: Loss = 0.0000\n",
      "Epoch 8, Step 350: Loss = 0.6710\n",
      "Epoch 8, Step 360: Loss = 0.4873\n",
      "Epoch 8, Step 370: Loss = 0.2860\n",
      "Epoch 8, Step 380: Loss = 0.5943\n",
      "Epoch 8, Step 390: Loss = 0.9586\n",
      "Epoch 8, Step 400: Loss = 0.7301\n",
      "Epoch 8, Step 410: Loss = 1.2434\n",
      "Epoch 8, Step 420: Loss = 0.2354\n",
      "Epoch 8, Step 430: Loss = 0.3168\n",
      "Epoch 8, Step 440: Loss = 0.0000\n",
      "Epoch 8, Step 450: Loss = 0.1068\n",
      "Epoch 8, Step 460: Loss = 0.3121\n",
      "Epoch 8, Step 470: Loss = 0.0000\n",
      "Epoch 8, Step 480: Loss = 0.7810\n",
      "Epoch 8, Step 490: Loss = 1.0402\n",
      "Epoch 8, Step 500: Loss = 0.1338\n",
      "Epoch 8, Step 510: Loss = 0.0000\n",
      "Epoch 8, Step 520: Loss = 0.4036\n",
      "Epoch 8, Step 530: Loss = 0.2934\n",
      "Epoch 8, Step 540: Loss = 0.2012\n",
      "Epoch 8, Step 550: Loss = 0.8525\n",
      "Epoch 8, Step 560: Loss = 0.3216\n",
      "Epoch 8, Step 570: Loss = 0.5212\n",
      "Epoch 8, Step 580: Loss = 0.1898\n",
      "Epoch 8, Step 590: Loss = 0.5280\n",
      "\n",
      "Epoch 8 Summary:\n",
      "Average Training Loss: 0.8599\n",
      "Validation Metrics:\n",
      "Val_loss: 0.7728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 0: Loss = 1.3125\n",
      "Epoch 9, Step 10: Loss = 1.0837\n",
      "Epoch 9, Step 20: Loss = 0.3677\n",
      "Epoch 9, Step 30: Loss = 0.0000\n",
      "Epoch 9, Step 40: Loss = 0.1115\n",
      "Epoch 9, Step 50: Loss = 0.3065\n",
      "Epoch 9, Step 60: Loss = 0.2711\n",
      "Epoch 9, Step 70: Loss = 0.9611\n",
      "Epoch 9, Step 80: Loss = 0.7760\n",
      "Epoch 9, Step 90: Loss = 0.4306\n",
      "Epoch 9, Step 100: Loss = 0.0343\n",
      "Epoch 9, Step 110: Loss = 0.1955\n",
      "Epoch 9, Step 120: Loss = 0.0612\n",
      "Epoch 9, Step 130: Loss = 0.6497\n",
      "Epoch 9, Step 140: Loss = 0.2490\n",
      "Epoch 9, Step 150: Loss = 0.6360\n",
      "Epoch 9, Step 160: Loss = 0.2944\n",
      "Epoch 9, Step 170: Loss = 0.4716\n",
      "Epoch 9, Step 180: Loss = 0.1511\n",
      "Epoch 9, Step 190: Loss = 0.6909\n",
      "Epoch 9, Step 200: Loss = 0.0822\n",
      "Epoch 9, Step 210: Loss = 0.1519\n",
      "Epoch 9, Step 220: Loss = 0.0369\n",
      "Epoch 9, Step 230: Loss = 1.2739\n",
      "Epoch 9, Step 240: Loss = 0.6786\n",
      "Epoch 9, Step 250: Loss = 0.1609\n",
      "Epoch 9, Step 260: Loss = 0.2369\n",
      "Epoch 9, Step 270: Loss = 0.4917\n",
      "Epoch 9, Step 280: Loss = 0.3892\n",
      "Epoch 9, Step 290: Loss = 0.2310\n",
      "Epoch 9, Step 300: Loss = 0.6928\n",
      "Epoch 9, Step 310: Loss = 0.2589\n",
      "Epoch 9, Step 320: Loss = 1.1645\n",
      "Epoch 9, Step 330: Loss = 0.8403\n",
      "Epoch 9, Step 340: Loss = 0.3403\n",
      "Epoch 9, Step 350: Loss = 0.5789\n",
      "Epoch 9, Step 360: Loss = 0.0158\n",
      "Epoch 9, Step 370: Loss = 0.0000\n",
      "Epoch 9, Step 380: Loss = 0.0942\n",
      "Epoch 9, Step 390: Loss = 0.0000\n",
      "Epoch 9, Step 400: Loss = 0.0995\n",
      "Epoch 9, Step 410: Loss = 0.5462\n",
      "Epoch 9, Step 420: Loss = 0.1692\n",
      "Epoch 9, Step 430: Loss = 0.7245\n",
      "Epoch 9, Step 440: Loss = 0.0000\n",
      "Epoch 9, Step 450: Loss = 0.5719\n",
      "Epoch 9, Step 460: Loss = 0.2952\n",
      "Epoch 9, Step 470: Loss = 1.3435\n",
      "Epoch 9, Step 480: Loss = 0.2784\n",
      "Epoch 9, Step 490: Loss = 0.2544\n",
      "Epoch 9, Step 500: Loss = 0.3709\n",
      "Epoch 9, Step 510: Loss = 1.2205\n",
      "Epoch 9, Step 520: Loss = 0.0000\n",
      "Epoch 9, Step 530: Loss = 0.2805\n",
      "Epoch 9, Step 540: Loss = 0.0654\n",
      "Epoch 9, Step 550: Loss = 0.4554\n",
      "Epoch 9, Step 560: Loss = 0.2191\n",
      "Epoch 9, Step 570: Loss = 0.3136\n",
      "Epoch 9, Step 580: Loss = 0.7645\n",
      "Epoch 9, Step 590: Loss = 0.2563\n",
      "\n",
      "Epoch 9 Summary:\n",
      "Average Training Loss: 0.8227\n",
      "Validation Metrics:\n",
      "Val_loss: 0.7177\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    torch.cuda.set_device(0)  \n",
    "    return torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataset class with data cleaning\n",
    "class ContrastiveEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        # Clean each text field\n",
    "        emails_df['sender'] = emails_df['sender'].apply(clean_text)\n",
    "        emails_df['subject'] = emails_df['subject'].apply(clean_text)\n",
    "        emails_df['body'] = emails_df['body'].apply(clean_text)\n",
    "        \n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.ham_indices = self.emails_df[self.emails_df['label'] == 0].index.tolist()\n",
    "        self.phish_indices = self.emails_df[self.emails_df['label'] == 1].index.tolist()\n",
    "        if not (self.ham_indices and self.phish_indices):\n",
    "            raise ValueError(\"Dataset must contain examples of both classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def _get_random_email_idx(self, label):\n",
    "        indices = self.ham_indices if label == 0 else self.phish_indices\n",
    "        return np.random.choice(indices)\n",
    "\n",
    "    def _prepare_email_input(self, email):\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        tokenized_output = self.tokenizer(input_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        return tokenized_output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_email = self.emails_df.iloc[idx]\n",
    "        anchor_label = anchor_email['label']\n",
    "        positives, negatives = [], []\n",
    "\n",
    "        for _ in range(3):\n",
    "            pos_idx = self._get_random_email_idx(anchor_label)\n",
    "            neg_idx = self._get_random_email_idx(1 - anchor_label)\n",
    "            positives.append(self._prepare_email_input(self.emails_df.iloc[pos_idx]))\n",
    "            negatives.append(self._prepare_email_input(self.emails_df.iloc[neg_idx]))\n",
    "\n",
    "        anchor_inputs = self._prepare_email_input(anchor_email)\n",
    "        return {\n",
    "            'anchor_input_ids': anchor_inputs['input_ids'].squeeze(),\n",
    "            'anchor_attention_mask': anchor_inputs['attention_mask'].squeeze(),\n",
    "            'positive_input_ids': torch.stack([p['input_ids'].squeeze() for p in positives]),\n",
    "            'positive_attention_mask': torch.stack([p['attention_mask'].squeeze() for p in positives]),\n",
    "            'negative_input_ids': torch.stack([n['input_ids'].squeeze() for n in negatives]),\n",
    "            'negative_attention_mask': torch.stack([n['attention_mask'].squeeze() for n in negatives])\n",
    "        }\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, LlamaForSequenceClassification,AutoModelForSequenceClassification\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    # Use the fast tokenizer to avoid the slow/legacy mismatch\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)  \n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Load model config\n",
    "    model_config = AutoConfig.from_pretrained(model_name, num_labels=2, pad_token_id=tokenizer.pad_token_id, use_cache=False)\n",
    "\n",
    "    # Quantize base model\n",
    "    quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,             # if needed for this checkpoint\n",
    "    )\n",
    "\n",
    "    # Attach LoRA\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "    )\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9, margin=1.0, accumulation_steps=2):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    model = model.to(device).to(torch.float16)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                # Compute embeddings for anchor, positive, and negative samples in parallel\n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                # Reshape positive and negative embeddings to match the batch structure (batch_size, 3, embedding_dim)\n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "                # Calculate the triplet loss for each triplet in the batch\n",
    "                loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "                loss /= accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        val_metrics = evaluate_model(model, val_loader, device, margin)\n",
    "        print_metrics(epoch, total_loss, len(train_loader), val_metrics)\n",
    "\n",
    "        if val_metrics['val_loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['val_loss']\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model(model, val_loader, device, margin=1.0):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "               \n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                \n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "               \n",
    "                val_loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "    return {'val_loss': avg_val_loss}\n",
    "\n",
    "\n",
    "# Main function\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  \n",
    "        text = \"\"\n",
    "    \n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def print_metrics(epoch, total_loss, num_batches, val_metrics):\n",
    "    \n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "def main():\n",
    "    login(token=\"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\")\n",
    "    device = setup_environment()\n",
    "    model_name = 'dreamgen/WizardLM-2-7B'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/newdata_cleaned.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "\n",
    "   \n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "    train_dataset = ContrastiveEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = ContrastiveEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "    best_model_state = train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/contrastive_7B_wiz\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f44f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738ae88e0a174a768f8f9a1a20cf8f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 1.4422\n",
      "Epoch 1, Step 10: Loss = 1.1817\n",
      "Epoch 1, Step 20: Loss = 2.0573\n",
      "Epoch 1, Step 30: Loss = 2.1412\n",
      "Epoch 1, Step 40: Loss = 1.9548\n",
      "Epoch 1, Step 50: Loss = 1.0802\n",
      "Epoch 1, Step 60: Loss = 1.6772\n",
      "Epoch 1, Step 70: Loss = 0.9746\n",
      "Epoch 1, Step 80: Loss = 0.9204\n",
      "Epoch 1, Step 90: Loss = 1.4151\n",
      "Epoch 1, Step 100: Loss = 0.9177\n",
      "Epoch 1, Step 110: Loss = 1.4063\n",
      "Epoch 1, Step 120: Loss = 1.3937\n",
      "Epoch 1, Step 130: Loss = 1.1164\n",
      "Epoch 1, Step 140: Loss = 1.7856\n",
      "Epoch 1, Step 150: Loss = 1.6946\n",
      "Epoch 1, Step 160: Loss = 1.1235\n",
      "Epoch 1, Step 170: Loss = 1.4161\n",
      "Epoch 1, Step 180: Loss = 1.5961\n",
      "Epoch 1, Step 190: Loss = 1.7093\n",
      "Epoch 1, Step 200: Loss = 2.7760\n",
      "Epoch 1, Step 210: Loss = 2.1215\n",
      "Epoch 1, Step 220: Loss = 1.0533\n",
      "Epoch 1, Step 230: Loss = 1.7038\n",
      "Epoch 1, Step 240: Loss = 0.8380\n",
      "Epoch 1, Step 250: Loss = 0.8123\n",
      "Epoch 1, Step 260: Loss = 1.9257\n",
      "Epoch 1, Step 270: Loss = 1.6220\n",
      "Epoch 1, Step 280: Loss = 1.6277\n",
      "Epoch 1, Step 290: Loss = 1.1407\n",
      "Epoch 1, Step 300: Loss = 1.4769\n",
      "Epoch 1, Step 310: Loss = 1.0934\n",
      "Epoch 1, Step 320: Loss = 1.7159\n",
      "Epoch 1, Step 330: Loss = 1.6800\n",
      "Epoch 1, Step 340: Loss = 1.6737\n",
      "Epoch 1, Step 350: Loss = 1.2999\n",
      "Epoch 1, Step 360: Loss = 1.2944\n",
      "Epoch 1, Step 370: Loss = 2.0505\n",
      "Epoch 1, Step 380: Loss = 1.4075\n",
      "Epoch 1, Step 390: Loss = 2.1649\n",
      "Epoch 1, Step 400: Loss = 1.2151\n",
      "Epoch 1, Step 410: Loss = 0.8408\n",
      "Epoch 1, Step 420: Loss = 1.1122\n",
      "Epoch 1, Step 430: Loss = 1.9961\n",
      "Epoch 1, Step 440: Loss = 1.7240\n",
      "Epoch 1, Step 450: Loss = 1.7836\n",
      "Epoch 1, Step 460: Loss = 2.1024\n",
      "Epoch 1, Step 470: Loss = 1.3380\n",
      "Epoch 1, Step 480: Loss = 1.7405\n",
      "Epoch 1, Step 490: Loss = 1.5194\n",
      "Epoch 1, Step 500: Loss = 0.9058\n",
      "Epoch 1, Step 510: Loss = 1.3801\n",
      "Epoch 1, Step 520: Loss = 1.4311\n",
      "Epoch 1, Step 530: Loss = 1.3890\n",
      "Epoch 1, Step 540: Loss = 1.5504\n",
      "Epoch 1, Step 550: Loss = 0.6527\n",
      "Epoch 1, Step 560: Loss = 1.5570\n",
      "Epoch 1, Step 570: Loss = 1.0493\n",
      "Epoch 1, Step 580: Loss = 1.4240\n",
      "Epoch 1, Step 590: Loss = 1.3105\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Average Training Loss: 2.8195\n",
      "Validation Metrics:\n",
      "Val_loss: 2.4345\n",
      "Epoch 2, Step 0: Loss = 1.1607\n",
      "Epoch 2, Step 10: Loss = 0.8737\n",
      "Epoch 2, Step 20: Loss = 1.5286\n",
      "Epoch 2, Step 30: Loss = 0.8969\n",
      "Epoch 2, Step 40: Loss = 0.7210\n",
      "Epoch 2, Step 50: Loss = 1.1022\n",
      "Epoch 2, Step 60: Loss = 0.8088\n",
      "Epoch 2, Step 70: Loss = 0.7377\n",
      "Epoch 2, Step 80: Loss = 1.1413\n",
      "Epoch 2, Step 90: Loss = 1.7065\n",
      "Epoch 2, Step 100: Loss = 1.1837\n",
      "Epoch 2, Step 110: Loss = 1.3643\n",
      "Epoch 2, Step 120: Loss = 1.3032\n",
      "Epoch 2, Step 130: Loss = 1.0018\n",
      "Epoch 2, Step 140: Loss = 0.7382\n",
      "Epoch 2, Step 150: Loss = 1.9997\n",
      "Epoch 2, Step 160: Loss = 0.9419\n",
      "Epoch 2, Step 170: Loss = 0.7688\n",
      "Epoch 2, Step 180: Loss = 1.3467\n",
      "Epoch 2, Step 190: Loss = 0.3756\n",
      "Epoch 2, Step 200: Loss = 0.5386\n",
      "Epoch 2, Step 210: Loss = 1.8670\n",
      "Epoch 2, Step 220: Loss = 1.5262\n",
      "Epoch 2, Step 230: Loss = 0.8158\n",
      "Epoch 2, Step 240: Loss = 1.9732\n",
      "Epoch 2, Step 250: Loss = 1.8217\n",
      "Epoch 2, Step 260: Loss = 0.2010\n",
      "Epoch 2, Step 270: Loss = 0.6527\n",
      "Epoch 2, Step 280: Loss = 1.1501\n",
      "Epoch 2, Step 290: Loss = 0.5998\n",
      "Epoch 2, Step 300: Loss = 1.2534\n",
      "Epoch 2, Step 310: Loss = 1.8897\n",
      "Epoch 2, Step 320: Loss = 2.6453\n",
      "Epoch 2, Step 330: Loss = 0.5888\n",
      "Epoch 2, Step 340: Loss = 0.4801\n",
      "Epoch 2, Step 350: Loss = 1.1502\n",
      "Epoch 2, Step 360: Loss = 0.9050\n",
      "Epoch 2, Step 370: Loss = 1.5946\n",
      "Epoch 2, Step 380: Loss = 1.2206\n",
      "Epoch 2, Step 390: Loss = 0.7813\n",
      "Epoch 2, Step 400: Loss = 0.7455\n",
      "Epoch 2, Step 410: Loss = 1.2088\n",
      "Epoch 2, Step 420: Loss = 0.8940\n",
      "Epoch 2, Step 430: Loss = 0.4070\n",
      "Epoch 2, Step 440: Loss = 0.4142\n",
      "Epoch 2, Step 450: Loss = 1.6635\n",
      "Epoch 2, Step 460: Loss = 0.0786\n",
      "Epoch 2, Step 470: Loss = 1.5981\n",
      "Epoch 2, Step 480: Loss = 0.6618\n",
      "Epoch 2, Step 490: Loss = 1.3377\n",
      "Epoch 2, Step 500: Loss = 0.7839\n",
      "Epoch 2, Step 510: Loss = 1.5443\n",
      "Epoch 2, Step 520: Loss = 0.6802\n",
      "Epoch 2, Step 530: Loss = 0.4387\n",
      "Epoch 2, Step 540: Loss = 1.0820\n",
      "Epoch 2, Step 550: Loss = 1.3270\n",
      "Epoch 2, Step 560: Loss = 0.4884\n",
      "Epoch 2, Step 570: Loss = 0.6835\n",
      "Epoch 2, Step 580: Loss = 1.1849\n",
      "Epoch 2, Step 590: Loss = 0.4227\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Average Training Loss: 1.8619\n",
      "Validation Metrics:\n",
      "Val_loss: 1.1044\n",
      "Epoch 3, Step 0: Loss = 0.6396\n",
      "Epoch 3, Step 10: Loss = 0.5712\n",
      "Epoch 3, Step 20: Loss = 0.6196\n",
      "Epoch 3, Step 30: Loss = 0.4337\n",
      "Epoch 3, Step 40: Loss = 1.4837\n",
      "Epoch 3, Step 50: Loss = 0.3308\n",
      "Epoch 3, Step 60: Loss = 0.5112\n",
      "Epoch 3, Step 70: Loss = 0.2270\n",
      "Epoch 3, Step 80: Loss = 0.8425\n",
      "Epoch 3, Step 90: Loss = 0.2867\n",
      "Epoch 3, Step 100: Loss = 0.3506\n",
      "Epoch 3, Step 110: Loss = 0.2182\n",
      "Epoch 3, Step 120: Loss = 0.4001\n",
      "Epoch 3, Step 130: Loss = 0.3922\n",
      "Epoch 3, Step 140: Loss = 0.9363\n",
      "Epoch 3, Step 150: Loss = 0.6389\n",
      "Epoch 3, Step 160: Loss = 0.2850\n",
      "Epoch 3, Step 170: Loss = 0.6991\n",
      "Epoch 3, Step 180: Loss = 0.1495\n",
      "Epoch 3, Step 190: Loss = 0.5073\n",
      "Epoch 3, Step 200: Loss = 0.6098\n",
      "Epoch 3, Step 210: Loss = 0.1289\n",
      "Epoch 3, Step 220: Loss = 0.0925\n",
      "Epoch 3, Step 230: Loss = 1.0627\n",
      "Epoch 3, Step 240: Loss = 0.6884\n",
      "Epoch 3, Step 250: Loss = 1.0280\n",
      "Epoch 3, Step 260: Loss = 0.5729\n",
      "Epoch 3, Step 270: Loss = 1.1118\n",
      "Epoch 3, Step 280: Loss = 0.7294\n",
      "Epoch 3, Step 290: Loss = 0.5514\n",
      "Epoch 3, Step 300: Loss = 0.1823\n",
      "Epoch 3, Step 310: Loss = 0.7814\n",
      "Epoch 3, Step 320: Loss = 0.3614\n",
      "Epoch 3, Step 330: Loss = 0.9871\n",
      "Epoch 3, Step 340: Loss = 0.7747\n",
      "Epoch 3, Step 350: Loss = 0.4904\n",
      "Epoch 3, Step 360: Loss = 0.2352\n",
      "Epoch 3, Step 370: Loss = 0.1504\n",
      "Epoch 3, Step 380: Loss = 0.3928\n",
      "Epoch 3, Step 390: Loss = 1.2080\n",
      "Epoch 3, Step 400: Loss = 1.1992\n",
      "Epoch 3, Step 410: Loss = 1.1718\n",
      "Epoch 3, Step 420: Loss = 0.0000\n",
      "Epoch 3, Step 430: Loss = 0.4557\n",
      "Epoch 3, Step 440: Loss = 1.0386\n",
      "Epoch 3, Step 450: Loss = 0.0410\n",
      "Epoch 3, Step 460: Loss = 0.2895\n",
      "Epoch 3, Step 470: Loss = 0.0139\n",
      "Epoch 3, Step 480: Loss = 0.4686\n",
      "Epoch 3, Step 490: Loss = 0.0000\n",
      "Epoch 3, Step 500: Loss = 0.3768\n",
      "Epoch 3, Step 510: Loss = 0.8906\n",
      "Epoch 3, Step 520: Loss = 0.6975\n",
      "Epoch 3, Step 530: Loss = 0.4802\n",
      "Epoch 3, Step 540: Loss = 0.1721\n",
      "Epoch 3, Step 550: Loss = 0.3627\n",
      "Epoch 3, Step 560: Loss = 1.0104\n",
      "Epoch 3, Step 570: Loss = 0.3312\n",
      "Epoch 3, Step 580: Loss = 0.0041\n",
      "Epoch 3, Step 590: Loss = 0.5195\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Average Training Loss: 1.0247\n",
      "Validation Metrics:\n",
      "Val_loss: 0.6895\n",
      "Epoch 4, Step 0: Loss = 0.2524\n",
      "Epoch 4, Step 10: Loss = 1.0963\n",
      "Epoch 4, Step 20: Loss = 0.9610\n",
      "Epoch 4, Step 30: Loss = 0.0019\n",
      "Epoch 4, Step 40: Loss = 0.7183\n",
      "Epoch 4, Step 50: Loss = 0.3322\n",
      "Epoch 4, Step 60: Loss = 0.4613\n",
      "Epoch 4, Step 70: Loss = 1.5433\n",
      "Epoch 4, Step 80: Loss = 0.0838\n",
      "Epoch 4, Step 90: Loss = 0.2865\n",
      "Epoch 4, Step 100: Loss = 0.3725\n",
      "Epoch 4, Step 110: Loss = 0.4076\n",
      "Epoch 4, Step 120: Loss = 0.5628\n",
      "Epoch 4, Step 130: Loss = 0.1350\n",
      "Epoch 4, Step 140: Loss = 0.0355\n",
      "Epoch 4, Step 150: Loss = 0.9775\n",
      "Epoch 4, Step 160: Loss = 0.2997\n",
      "Epoch 4, Step 170: Loss = 0.7717\n",
      "Epoch 4, Step 180: Loss = 0.0630\n",
      "Epoch 4, Step 190: Loss = 0.0226\n",
      "Epoch 4, Step 200: Loss = 0.2938\n",
      "Epoch 4, Step 210: Loss = 0.0000\n",
      "Epoch 4, Step 220: Loss = 0.1996\n",
      "Epoch 4, Step 230: Loss = 0.2913\n",
      "Epoch 4, Step 240: Loss = 0.2921\n",
      "Epoch 4, Step 250: Loss = 0.0757\n",
      "Epoch 4, Step 260: Loss = 1.2211\n",
      "Epoch 4, Step 270: Loss = 0.3794\n",
      "Epoch 4, Step 280: Loss = 0.2072\n",
      "Epoch 4, Step 290: Loss = 0.1438\n",
      "Epoch 4, Step 300: Loss = 0.1623\n",
      "Epoch 4, Step 310: Loss = 0.3425\n",
      "Epoch 4, Step 320: Loss = 0.1794\n",
      "Epoch 4, Step 330: Loss = 0.0000\n",
      "Epoch 4, Step 340: Loss = 0.1695\n",
      "Epoch 4, Step 350: Loss = 0.2571\n",
      "Epoch 4, Step 360: Loss = 0.9145\n",
      "Epoch 4, Step 370: Loss = 0.2428\n",
      "Epoch 4, Step 380: Loss = 0.0175\n",
      "Epoch 4, Step 390: Loss = 0.2801\n",
      "Epoch 4, Step 400: Loss = 0.2731\n",
      "Epoch 4, Step 410: Loss = 0.1843\n",
      "Epoch 4, Step 420: Loss = 0.3266\n",
      "Epoch 4, Step 430: Loss = 0.0290\n",
      "Epoch 4, Step 440: Loss = 0.0437\n",
      "Epoch 4, Step 450: Loss = 0.5408\n",
      "Epoch 4, Step 460: Loss = 0.1214\n",
      "Epoch 4, Step 470: Loss = 0.4053\n",
      "Epoch 4, Step 480: Loss = 0.9665\n",
      "Epoch 4, Step 490: Loss = 1.4118\n",
      "Epoch 4, Step 500: Loss = 0.0332\n",
      "Epoch 4, Step 510: Loss = 2.3536\n",
      "Epoch 4, Step 520: Loss = 0.7147\n",
      "Epoch 4, Step 530: Loss = 0.0000\n",
      "Epoch 4, Step 540: Loss = 0.0288\n",
      "Epoch 4, Step 550: Loss = 0.0000\n",
      "Epoch 4, Step 560: Loss = 0.2582\n",
      "Epoch 4, Step 570: Loss = 0.1106\n",
      "Epoch 4, Step 580: Loss = 0.0649\n",
      "Epoch 4, Step 590: Loss = 0.1902\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Average Training Loss: 0.7814\n",
      "Validation Metrics:\n",
      "Val_loss: 0.5349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.1285\n",
      "Epoch 5, Step 10: Loss = 0.0773\n",
      "Epoch 5, Step 20: Loss = 0.5819\n",
      "Epoch 5, Step 30: Loss = 0.0733\n",
      "Epoch 5, Step 40: Loss = 0.1115\n",
      "Epoch 5, Step 50: Loss = 0.5313\n",
      "Epoch 5, Step 60: Loss = 0.8941\n",
      "Epoch 5, Step 70: Loss = 0.8116\n",
      "Epoch 5, Step 80: Loss = 0.3450\n",
      "Epoch 5, Step 90: Loss = 0.0555\n",
      "Epoch 5, Step 100: Loss = 0.1123\n",
      "Epoch 5, Step 110: Loss = 0.1539\n",
      "Epoch 5, Step 120: Loss = 0.3463\n",
      "Epoch 5, Step 130: Loss = 0.0611\n",
      "Epoch 5, Step 140: Loss = 0.0116\n",
      "Epoch 5, Step 150: Loss = 0.6575\n",
      "Epoch 5, Step 160: Loss = 0.8190\n",
      "Epoch 5, Step 170: Loss = 0.3793\n",
      "Epoch 5, Step 180: Loss = 0.0110\n",
      "Epoch 5, Step 190: Loss = 1.1273\n",
      "Epoch 5, Step 200: Loss = 0.0000\n",
      "Epoch 5, Step 210: Loss = 0.0000\n",
      "Epoch 5, Step 220: Loss = 0.2307\n",
      "Epoch 5, Step 230: Loss = 0.1032\n",
      "Epoch 5, Step 240: Loss = 0.3041\n",
      "Epoch 5, Step 250: Loss = 0.0000\n",
      "Epoch 5, Step 260: Loss = 1.2399\n",
      "Epoch 5, Step 270: Loss = 0.6720\n",
      "Epoch 5, Step 280: Loss = 1.0245\n",
      "Epoch 5, Step 290: Loss = 0.4175\n",
      "Epoch 5, Step 300: Loss = 0.7202\n",
      "Epoch 5, Step 310: Loss = 0.0000\n",
      "Epoch 5, Step 320: Loss = 0.0000\n",
      "Epoch 5, Step 330: Loss = 0.3806\n",
      "Epoch 5, Step 340: Loss = 0.5418\n",
      "Epoch 5, Step 350: Loss = 0.7828\n",
      "Epoch 5, Step 360: Loss = 0.1088\n",
      "Epoch 5, Step 370: Loss = 0.1668\n",
      "Epoch 5, Step 380: Loss = 0.2689\n",
      "Epoch 5, Step 390: Loss = 0.1167\n",
      "Epoch 5, Step 400: Loss = 0.6076\n",
      "Epoch 5, Step 410: Loss = 0.2677\n",
      "Epoch 5, Step 420: Loss = 0.4666\n",
      "Epoch 5, Step 430: Loss = 0.8020\n",
      "Epoch 5, Step 440: Loss = 0.0945\n",
      "Epoch 5, Step 450: Loss = 0.4543\n",
      "Epoch 5, Step 460: Loss = 0.2667\n",
      "Epoch 5, Step 470: Loss = 0.9816\n",
      "Epoch 5, Step 480: Loss = 0.0000\n",
      "Epoch 5, Step 490: Loss = 0.0343\n",
      "Epoch 5, Step 500: Loss = 0.1271\n",
      "Epoch 5, Step 510: Loss = 1.4097\n",
      "Epoch 5, Step 520: Loss = 0.3367\n",
      "Epoch 5, Step 530: Loss = 0.4109\n",
      "Epoch 5, Step 540: Loss = 0.5142\n",
      "Epoch 5, Step 550: Loss = 0.5371\n",
      "Epoch 5, Step 560: Loss = 0.2027\n",
      "Epoch 5, Step 570: Loss = 0.1007\n",
      "Epoch 5, Step 580: Loss = 0.3320\n",
      "Epoch 5, Step 590: Loss = 0.3395\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Average Training Loss: 0.6890\n",
      "Validation Metrics:\n",
      "Val_loss: 0.4669\n",
      "Epoch 6, Step 0: Loss = 0.1122\n",
      "Epoch 6, Step 10: Loss = 0.6695\n",
      "Epoch 6, Step 20: Loss = 0.1568\n",
      "Epoch 6, Step 30: Loss = 1.1036\n",
      "Epoch 6, Step 40: Loss = 0.0838\n",
      "Epoch 6, Step 50: Loss = 0.4236\n",
      "Epoch 6, Step 60: Loss = 0.0000\n",
      "Epoch 6, Step 70: Loss = 0.0000\n",
      "Epoch 6, Step 80: Loss = 0.2051\n",
      "Epoch 6, Step 90: Loss = 0.0000\n",
      "Epoch 6, Step 100: Loss = 0.8886\n",
      "Epoch 6, Step 110: Loss = 0.3993\n",
      "Epoch 6, Step 120: Loss = 0.1428\n",
      "Epoch 6, Step 130: Loss = 0.1189\n",
      "Epoch 6, Step 140: Loss = 0.1707\n",
      "Epoch 6, Step 150: Loss = 0.1090\n",
      "Epoch 6, Step 160: Loss = 0.4090\n",
      "Epoch 6, Step 170: Loss = 0.2404\n",
      "Epoch 6, Step 180: Loss = 0.2871\n",
      "Epoch 6, Step 190: Loss = 0.4342\n",
      "Epoch 6, Step 200: Loss = 0.0000\n",
      "Epoch 6, Step 210: Loss = 0.1502\n",
      "Epoch 6, Step 220: Loss = 0.8219\n",
      "Epoch 6, Step 230: Loss = 0.7929\n",
      "Epoch 6, Step 240: Loss = 0.4243\n",
      "Epoch 6, Step 250: Loss = 0.0000\n",
      "Epoch 6, Step 260: Loss = 0.5521\n",
      "Epoch 6, Step 270: Loss = 0.1506\n",
      "Epoch 6, Step 280: Loss = 0.0328\n",
      "Epoch 6, Step 290: Loss = 0.0000\n",
      "Epoch 6, Step 300: Loss = 0.0000\n",
      "Epoch 6, Step 310: Loss = 0.2202\n",
      "Epoch 6, Step 320: Loss = 0.3846\n",
      "Epoch 6, Step 330: Loss = 0.4955\n",
      "Epoch 6, Step 340: Loss = 0.2797\n",
      "Epoch 6, Step 350: Loss = 0.1134\n",
      "Epoch 6, Step 360: Loss = 0.6703\n",
      "Epoch 6, Step 370: Loss = 0.2682\n",
      "Epoch 6, Step 380: Loss = 0.0000\n",
      "Epoch 6, Step 390: Loss = 0.3369\n",
      "Epoch 6, Step 400: Loss = 0.4067\n",
      "Epoch 6, Step 410: Loss = 0.3889\n",
      "Epoch 6, Step 420: Loss = 0.0000\n",
      "Epoch 6, Step 430: Loss = 0.1847\n",
      "Epoch 6, Step 440: Loss = 0.0000\n",
      "Epoch 6, Step 450: Loss = 0.2866\n",
      "Epoch 6, Step 460: Loss = 0.7139\n",
      "Epoch 6, Step 470: Loss = 0.3413\n",
      "Epoch 6, Step 480: Loss = 0.0000\n",
      "Epoch 6, Step 490: Loss = 0.2559\n",
      "Epoch 6, Step 500: Loss = 0.5635\n",
      "Epoch 6, Step 510: Loss = 0.0000\n",
      "Epoch 6, Step 520: Loss = 0.2167\n",
      "Epoch 6, Step 530: Loss = 0.6788\n",
      "Epoch 6, Step 540: Loss = 0.0003\n",
      "Epoch 6, Step 550: Loss = 0.4249\n",
      "Epoch 6, Step 560: Loss = 0.1343\n",
      "Epoch 6, Step 570: Loss = 0.1449\n",
      "Epoch 6, Step 580: Loss = 0.0589\n",
      "Epoch 6, Step 590: Loss = 0.0586\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Average Training Loss: 0.5956\n",
      "Validation Metrics:\n",
      "Val_loss: 0.4382\n",
      "Epoch 7, Step 0: Loss = 0.2349\n",
      "Epoch 7, Step 10: Loss = 0.0652\n",
      "Epoch 7, Step 20: Loss = 0.3290\n",
      "Epoch 7, Step 30: Loss = 0.0567\n",
      "Epoch 7, Step 40: Loss = 0.0000\n",
      "Epoch 7, Step 50: Loss = 0.0620\n",
      "Epoch 7, Step 60: Loss = 0.0042\n",
      "Epoch 7, Step 70: Loss = 0.0777\n",
      "Epoch 7, Step 80: Loss = 0.0000\n",
      "Epoch 7, Step 90: Loss = 0.1398\n",
      "Epoch 7, Step 100: Loss = 0.0000\n",
      "Epoch 7, Step 110: Loss = 0.1076\n",
      "Epoch 7, Step 120: Loss = 1.0842\n",
      "Epoch 7, Step 130: Loss = 0.3571\n",
      "Epoch 7, Step 140: Loss = 0.0701\n",
      "Epoch 7, Step 150: Loss = 0.3828\n",
      "Epoch 7, Step 160: Loss = 0.3661\n",
      "Epoch 7, Step 170: Loss = 0.3467\n",
      "Epoch 7, Step 180: Loss = 0.1454\n",
      "Epoch 7, Step 190: Loss = 0.0000\n",
      "Epoch 7, Step 200: Loss = 0.7107\n",
      "Epoch 7, Step 210: Loss = 0.3133\n",
      "Epoch 7, Step 220: Loss = 0.8634\n",
      "Epoch 7, Step 230: Loss = 0.9289\n",
      "Epoch 7, Step 240: Loss = 0.9874\n",
      "Epoch 7, Step 250: Loss = 0.2109\n",
      "Epoch 7, Step 260: Loss = 0.5828\n",
      "Epoch 7, Step 270: Loss = 0.0000\n",
      "Epoch 7, Step 280: Loss = 0.0292\n",
      "Epoch 7, Step 290: Loss = 0.0307\n",
      "Epoch 7, Step 300: Loss = 0.3175\n",
      "Epoch 7, Step 310: Loss = 0.4318\n",
      "Epoch 7, Step 320: Loss = 0.0279\n",
      "Epoch 7, Step 330: Loss = 0.3678\n",
      "Epoch 7, Step 340: Loss = 0.0004\n",
      "Epoch 7, Step 350: Loss = 0.0000\n",
      "Epoch 7, Step 360: Loss = 0.1572\n",
      "Epoch 7, Step 370: Loss = 0.0000\n",
      "Epoch 7, Step 380: Loss = 0.3850\n",
      "Epoch 7, Step 390: Loss = 0.0560\n",
      "Epoch 7, Step 400: Loss = 0.0000\n",
      "Epoch 7, Step 410: Loss = 1.3472\n",
      "Epoch 7, Step 420: Loss = 0.1036\n",
      "Epoch 7, Step 430: Loss = 0.1614\n",
      "Epoch 7, Step 440: Loss = 0.6017\n",
      "Epoch 7, Step 450: Loss = 0.5163\n",
      "Epoch 7, Step 460: Loss = 0.2967\n",
      "Epoch 7, Step 470: Loss = 0.1591\n",
      "Epoch 7, Step 480: Loss = 0.2000\n",
      "Epoch 7, Step 490: Loss = 0.0751\n",
      "Epoch 7, Step 500: Loss = 0.0573\n",
      "Epoch 7, Step 510: Loss = 0.2529\n",
      "Epoch 7, Step 520: Loss = 0.1232\n",
      "Epoch 7, Step 530: Loss = 0.0000\n",
      "Epoch 7, Step 540: Loss = 0.2699\n",
      "Epoch 7, Step 550: Loss = 0.1651\n",
      "Epoch 7, Step 560: Loss = 1.3093\n",
      "Epoch 7, Step 570: Loss = 0.2902\n",
      "Epoch 7, Step 580: Loss = 0.3883\n",
      "Epoch 7, Step 590: Loss = 0.4286\n",
      "\n",
      "Epoch 7 Summary:\n",
      "Average Training Loss: 0.5440\n",
      "Validation Metrics:\n",
      "Val_loss: 0.4980\n",
      "Epoch 8, Step 0: Loss = 0.3724\n",
      "Epoch 8, Step 10: Loss = 0.1058\n",
      "Epoch 8, Step 20: Loss = 0.4150\n",
      "Epoch 8, Step 30: Loss = 0.2185\n",
      "Epoch 8, Step 40: Loss = 0.4851\n",
      "Epoch 8, Step 50: Loss = 0.0894\n",
      "Epoch 8, Step 60: Loss = 0.1119\n",
      "Epoch 8, Step 70: Loss = 1.5424\n",
      "Epoch 8, Step 80: Loss = 0.3158\n",
      "Epoch 8, Step 90: Loss = 0.6270\n",
      "Epoch 8, Step 100: Loss = 0.2735\n",
      "Epoch 8, Step 110: Loss = 0.1422\n",
      "Epoch 8, Step 120: Loss = 0.0369\n",
      "Epoch 8, Step 130: Loss = 0.0946\n",
      "Epoch 8, Step 140: Loss = 0.0000\n",
      "Epoch 8, Step 150: Loss = 0.9177\n",
      "Epoch 8, Step 160: Loss = 0.0129\n",
      "Epoch 8, Step 170: Loss = 0.1112\n",
      "Epoch 8, Step 180: Loss = 0.0000\n",
      "Epoch 8, Step 190: Loss = 0.0373\n",
      "Epoch 8, Step 200: Loss = 0.1057\n",
      "Epoch 8, Step 210: Loss = 0.5536\n",
      "Epoch 8, Step 220: Loss = 0.7177\n",
      "Epoch 8, Step 230: Loss = 0.1801\n",
      "Epoch 8, Step 240: Loss = 0.1829\n",
      "Epoch 8, Step 250: Loss = 0.0000\n",
      "Epoch 8, Step 260: Loss = 0.3480\n",
      "Epoch 8, Step 270: Loss = 0.5406\n",
      "Epoch 8, Step 280: Loss = 1.6584\n",
      "Epoch 8, Step 290: Loss = 0.0000\n",
      "Epoch 8, Step 300: Loss = 0.0000\n",
      "Epoch 8, Step 310: Loss = 0.4251\n",
      "Epoch 8, Step 320: Loss = 0.6672\n",
      "Epoch 8, Step 330: Loss = 0.4825\n",
      "Epoch 8, Step 340: Loss = 0.3842\n",
      "Epoch 8, Step 350: Loss = 0.2339\n",
      "Epoch 8, Step 360: Loss = 0.1528\n",
      "Epoch 8, Step 370: Loss = 0.0000\n",
      "Epoch 8, Step 380: Loss = 0.1220\n",
      "Epoch 8, Step 390: Loss = 0.8455\n",
      "Epoch 8, Step 400: Loss = 0.0000\n",
      "Epoch 8, Step 410: Loss = 0.7672\n",
      "Epoch 8, Step 420: Loss = 0.3414\n",
      "Epoch 8, Step 430: Loss = 0.5237\n",
      "Epoch 8, Step 440: Loss = 0.0000\n",
      "Epoch 8, Step 450: Loss = 0.0098\n",
      "Epoch 8, Step 460: Loss = 0.5614\n",
      "Epoch 8, Step 470: Loss = 0.3426\n",
      "Epoch 8, Step 480: Loss = 0.2664\n",
      "Epoch 8, Step 490: Loss = 0.7374\n",
      "Epoch 8, Step 500: Loss = 0.1505\n",
      "Epoch 8, Step 510: Loss = 0.5465\n",
      "Epoch 8, Step 520: Loss = 0.0000\n",
      "Epoch 8, Step 530: Loss = 0.1890\n",
      "Epoch 8, Step 540: Loss = 0.0000\n",
      "Epoch 8, Step 550: Loss = 0.0385\n",
      "Epoch 8, Step 560: Loss = 0.2672\n",
      "Epoch 8, Step 570: Loss = 0.0979\n",
      "Epoch 8, Step 580: Loss = 0.0446\n",
      "Epoch 8, Step 590: Loss = 0.2539\n",
      "\n",
      "Epoch 8 Summary:\n",
      "Average Training Loss: 0.5116\n",
      "Validation Metrics:\n",
      "Val_loss: 0.4061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 0: Loss = 0.6492\n",
      "Epoch 9, Step 10: Loss = 0.0000\n",
      "Epoch 9, Step 20: Loss = 0.3952\n",
      "Epoch 9, Step 30: Loss = 0.5238\n",
      "Epoch 9, Step 40: Loss = 0.5307\n",
      "Epoch 9, Step 50: Loss = 0.4734\n",
      "Epoch 9, Step 60: Loss = 0.4899\n",
      "Epoch 9, Step 70: Loss = 0.0751\n",
      "Epoch 9, Step 80: Loss = 0.2115\n",
      "Epoch 9, Step 90: Loss = 0.0000\n",
      "Epoch 9, Step 100: Loss = 0.0000\n",
      "Epoch 9, Step 110: Loss = 0.0858\n",
      "Epoch 9, Step 120: Loss = 0.2784\n",
      "Epoch 9, Step 130: Loss = 0.1853\n",
      "Epoch 9, Step 140: Loss = 0.0447\n",
      "Epoch 9, Step 150: Loss = 0.0000\n",
      "Epoch 9, Step 160: Loss = 0.0303\n",
      "Epoch 9, Step 170: Loss = 0.0987\n",
      "Epoch 9, Step 180: Loss = 0.0676\n",
      "Epoch 9, Step 190: Loss = 0.2087\n",
      "Epoch 9, Step 200: Loss = 0.1384\n",
      "Epoch 9, Step 210: Loss = 0.4276\n",
      "Epoch 9, Step 220: Loss = 1.7658\n",
      "Epoch 9, Step 230: Loss = 0.0000\n",
      "Epoch 9, Step 240: Loss = 0.2245\n",
      "Epoch 9, Step 250: Loss = 0.0700\n",
      "Epoch 9, Step 260: Loss = 0.0093\n",
      "Epoch 9, Step 270: Loss = 0.0519\n",
      "Epoch 9, Step 280: Loss = 0.0000\n",
      "Epoch 9, Step 290: Loss = 0.0000\n",
      "Epoch 9, Step 300: Loss = 0.0000\n",
      "Epoch 9, Step 310: Loss = 0.0000\n",
      "Epoch 9, Step 320: Loss = 0.2290\n",
      "Epoch 9, Step 330: Loss = 0.0577\n",
      "Epoch 9, Step 340: Loss = 0.4868\n",
      "Epoch 9, Step 350: Loss = 0.6756\n",
      "Epoch 9, Step 360: Loss = 0.2246\n",
      "Epoch 9, Step 370: Loss = 0.2939\n",
      "Epoch 9, Step 380: Loss = 0.1477\n",
      "Epoch 9, Step 390: Loss = 0.1171\n",
      "Epoch 9, Step 400: Loss = 0.1563\n",
      "Epoch 9, Step 410: Loss = 0.0992\n",
      "Epoch 9, Step 420: Loss = 0.0000\n",
      "Epoch 9, Step 430: Loss = 2.1123\n",
      "Epoch 9, Step 440: Loss = 0.0000\n",
      "Epoch 9, Step 450: Loss = 0.0000\n",
      "Epoch 9, Step 460: Loss = 0.3303\n",
      "Epoch 9, Step 470: Loss = 0.3177\n",
      "Epoch 9, Step 480: Loss = 0.0240\n",
      "Epoch 9, Step 490: Loss = 0.3069\n",
      "Epoch 9, Step 500: Loss = 0.0000\n",
      "Epoch 9, Step 510: Loss = 0.0005\n",
      "Epoch 9, Step 520: Loss = 0.2545\n",
      "Epoch 9, Step 530: Loss = 0.0302\n",
      "Epoch 9, Step 540: Loss = 0.1907\n",
      "Epoch 9, Step 550: Loss = 0.1062\n",
      "Epoch 9, Step 560: Loss = 0.3208\n",
      "Epoch 9, Step 570: Loss = 0.0989\n",
      "Epoch 9, Step 580: Loss = 0.0000\n",
      "Epoch 9, Step 590: Loss = 0.2567\n",
      "\n",
      "Epoch 9 Summary:\n",
      "Average Training Loss: 0.4843\n",
      "Validation Metrics:\n",
      "Val_loss: 0.4283\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    torch.cuda.set_device(0)  \n",
    "    return torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataset class with data cleaning\n",
    "class ContrastiveEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        # Clean each text field\n",
    "        emails_df['sender'] = emails_df['sender'].apply(clean_text)\n",
    "        emails_df['subject'] = emails_df['subject'].apply(clean_text)\n",
    "        emails_df['body'] = emails_df['body'].apply(clean_text)\n",
    "        \n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.ham_indices = self.emails_df[self.emails_df['label'] == 0].index.tolist()\n",
    "        self.phish_indices = self.emails_df[self.emails_df['label'] == 1].index.tolist()\n",
    "        if not (self.ham_indices and self.phish_indices):\n",
    "            raise ValueError(\"Dataset must contain examples of both classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def _get_random_email_idx(self, label):\n",
    "        indices = self.ham_indices if label == 0 else self.phish_indices\n",
    "        return np.random.choice(indices)\n",
    "\n",
    "    def _prepare_email_input(self, email):\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        tokenized_output = self.tokenizer(input_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        return tokenized_output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_email = self.emails_df.iloc[idx]\n",
    "        anchor_label = anchor_email['label']\n",
    "        positives, negatives = [], []\n",
    "\n",
    "        for _ in range(3):\n",
    "            pos_idx = self._get_random_email_idx(anchor_label)\n",
    "            neg_idx = self._get_random_email_idx(1 - anchor_label)\n",
    "            positives.append(self._prepare_email_input(self.emails_df.iloc[pos_idx]))\n",
    "            negatives.append(self._prepare_email_input(self.emails_df.iloc[neg_idx]))\n",
    "\n",
    "        anchor_inputs = self._prepare_email_input(anchor_email)\n",
    "        return {\n",
    "            'anchor_input_ids': anchor_inputs['input_ids'].squeeze(),\n",
    "            'anchor_attention_mask': anchor_inputs['attention_mask'].squeeze(),\n",
    "            'positive_input_ids': torch.stack([p['input_ids'].squeeze() for p in positives]),\n",
    "            'positive_attention_mask': torch.stack([p['attention_mask'].squeeze() for p in positives]),\n",
    "            'negative_input_ids': torch.stack([n['input_ids'].squeeze() for n in negatives]),\n",
    "            'negative_attention_mask': torch.stack([n['attention_mask'].squeeze() for n in negatives])\n",
    "        }\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, LlamaForSequenceClassification,AutoModelForSequenceClassification\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    # Use the fast tokenizer to avoid the slow/legacy mismatch\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)  \n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Load model config\n",
    "    model_config = AutoConfig.from_pretrained(model_name, num_labels=2, pad_token_id=tokenizer.pad_token_id, use_cache=False)\n",
    "\n",
    "    # Quantize base model\n",
    "    quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,             # if needed for this checkpoint\n",
    "    )\n",
    "\n",
    "    # Attach LoRA\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "    )\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9, margin=1.0, accumulation_steps=2):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    model = model.to(device).to(torch.float16)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                # Compute embeddings for anchor, positive, and negative samples in parallel\n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                # Reshape positive and negative embeddings to match the batch structure (batch_size, 3, embedding_dim)\n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "                # Calculate the triplet loss for each triplet in the batch\n",
    "                loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "                loss /= accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        val_metrics = evaluate_model(model, val_loader, device, margin)\n",
    "        print_metrics(epoch, total_loss, len(train_loader), val_metrics)\n",
    "\n",
    "        if val_metrics['val_loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['val_loss']\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model(model, val_loader, device, margin=1.0):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "               \n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                \n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "               \n",
    "                val_loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "    return {'val_loss': avg_val_loss}\n",
    "\n",
    "\n",
    "# Main function\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  \n",
    "        text = \"\"\n",
    "    \n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def print_metrics(epoch, total_loss, num_batches, val_metrics):\n",
    "    \n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "def main():\n",
    "    login(token=\"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\")\n",
    "    device = setup_environment()\n",
    "    model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/newdata_cleaned.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "\n",
    "   \n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "    train_dataset = ContrastiveEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = ContrastiveEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "    best_model_state = train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/contrastive_7B_Mistral\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d570ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b60d3309904d9f9eefc6e694abdffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ffbf59103247c985db7d70841a068a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 0.6146\n",
      "Epoch 1, Step 10: Loss = 0.6401\n",
      "Epoch 1, Step 20: Loss = 0.6859\n",
      "Epoch 1, Step 30: Loss = 0.5828\n",
      "Epoch 1, Step 40: Loss = 0.5446\n",
      "Epoch 1, Step 50: Loss = 0.4075\n",
      "Epoch 1, Step 60: Loss = 0.5262\n",
      "Epoch 1, Step 70: Loss = 0.6076\n",
      "Epoch 1, Step 80: Loss = 0.5714\n",
      "Epoch 1, Step 90: Loss = 0.7249\n",
      "Epoch 1, Step 100: Loss = 0.5793\n",
      "Epoch 1, Step 110: Loss = 0.4526\n",
      "Epoch 1, Step 120: Loss = 0.5871\n",
      "Epoch 1, Step 130: Loss = 0.6251\n",
      "Epoch 1, Step 140: Loss = 0.4585\n",
      "Epoch 1, Step 150: Loss = 0.6158\n",
      "Epoch 1, Step 160: Loss = 0.6849\n",
      "Epoch 1, Step 170: Loss = 0.5393\n",
      "Epoch 1, Step 180: Loss = 0.5804\n",
      "Epoch 1, Step 190: Loss = 0.5369\n",
      "Epoch 1, Step 200: Loss = 0.7754\n",
      "Epoch 1, Step 210: Loss = 0.7759\n",
      "Epoch 1, Step 220: Loss = 0.4564\n",
      "Epoch 1, Step 230: Loss = 0.4169\n",
      "Epoch 1, Step 240: Loss = 0.4820\n",
      "Epoch 1, Step 250: Loss = 0.4652\n",
      "Epoch 1, Step 260: Loss = 0.6481\n",
      "Epoch 1, Step 270: Loss = 0.4827\n",
      "Epoch 1, Step 280: Loss = 0.5719\n",
      "Epoch 1, Step 290: Loss = 0.4441\n",
      "Epoch 1, Step 300: Loss = 0.5893\n",
      "Epoch 1, Step 310: Loss = 0.3880\n",
      "Epoch 1, Step 320: Loss = 0.4910\n",
      "Epoch 1, Step 330: Loss = 0.5883\n",
      "Epoch 1, Step 340: Loss = 0.5293\n",
      "Epoch 1, Step 350: Loss = 0.6621\n",
      "Epoch 1, Step 360: Loss = 0.3369\n",
      "Epoch 1, Step 370: Loss = 0.3592\n",
      "Epoch 1, Step 380: Loss = 0.5558\n",
      "Epoch 1, Step 390: Loss = 0.3289\n",
      "Epoch 1, Step 400: Loss = 0.4202\n",
      "Epoch 1, Step 410: Loss = 0.5664\n",
      "Epoch 1, Step 420: Loss = 0.4306\n",
      "Epoch 1, Step 430: Loss = 0.3551\n",
      "Epoch 1, Step 440: Loss = 0.5924\n",
      "Epoch 1, Step 450: Loss = 0.5106\n",
      "Epoch 1, Step 460: Loss = 0.4740\n",
      "Epoch 1, Step 470: Loss = 0.3440\n",
      "Epoch 1, Step 480: Loss = 0.5268\n",
      "Epoch 1, Step 490: Loss = 0.6541\n",
      "Epoch 1, Step 500: Loss = 0.5724\n",
      "Epoch 1, Step 510: Loss = 0.3737\n",
      "Epoch 1, Step 520: Loss = 0.5983\n",
      "Epoch 1, Step 530: Loss = 0.4531\n",
      "Epoch 1, Step 540: Loss = 0.3600\n",
      "Epoch 1, Step 550: Loss = 0.3558\n",
      "Epoch 1, Step 560: Loss = 0.2956\n",
      "Epoch 1, Step 570: Loss = 0.6005\n",
      "Epoch 1, Step 580: Loss = 0.5148\n",
      "Epoch 1, Step 590: Loss = 0.3816\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Average Training Loss: 1.0213\n",
      "Validation Metrics:\n",
      "Val_loss: 0.7908\n",
      "Epoch 2, Step 0: Loss = 0.3725\n",
      "Epoch 2, Step 10: Loss = 0.2383\n",
      "Epoch 2, Step 20: Loss = 0.4883\n",
      "Epoch 2, Step 30: Loss = 0.3055\n",
      "Epoch 2, Step 40: Loss = 0.4029\n",
      "Epoch 2, Step 50: Loss = 0.4642\n",
      "Epoch 2, Step 60: Loss = 0.3044\n",
      "Epoch 2, Step 70: Loss = 0.3177\n",
      "Epoch 2, Step 80: Loss = 0.4958\n",
      "Epoch 2, Step 90: Loss = 0.0946\n",
      "Epoch 2, Step 100: Loss = 0.5758\n",
      "Epoch 2, Step 110: Loss = 0.3885\n",
      "Epoch 2, Step 120: Loss = 0.6787\n",
      "Epoch 2, Step 130: Loss = 0.2547\n",
      "Epoch 2, Step 140: Loss = 0.3633\n",
      "Epoch 2, Step 150: Loss = 0.2603\n",
      "Epoch 2, Step 160: Loss = 0.5576\n",
      "Epoch 2, Step 170: Loss = 0.2706\n",
      "Epoch 2, Step 180: Loss = 0.5215\n",
      "Epoch 2, Step 190: Loss = 0.4163\n",
      "Epoch 2, Step 200: Loss = 0.2944\n",
      "Epoch 2, Step 210: Loss = 0.2521\n",
      "Epoch 2, Step 220: Loss = 0.4411\n",
      "Epoch 2, Step 230: Loss = 0.1814\n",
      "Epoch 2, Step 240: Loss = 0.3281\n",
      "Epoch 2, Step 250: Loss = 0.5152\n",
      "Epoch 2, Step 260: Loss = 0.0983\n",
      "Epoch 2, Step 270: Loss = 0.2878\n",
      "Epoch 2, Step 280: Loss = 0.5258\n",
      "Epoch 2, Step 290: Loss = 0.3337\n",
      "Epoch 2, Step 300: Loss = 0.2164\n",
      "Epoch 2, Step 310: Loss = 0.1032\n",
      "Epoch 2, Step 320: Loss = 0.2176\n",
      "Epoch 2, Step 330: Loss = 0.4866\n",
      "Epoch 2, Step 340: Loss = 0.2393\n",
      "Epoch 2, Step 350: Loss = 0.3812\n",
      "Epoch 2, Step 360: Loss = 0.2725\n",
      "Epoch 2, Step 370: Loss = 0.2828\n",
      "Epoch 2, Step 380: Loss = 0.3712\n",
      "Epoch 2, Step 390: Loss = 0.1183\n",
      "Epoch 2, Step 400: Loss = 0.2621\n",
      "Epoch 2, Step 410: Loss = 0.1403\n",
      "Epoch 2, Step 420: Loss = 0.2358\n",
      "Epoch 2, Step 430: Loss = 0.3084\n",
      "Epoch 2, Step 440: Loss = 0.2400\n",
      "Epoch 2, Step 450: Loss = 0.1240\n",
      "Epoch 2, Step 460: Loss = 0.4018\n",
      "Epoch 2, Step 470: Loss = 0.3287\n",
      "Epoch 2, Step 480: Loss = 0.2234\n",
      "Epoch 2, Step 490: Loss = 0.1189\n",
      "Epoch 2, Step 500: Loss = 0.1861\n",
      "Epoch 2, Step 510: Loss = 0.1326\n",
      "Epoch 2, Step 520: Loss = 0.0900\n",
      "Epoch 2, Step 530: Loss = 0.1514\n",
      "Epoch 2, Step 540: Loss = 0.1179\n",
      "Epoch 2, Step 550: Loss = 0.4301\n",
      "Epoch 2, Step 560: Loss = 0.1335\n",
      "Epoch 2, Step 570: Loss = 0.1201\n",
      "Epoch 2, Step 580: Loss = 0.2681\n",
      "Epoch 2, Step 590: Loss = 0.1681\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Average Training Loss: 0.5817\n",
      "Validation Metrics:\n",
      "Val_loss: 0.3987\n",
      "Epoch 3, Step 0: Loss = 0.1475\n",
      "Epoch 3, Step 10: Loss = 0.1798\n",
      "Epoch 3, Step 20: Loss = 0.1089\n",
      "Epoch 3, Step 30: Loss = 0.1985\n",
      "Epoch 3, Step 40: Loss = 0.2682\n",
      "Epoch 3, Step 50: Loss = 0.2308\n",
      "Epoch 3, Step 60: Loss = 0.0667\n",
      "Epoch 3, Step 70: Loss = 0.0568\n",
      "Epoch 3, Step 80: Loss = 0.1791\n",
      "Epoch 3, Step 90: Loss = 0.4318\n",
      "Epoch 3, Step 100: Loss = 0.3523\n",
      "Epoch 3, Step 110: Loss = 0.2780\n",
      "Epoch 3, Step 120: Loss = 0.2907\n",
      "Epoch 3, Step 130: Loss = 0.1179\n",
      "Epoch 3, Step 140: Loss = 0.3708\n",
      "Epoch 3, Step 150: Loss = 0.4301\n",
      "Epoch 3, Step 160: Loss = 0.1924\n",
      "Epoch 3, Step 170: Loss = 0.2318\n",
      "Epoch 3, Step 180: Loss = 0.0423\n",
      "Epoch 3, Step 190: Loss = 0.1357\n",
      "Epoch 3, Step 200: Loss = 0.0678\n",
      "Epoch 3, Step 210: Loss = 0.2011\n",
      "Epoch 3, Step 220: Loss = 0.1647\n",
      "Epoch 3, Step 230: Loss = 0.3480\n",
      "Epoch 3, Step 240: Loss = 0.1418\n",
      "Epoch 3, Step 250: Loss = 0.2097\n",
      "Epoch 3, Step 260: Loss = 0.1238\n",
      "Epoch 3, Step 270: Loss = 0.2920\n",
      "Epoch 3, Step 280: Loss = 0.0967\n",
      "Epoch 3, Step 290: Loss = 0.0417\n",
      "Epoch 3, Step 300: Loss = 0.2216\n",
      "Epoch 3, Step 310: Loss = 0.1765\n",
      "Epoch 3, Step 320: Loss = 0.1241\n",
      "Epoch 3, Step 330: Loss = 0.1981\n",
      "Epoch 3, Step 340: Loss = 0.0731\n",
      "Epoch 3, Step 350: Loss = 0.1944\n",
      "Epoch 3, Step 360: Loss = 0.2169\n",
      "Epoch 3, Step 370: Loss = 0.1709\n",
      "Epoch 3, Step 380: Loss = 0.2104\n",
      "Epoch 3, Step 390: Loss = 0.0714\n",
      "Epoch 3, Step 400: Loss = 0.3085\n",
      "Epoch 3, Step 410: Loss = 0.0203\n",
      "Epoch 3, Step 420: Loss = 0.2490\n",
      "Epoch 3, Step 430: Loss = 0.1751\n",
      "Epoch 3, Step 440: Loss = 0.3530\n",
      "Epoch 3, Step 450: Loss = 0.3999\n",
      "Epoch 3, Step 460: Loss = 0.1440\n",
      "Epoch 3, Step 470: Loss = 0.1348\n",
      "Epoch 3, Step 480: Loss = 0.3111\n",
      "Epoch 3, Step 490: Loss = 0.1107\n",
      "Epoch 3, Step 500: Loss = 0.2030\n",
      "Epoch 3, Step 510: Loss = 0.1275\n",
      "Epoch 3, Step 520: Loss = 0.0544\n",
      "Epoch 3, Step 530: Loss = 0.2489\n",
      "Epoch 3, Step 540: Loss = 0.3376\n",
      "Epoch 3, Step 550: Loss = 0.0227\n",
      "Epoch 3, Step 560: Loss = 0.0678\n",
      "Epoch 3, Step 570: Loss = 0.2073\n",
      "Epoch 3, Step 580: Loss = 0.0696\n",
      "Epoch 3, Step 590: Loss = 0.2857\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Average Training Loss: 0.3579\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2652\n",
      "Epoch 4, Step 0: Loss = 0.0468\n",
      "Epoch 4, Step 10: Loss = 0.0208\n",
      "Epoch 4, Step 20: Loss = 0.1507\n",
      "Epoch 4, Step 30: Loss = 0.0009\n",
      "Epoch 4, Step 40: Loss = 0.1772\n",
      "Epoch 4, Step 50: Loss = 0.1729\n",
      "Epoch 4, Step 60: Loss = 0.0068\n",
      "Epoch 4, Step 70: Loss = 0.1250\n",
      "Epoch 4, Step 80: Loss = 0.0435\n",
      "Epoch 4, Step 90: Loss = 0.1451\n",
      "Epoch 4, Step 100: Loss = 0.2338\n",
      "Epoch 4, Step 110: Loss = 0.1166\n",
      "Epoch 4, Step 120: Loss = 0.1018\n",
      "Epoch 4, Step 130: Loss = 0.3617\n",
      "Epoch 4, Step 140: Loss = 0.1048\n",
      "Epoch 4, Step 150: Loss = 0.1275\n",
      "Epoch 4, Step 160: Loss = 0.1516\n",
      "Epoch 4, Step 170: Loss = 0.3848\n",
      "Epoch 4, Step 180: Loss = 0.4012\n",
      "Epoch 4, Step 190: Loss = 0.0369\n",
      "Epoch 4, Step 200: Loss = 0.1518\n",
      "Epoch 4, Step 210: Loss = 0.1996\n",
      "Epoch 4, Step 220: Loss = 0.0803\n",
      "Epoch 4, Step 230: Loss = 0.1194\n",
      "Epoch 4, Step 240: Loss = 0.0196\n",
      "Epoch 4, Step 250: Loss = 0.1373\n",
      "Epoch 4, Step 260: Loss = 0.4596\n",
      "Epoch 4, Step 270: Loss = 0.1433\n",
      "Epoch 4, Step 280: Loss = 0.1414\n",
      "Epoch 4, Step 290: Loss = 0.1271\n",
      "Epoch 4, Step 300: Loss = 0.0703\n",
      "Epoch 4, Step 310: Loss = 0.1278\n",
      "Epoch 4, Step 320: Loss = 0.0355\n",
      "Epoch 4, Step 330: Loss = 0.0841\n",
      "Epoch 4, Step 340: Loss = 0.0681\n",
      "Epoch 4, Step 350: Loss = 0.1423\n",
      "Epoch 4, Step 360: Loss = 0.3041\n",
      "Epoch 4, Step 370: Loss = 0.0364\n",
      "Epoch 4, Step 380: Loss = 0.1694\n",
      "Epoch 4, Step 390: Loss = 0.1426\n",
      "Epoch 4, Step 400: Loss = 0.1048\n",
      "Epoch 4, Step 410: Loss = 0.0998\n",
      "Epoch 4, Step 420: Loss = 0.0279\n",
      "Epoch 4, Step 430: Loss = 0.0654\n",
      "Epoch 4, Step 440: Loss = 0.1346\n",
      "Epoch 4, Step 450: Loss = 0.2622\n",
      "Epoch 4, Step 460: Loss = 0.2962\n",
      "Epoch 4, Step 470: Loss = 0.0700\n",
      "Epoch 4, Step 480: Loss = 0.1689\n",
      "Epoch 4, Step 490: Loss = 0.1771\n",
      "Epoch 4, Step 500: Loss = 0.1321\n",
      "Epoch 4, Step 510: Loss = 0.1756\n",
      "Epoch 4, Step 520: Loss = 0.1522\n",
      "Epoch 4, Step 530: Loss = 0.1849\n",
      "Epoch 4, Step 540: Loss = 0.1590\n",
      "Epoch 4, Step 550: Loss = 0.0288\n",
      "Epoch 4, Step 560: Loss = 0.3974\n",
      "Epoch 4, Step 570: Loss = 0.4115\n",
      "Epoch 4, Step 580: Loss = 0.1110\n",
      "Epoch 4, Step 590: Loss = 0.0342\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Average Training Loss: 0.2762\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.1508\n",
      "Epoch 5, Step 10: Loss = 0.1403\n",
      "Epoch 5, Step 20: Loss = 0.0471\n",
      "Epoch 5, Step 30: Loss = 0.6500\n",
      "Epoch 5, Step 40: Loss = 0.0494\n",
      "Epoch 5, Step 50: Loss = 0.3249\n",
      "Epoch 5, Step 60: Loss = 0.2425\n",
      "Epoch 5, Step 70: Loss = 0.1295\n",
      "Epoch 5, Step 80: Loss = 0.1309\n",
      "Epoch 5, Step 90: Loss = 0.2391\n",
      "Epoch 5, Step 100: Loss = 0.0538\n",
      "Epoch 5, Step 110: Loss = 0.0595\n",
      "Epoch 5, Step 120: Loss = 0.0888\n",
      "Epoch 5, Step 130: Loss = 0.2564\n",
      "Epoch 5, Step 140: Loss = 0.0168\n",
      "Epoch 5, Step 150: Loss = 0.0542\n",
      "Epoch 5, Step 160: Loss = 0.1780\n",
      "Epoch 5, Step 170: Loss = 0.0925\n",
      "Epoch 5, Step 180: Loss = 0.1510\n",
      "Epoch 5, Step 190: Loss = 0.1251\n",
      "Epoch 5, Step 200: Loss = 0.0153\n",
      "Epoch 5, Step 210: Loss = 0.0743\n",
      "Epoch 5, Step 220: Loss = 0.1483\n",
      "Epoch 5, Step 230: Loss = 0.2424\n",
      "Epoch 5, Step 240: Loss = 0.1060\n",
      "Epoch 5, Step 250: Loss = 0.1605\n",
      "Epoch 5, Step 260: Loss = 0.0524\n",
      "Epoch 5, Step 270: Loss = 0.0539\n",
      "Epoch 5, Step 280: Loss = 0.1283\n",
      "Epoch 5, Step 290: Loss = 0.0063\n",
      "Epoch 5, Step 300: Loss = 0.1037\n",
      "Epoch 5, Step 310: Loss = 0.0322\n",
      "Epoch 5, Step 320: Loss = 0.0745\n",
      "Epoch 5, Step 330: Loss = 0.1189\n",
      "Epoch 5, Step 340: Loss = 0.0770\n",
      "Epoch 5, Step 350: Loss = 0.1169\n",
      "Epoch 5, Step 360: Loss = 0.0918\n",
      "Epoch 5, Step 370: Loss = 0.0175\n",
      "Epoch 5, Step 380: Loss = 0.0812\n",
      "Epoch 5, Step 390: Loss = 0.0493\n",
      "Epoch 5, Step 400: Loss = 0.3812\n",
      "Epoch 5, Step 410: Loss = 0.0593\n",
      "Epoch 5, Step 420: Loss = 0.1930\n",
      "Epoch 5, Step 430: Loss = 0.0252\n",
      "Epoch 5, Step 440: Loss = 0.0000\n",
      "Epoch 5, Step 450: Loss = 0.0000\n",
      "Epoch 5, Step 460: Loss = 0.2275\n",
      "Epoch 5, Step 470: Loss = 0.1733\n",
      "Epoch 5, Step 480: Loss = 0.2723\n",
      "Epoch 5, Step 490: Loss = 0.0561\n",
      "Epoch 5, Step 500: Loss = 0.2718\n",
      "Epoch 5, Step 510: Loss = 0.0857\n",
      "Epoch 5, Step 520: Loss = 0.1236\n",
      "Epoch 5, Step 530: Loss = 0.0037\n",
      "Epoch 5, Step 540: Loss = 0.0718\n",
      "Epoch 5, Step 550: Loss = 0.1307\n",
      "Epoch 5, Step 560: Loss = 0.0886\n",
      "Epoch 5, Step 570: Loss = 0.0810\n",
      "Epoch 5, Step 580: Loss = 0.1151\n",
      "Epoch 5, Step 590: Loss = 0.0206\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Average Training Loss: 0.2347\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2075\n",
      "Epoch 6, Step 0: Loss = 0.0561\n",
      "Epoch 6, Step 10: Loss = 0.3721\n",
      "Epoch 6, Step 20: Loss = 0.3911\n",
      "Epoch 6, Step 30: Loss = 0.0735\n",
      "Epoch 6, Step 40: Loss = 0.0383\n",
      "Epoch 6, Step 50: Loss = 0.0521\n",
      "Epoch 6, Step 60: Loss = 0.1190\n",
      "Epoch 6, Step 70: Loss = 0.0425\n",
      "Epoch 6, Step 80: Loss = 0.0295\n",
      "Epoch 6, Step 90: Loss = 0.0804\n",
      "Epoch 6, Step 100: Loss = 0.0903\n",
      "Epoch 6, Step 110: Loss = 0.1500\n",
      "Epoch 6, Step 120: Loss = 0.0437\n",
      "Epoch 6, Step 130: Loss = 0.0000\n",
      "Epoch 6, Step 140: Loss = 0.0831\n",
      "Epoch 6, Step 150: Loss = 0.0246\n",
      "Epoch 6, Step 160: Loss = 0.0568\n",
      "Epoch 6, Step 170: Loss = 0.0572\n",
      "Epoch 6, Step 180: Loss = 0.0097\n",
      "Epoch 6, Step 190: Loss = 0.0933\n",
      "Epoch 6, Step 200: Loss = 0.2970\n",
      "Epoch 6, Step 210: Loss = 0.1294\n",
      "Epoch 6, Step 220: Loss = 0.0685\n",
      "Epoch 6, Step 230: Loss = 0.0290\n",
      "Epoch 6, Step 240: Loss = 0.0190\n",
      "Epoch 6, Step 250: Loss = 0.1790\n",
      "Epoch 6, Step 260: Loss = 0.0187\n",
      "Epoch 6, Step 270: Loss = 0.1921\n",
      "Epoch 6, Step 280: Loss = 0.1369\n",
      "Epoch 6, Step 290: Loss = 0.2276\n",
      "Epoch 6, Step 300: Loss = 0.0583\n",
      "Epoch 6, Step 310: Loss = 0.3159\n",
      "Epoch 6, Step 320: Loss = 0.1769\n",
      "Epoch 6, Step 330: Loss = 0.0035\n",
      "Epoch 6, Step 340: Loss = 0.1398\n",
      "Epoch 6, Step 350: Loss = 0.1101\n",
      "Epoch 6, Step 360: Loss = 0.0587\n",
      "Epoch 6, Step 370: Loss = 0.0000\n",
      "Epoch 6, Step 380: Loss = 0.0192\n",
      "Epoch 6, Step 390: Loss = 0.1635\n",
      "Epoch 6, Step 400: Loss = 0.1845\n",
      "Epoch 6, Step 410: Loss = 0.3453\n",
      "Epoch 6, Step 420: Loss = 0.1102\n",
      "Epoch 6, Step 430: Loss = 0.1206\n",
      "Epoch 6, Step 440: Loss = 0.0000\n",
      "Epoch 6, Step 450: Loss = 0.1426\n",
      "Epoch 6, Step 460: Loss = 0.1410\n",
      "Epoch 6, Step 470: Loss = 0.1600\n",
      "Epoch 6, Step 480: Loss = 0.3262\n",
      "Epoch 6, Step 490: Loss = 0.0597\n",
      "Epoch 6, Step 500: Loss = 0.1001\n",
      "Epoch 6, Step 510: Loss = 0.2037\n",
      "Epoch 6, Step 520: Loss = 0.0800\n",
      "Epoch 6, Step 530: Loss = 0.2442\n",
      "Epoch 6, Step 540: Loss = 0.0196\n",
      "Epoch 6, Step 550: Loss = 0.0162\n",
      "Epoch 6, Step 560: Loss = 0.2042\n",
      "Epoch 6, Step 570: Loss = 0.1914\n",
      "Epoch 6, Step 580: Loss = 0.0661\n",
      "Epoch 6, Step 590: Loss = 0.1689\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Average Training Loss: 0.2029\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1817\n",
      "Epoch 7, Step 0: Loss = 0.1204\n",
      "Epoch 7, Step 10: Loss = 0.0011\n",
      "Epoch 7, Step 20: Loss = 0.0175\n",
      "Epoch 7, Step 30: Loss = 0.1745\n",
      "Epoch 7, Step 40: Loss = 0.0287\n",
      "Epoch 7, Step 50: Loss = 0.0008\n",
      "Epoch 7, Step 60: Loss = 0.0417\n",
      "Epoch 7, Step 70: Loss = 0.1338\n",
      "Epoch 7, Step 80: Loss = 0.0792\n",
      "Epoch 7, Step 90: Loss = 0.0000\n",
      "Epoch 7, Step 100: Loss = 0.1258\n",
      "Epoch 7, Step 110: Loss = 0.0600\n",
      "Epoch 7, Step 120: Loss = 0.0399\n",
      "Epoch 7, Step 130: Loss = 0.2262\n",
      "Epoch 7, Step 140: Loss = 0.1234\n",
      "Epoch 7, Step 150: Loss = 0.1344\n",
      "Epoch 7, Step 160: Loss = 0.0459\n",
      "Epoch 7, Step 170: Loss = 0.0690\n",
      "Epoch 7, Step 180: Loss = 0.0191\n",
      "Epoch 7, Step 190: Loss = 0.2303\n",
      "Epoch 7, Step 200: Loss = 0.0108\n",
      "Epoch 7, Step 210: Loss = 0.1764\n",
      "Epoch 7, Step 220: Loss = 0.0169\n",
      "Epoch 7, Step 230: Loss = 0.1582\n",
      "Epoch 7, Step 240: Loss = 0.0892\n",
      "Epoch 7, Step 250: Loss = 0.0862\n",
      "Epoch 7, Step 260: Loss = 0.0102\n",
      "Epoch 7, Step 270: Loss = 0.0935\n",
      "Epoch 7, Step 280: Loss = 0.0045\n",
      "Epoch 7, Step 290: Loss = 0.1606\n",
      "Epoch 7, Step 300: Loss = 0.3293\n",
      "Epoch 7, Step 310: Loss = 0.0187\n",
      "Epoch 7, Step 320: Loss = 0.0062\n",
      "Epoch 7, Step 330: Loss = 0.0393\n",
      "Epoch 7, Step 340: Loss = 0.0596\n",
      "Epoch 7, Step 350: Loss = 0.0988\n",
      "Epoch 7, Step 360: Loss = 0.0320\n",
      "Epoch 7, Step 370: Loss = 0.0808\n",
      "Epoch 7, Step 380: Loss = 0.0910\n",
      "Epoch 7, Step 390: Loss = 0.1706\n",
      "Epoch 7, Step 400: Loss = 0.0073\n",
      "Epoch 7, Step 410: Loss = 0.3388\n",
      "Epoch 7, Step 420: Loss = 0.0297\n",
      "Epoch 7, Step 430: Loss = 0.1284\n",
      "Epoch 7, Step 440: Loss = 0.0210\n",
      "Epoch 7, Step 450: Loss = 0.0392\n",
      "Epoch 7, Step 460: Loss = 0.2200\n",
      "Epoch 7, Step 470: Loss = 0.1050\n",
      "Epoch 7, Step 480: Loss = 0.0306\n",
      "Epoch 7, Step 490: Loss = 0.0188\n",
      "Epoch 7, Step 500: Loss = 0.0982\n",
      "Epoch 7, Step 510: Loss = 0.0685\n",
      "Epoch 7, Step 520: Loss = 0.1590\n",
      "Epoch 7, Step 530: Loss = 0.0170\n",
      "Epoch 7, Step 540: Loss = 0.0106\n",
      "Epoch 7, Step 550: Loss = 0.1100\n",
      "Epoch 7, Step 560: Loss = 0.1106\n",
      "Epoch 7, Step 570: Loss = 0.1030\n",
      "Epoch 7, Step 580: Loss = 0.0047\n",
      "Epoch 7, Step 590: Loss = 0.0766\n",
      "\n",
      "Epoch 7 Summary:\n",
      "Average Training Loss: 0.1884\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1746\n",
      "Epoch 8, Step 0: Loss = 0.0625\n",
      "Epoch 8, Step 10: Loss = 0.0764\n",
      "Epoch 8, Step 20: Loss = 0.1275\n",
      "Epoch 8, Step 30: Loss = 0.5310\n",
      "Epoch 8, Step 40: Loss = 0.2236\n",
      "Epoch 8, Step 50: Loss = 0.0411\n",
      "Epoch 8, Step 60: Loss = 0.0216\n",
      "Epoch 8, Step 70: Loss = 0.0728\n",
      "Epoch 8, Step 80: Loss = 0.0023\n",
      "Epoch 8, Step 90: Loss = 0.0357\n",
      "Epoch 8, Step 100: Loss = 0.0199\n",
      "Epoch 8, Step 110: Loss = 0.1932\n",
      "Epoch 8, Step 120: Loss = 0.2701\n",
      "Epoch 8, Step 130: Loss = 0.0956\n",
      "Epoch 8, Step 140: Loss = 0.0447\n",
      "Epoch 8, Step 150: Loss = 0.0042\n",
      "Epoch 8, Step 160: Loss = 0.0500\n",
      "Epoch 8, Step 170: Loss = 0.1070\n",
      "Epoch 8, Step 180: Loss = 0.2423\n",
      "Epoch 8, Step 190: Loss = 0.0415\n",
      "Epoch 8, Step 200: Loss = 0.0000\n",
      "Epoch 8, Step 210: Loss = 0.1378\n",
      "Epoch 8, Step 220: Loss = 0.2949\n",
      "Epoch 8, Step 230: Loss = 0.0879\n",
      "Epoch 8, Step 240: Loss = 0.1072\n",
      "Epoch 8, Step 250: Loss = 0.0421\n",
      "Epoch 8, Step 260: Loss = 0.0394\n",
      "Epoch 8, Step 270: Loss = 0.0126\n",
      "Epoch 8, Step 280: Loss = 0.0015\n",
      "Epoch 8, Step 290: Loss = 0.0199\n",
      "Epoch 8, Step 300: Loss = 0.0913\n",
      "Epoch 8, Step 310: Loss = 0.1049\n",
      "Epoch 8, Step 320: Loss = 0.2532\n",
      "Epoch 8, Step 330: Loss = 0.1487\n",
      "Epoch 8, Step 340: Loss = 0.1547\n",
      "Epoch 8, Step 350: Loss = 0.0000\n",
      "Epoch 8, Step 360: Loss = 0.0659\n",
      "Epoch 8, Step 370: Loss = 0.0071\n",
      "Epoch 8, Step 380: Loss = 0.0127\n",
      "Epoch 8, Step 390: Loss = 0.0297\n",
      "Epoch 8, Step 400: Loss = 0.0582\n",
      "Epoch 8, Step 410: Loss = 0.2312\n",
      "Epoch 8, Step 420: Loss = 0.0303\n",
      "Epoch 8, Step 430: Loss = 0.0000\n",
      "Epoch 8, Step 440: Loss = 0.2797\n",
      "Epoch 8, Step 450: Loss = 0.0137\n",
      "Epoch 8, Step 460: Loss = 0.1366\n",
      "Epoch 8, Step 470: Loss = 0.0322\n",
      "Epoch 8, Step 480: Loss = 0.0847\n",
      "Epoch 8, Step 490: Loss = 0.0177\n",
      "Epoch 8, Step 500: Loss = 0.1685\n",
      "Epoch 8, Step 510: Loss = 0.0146\n",
      "Epoch 8, Step 520: Loss = 0.2628\n",
      "Epoch 8, Step 530: Loss = 0.1134\n",
      "Epoch 8, Step 540: Loss = 0.1820\n",
      "Epoch 8, Step 550: Loss = 0.0611\n",
      "Epoch 8, Step 560: Loss = 0.0627\n",
      "Epoch 8, Step 570: Loss = 0.0957\n",
      "Epoch 8, Step 580: Loss = 0.0670\n",
      "Epoch 8, Step 590: Loss = 0.0972\n",
      "\n",
      "Epoch 8 Summary:\n",
      "Average Training Loss: 0.1683\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 0: Loss = 0.0018\n",
      "Epoch 9, Step 10: Loss = 0.1107\n",
      "Epoch 9, Step 20: Loss = 0.3005\n",
      "Epoch 9, Step 30: Loss = 0.1307\n",
      "Epoch 9, Step 40: Loss = 0.0015\n",
      "Epoch 9, Step 50: Loss = 0.0343\n",
      "Epoch 9, Step 60: Loss = 0.0708\n",
      "Epoch 9, Step 70: Loss = 0.0000\n",
      "Epoch 9, Step 80: Loss = 0.0298\n",
      "Epoch 9, Step 90: Loss = 0.0117\n",
      "Epoch 9, Step 100: Loss = 0.0142\n",
      "Epoch 9, Step 110: Loss = 0.1360\n",
      "Epoch 9, Step 120: Loss = 0.0306\n",
      "Epoch 9, Step 130: Loss = 0.0000\n",
      "Epoch 9, Step 140: Loss = 0.1032\n",
      "Epoch 9, Step 150: Loss = 0.0254\n",
      "Epoch 9, Step 160: Loss = 0.0333\n",
      "Epoch 9, Step 170: Loss = 0.0448\n",
      "Epoch 9, Step 180: Loss = 0.1480\n",
      "Epoch 9, Step 190: Loss = 0.0290\n",
      "Epoch 9, Step 200: Loss = 0.0245\n",
      "Epoch 9, Step 210: Loss = 0.0378\n",
      "Epoch 9, Step 220: Loss = 0.0228\n",
      "Epoch 9, Step 230: Loss = 0.0840\n",
      "Epoch 9, Step 240: Loss = 0.0000\n",
      "Epoch 9, Step 250: Loss = 0.1462\n",
      "Epoch 9, Step 260: Loss = 0.0549\n",
      "Epoch 9, Step 270: Loss = 0.0306\n",
      "Epoch 9, Step 280: Loss = 0.0000\n",
      "Epoch 9, Step 290: Loss = 0.0000\n",
      "Epoch 9, Step 300: Loss = 0.1218\n",
      "Epoch 9, Step 310: Loss = 0.1375\n",
      "Epoch 9, Step 320: Loss = 0.1448\n",
      "Epoch 9, Step 330: Loss = 0.0375\n",
      "Epoch 9, Step 340: Loss = 0.0657\n",
      "Epoch 9, Step 350: Loss = 0.1659\n",
      "Epoch 9, Step 360: Loss = 0.0111\n",
      "Epoch 9, Step 370: Loss = 0.2205\n",
      "Epoch 9, Step 380: Loss = 0.0000\n",
      "Epoch 9, Step 390: Loss = 0.2846\n",
      "Epoch 9, Step 400: Loss = 0.0000\n",
      "Epoch 9, Step 410: Loss = 0.0740\n",
      "Epoch 9, Step 420: Loss = 0.0476\n",
      "Epoch 9, Step 430: Loss = 0.1411\n",
      "Epoch 9, Step 440: Loss = 0.0000\n",
      "Epoch 9, Step 450: Loss = 0.0352\n",
      "Epoch 9, Step 460: Loss = 0.0092\n",
      "Epoch 9, Step 470: Loss = 0.0214\n",
      "Epoch 9, Step 480: Loss = 0.0118\n",
      "Epoch 9, Step 490: Loss = 0.1061\n",
      "Epoch 9, Step 500: Loss = 0.1486\n",
      "Epoch 9, Step 510: Loss = 0.1081\n",
      "Epoch 9, Step 520: Loss = 0.0000\n",
      "Epoch 9, Step 530: Loss = 0.0971\n",
      "Epoch 9, Step 540: Loss = 0.1855\n",
      "Epoch 9, Step 550: Loss = 0.0203\n",
      "Epoch 9, Step 560: Loss = 0.1340\n",
      "Epoch 9, Step 570: Loss = 0.0184\n",
      "Epoch 9, Step 580: Loss = 0.0000\n",
      "Epoch 9, Step 590: Loss = 0.0974\n",
      "\n",
      "Epoch 9 Summary:\n",
      "Average Training Loss: 0.1667\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1630\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    torch.cuda.set_device(0)  \n",
    "    return torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataset class with data cleaning\n",
    "class ContrastiveEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        # Clean each text field\n",
    "        emails_df['sender'] = emails_df['sender'].apply(clean_text)\n",
    "        emails_df['subject'] = emails_df['subject'].apply(clean_text)\n",
    "        emails_df['body'] = emails_df['body'].apply(clean_text)\n",
    "        \n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.ham_indices = self.emails_df[self.emails_df['label'] == 0].index.tolist()\n",
    "        self.phish_indices = self.emails_df[self.emails_df['label'] == 1].index.tolist()\n",
    "        if not (self.ham_indices and self.phish_indices):\n",
    "            raise ValueError(\"Dataset must contain examples of both classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def _get_random_email_idx(self, label):\n",
    "        indices = self.ham_indices if label == 0 else self.phish_indices\n",
    "        return np.random.choice(indices)\n",
    "\n",
    "    def _prepare_email_input(self, email):\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        tokenized_output = self.tokenizer(input_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        return tokenized_output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_email = self.emails_df.iloc[idx]\n",
    "        anchor_label = anchor_email['label']\n",
    "        positives, negatives = [], []\n",
    "\n",
    "        for _ in range(3):\n",
    "            pos_idx = self._get_random_email_idx(anchor_label)\n",
    "            neg_idx = self._get_random_email_idx(1 - anchor_label)\n",
    "            positives.append(self._prepare_email_input(self.emails_df.iloc[pos_idx]))\n",
    "            negatives.append(self._prepare_email_input(self.emails_df.iloc[neg_idx]))\n",
    "\n",
    "        anchor_inputs = self._prepare_email_input(anchor_email)\n",
    "        return {\n",
    "            'anchor_input_ids': anchor_inputs['input_ids'].squeeze(),\n",
    "            'anchor_attention_mask': anchor_inputs['attention_mask'].squeeze(),\n",
    "            'positive_input_ids': torch.stack([p['input_ids'].squeeze() for p in positives]),\n",
    "            'positive_attention_mask': torch.stack([p['attention_mask'].squeeze() for p in positives]),\n",
    "            'negative_input_ids': torch.stack([n['input_ids'].squeeze() for n in negatives]),\n",
    "            'negative_attention_mask': torch.stack([n['attention_mask'].squeeze() for n in negatives])\n",
    "        }\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, LlamaForSequenceClassification,AutoModelForSequenceClassification\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    # Use the fast tokenizer to avoid the slow/legacy mismatch\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)  \n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Load model config\n",
    "    model_config = AutoConfig.from_pretrained(model_name, num_labels=2, pad_token_id=tokenizer.pad_token_id, use_cache=False)\n",
    "\n",
    "    # Quantize base model\n",
    "    quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,             # if needed for this checkpoint\n",
    "    )\n",
    "\n",
    "    # Attach LoRA\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "    )\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9, margin=1.0, accumulation_steps=2):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    model = model.to(device).to(torch.float16)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                # Compute embeddings for anchor, positive, and negative samples in parallel\n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                # Reshape positive and negative embeddings to match the batch structure (batch_size, 3, embedding_dim)\n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "                # Calculate the triplet loss for each triplet in the batch\n",
    "                loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "                loss /= accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        val_metrics = evaluate_model(model, val_loader, device, margin)\n",
    "        print_metrics(epoch, total_loss, len(train_loader), val_metrics)\n",
    "\n",
    "        if val_metrics['val_loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['val_loss']\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model(model, val_loader, device, margin=1.0):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "               \n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                \n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "               \n",
    "                val_loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "    return {'val_loss': avg_val_loss}\n",
    "\n",
    "\n",
    "# Main function\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  \n",
    "        text = \"\"\n",
    "    \n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def print_metrics(epoch, total_loss, num_batches, val_metrics):\n",
    "    \n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "def main():\n",
    "    login(token=\"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\")\n",
    "    device = setup_environment()\n",
    "    model_name ='Qwen/Qwen3-8B'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/newdata_cleaned.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "\n",
    "   \n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "    train_dataset = ContrastiveEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = ContrastiveEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "    best_model_state = train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/contrastive_8B_Qwen\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6b03b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 0.4716\n",
      "Epoch 1, Step 10: Loss = 0.4720\n",
      "Epoch 1, Step 20: Loss = 0.5014\n",
      "Epoch 1, Step 30: Loss = 0.5029\n",
      "Epoch 1, Step 40: Loss = 0.4967\n",
      "Epoch 1, Step 50: Loss = 0.5201\n",
      "Epoch 1, Step 60: Loss = 0.4843\n",
      "Epoch 1, Step 70: Loss = 0.4999\n",
      "Epoch 1, Step 80: Loss = 0.4959\n",
      "Epoch 1, Step 90: Loss = 0.4868\n",
      "Epoch 1, Step 100: Loss = 0.4495\n",
      "Epoch 1, Step 110: Loss = 0.4924\n",
      "Epoch 1, Step 120: Loss = 0.4389\n",
      "Epoch 1, Step 130: Loss = 0.3904\n",
      "Epoch 1, Step 140: Loss = 0.2097\n",
      "Epoch 1, Step 150: Loss = 0.1661\n",
      "Epoch 1, Step 160: Loss = 0.1652\n",
      "Epoch 1, Step 170: Loss = 0.0013\n",
      "Epoch 1, Step 180: Loss = 0.1500\n",
      "Epoch 1, Step 190: Loss = 0.0000\n",
      "Epoch 1, Step 200: Loss = 0.0165\n",
      "Epoch 1, Step 210: Loss = 0.0905\n",
      "Epoch 1, Step 220: Loss = 0.0150\n",
      "Epoch 1, Step 230: Loss = 0.0016\n",
      "Epoch 1, Step 240: Loss = 0.1572\n",
      "Epoch 1, Step 250: Loss = 0.0127\n",
      "Epoch 1, Step 260: Loss = 0.0000\n",
      "Epoch 1, Step 270: Loss = 0.0000\n",
      "Epoch 1, Step 280: Loss = 0.0097\n",
      "Epoch 1, Step 290: Loss = 0.0000\n",
      "Epoch 1, Step 300: Loss = 0.0000\n",
      "Epoch 1, Step 310: Loss = 0.0188\n",
      "Epoch 1, Step 320: Loss = 0.0232\n",
      "Epoch 1, Step 330: Loss = 0.0222\n",
      "Epoch 1, Step 340: Loss = 0.1188\n",
      "Epoch 1, Step 350: Loss = 0.0474\n",
      "Epoch 1, Step 360: Loss = 0.0000\n",
      "Epoch 1, Step 370: Loss = 0.0000\n",
      "Epoch 1, Step 380: Loss = 0.0000\n",
      "Epoch 1, Step 390: Loss = 0.0178\n",
      "Epoch 1, Step 400: Loss = 0.0000\n",
      "Epoch 1, Step 410: Loss = 0.0000\n",
      "Epoch 1, Step 420: Loss = 0.0024\n",
      "Epoch 1, Step 430: Loss = 0.0000\n",
      "Epoch 1, Step 440: Loss = 0.2443\n",
      "Epoch 1, Step 450: Loss = 0.1887\n",
      "Epoch 1, Step 460: Loss = 0.0000\n",
      "Epoch 1, Step 470: Loss = 0.0077\n",
      "Epoch 1, Step 480: Loss = 0.1136\n",
      "Epoch 1, Step 490: Loss = 0.0000\n",
      "Epoch 1, Step 500: Loss = 0.0000\n",
      "Epoch 1, Step 510: Loss = 0.0161\n",
      "Epoch 1, Step 520: Loss = 0.0000\n",
      "Epoch 1, Step 530: Loss = 0.0000\n",
      "Epoch 1, Step 540: Loss = 0.0000\n",
      "Epoch 1, Step 550: Loss = 0.0000\n",
      "Epoch 1, Step 560: Loss = 0.0000\n",
      "Epoch 1, Step 570: Loss = 0.0000\n",
      "Epoch 1, Step 580: Loss = 0.0000\n",
      "Epoch 1, Step 590: Loss = 0.0000\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Average Training Loss: 0.2799\n",
      "Validation Metrics:\n",
      "Val_loss: 0.0259\n",
      "Epoch 2, Step 0: Loss = 0.0000\n",
      "Epoch 2, Step 10: Loss = 0.0145\n",
      "Epoch 2, Step 20: Loss = 0.1322\n",
      "Epoch 2, Step 30: Loss = 0.0000\n",
      "Epoch 2, Step 40: Loss = 0.0000\n",
      "Epoch 2, Step 50: Loss = 0.0000\n",
      "Epoch 2, Step 60: Loss = 0.0000\n",
      "Epoch 2, Step 70: Loss = 0.0000\n",
      "Epoch 2, Step 80: Loss = 0.0000\n",
      "Epoch 2, Step 90: Loss = 0.0000\n",
      "Epoch 2, Step 100: Loss = 0.0000\n",
      "Epoch 2, Step 110: Loss = 0.0000\n",
      "Epoch 2, Step 120: Loss = 0.0000\n",
      "Epoch 2, Step 130: Loss = 0.0000\n",
      "Epoch 2, Step 140: Loss = 0.0000\n",
      "Epoch 2, Step 150: Loss = 0.0000\n",
      "Epoch 2, Step 160: Loss = 0.0000\n",
      "Epoch 2, Step 170: Loss = 0.0000\n",
      "Epoch 2, Step 180: Loss = 0.0000\n",
      "Epoch 2, Step 190: Loss = 0.0421\n",
      "Epoch 2, Step 200: Loss = 0.0000\n",
      "Epoch 2, Step 210: Loss = 0.0000\n",
      "Epoch 2, Step 220: Loss = 0.0000\n",
      "Epoch 2, Step 230: Loss = 0.0000\n",
      "Epoch 2, Step 240: Loss = 0.0000\n",
      "Epoch 2, Step 250: Loss = 0.0000\n",
      "Epoch 2, Step 260: Loss = 0.0000\n",
      "Epoch 2, Step 270: Loss = 0.0000\n",
      "Epoch 2, Step 280: Loss = 0.0000\n",
      "Epoch 2, Step 290: Loss = 0.0000\n",
      "Epoch 2, Step 300: Loss = 0.0000\n",
      "Epoch 2, Step 310: Loss = 0.0000\n",
      "Epoch 2, Step 320: Loss = 0.0000\n",
      "Epoch 2, Step 330: Loss = 0.0000\n",
      "Epoch 2, Step 340: Loss = 0.0000\n",
      "Epoch 2, Step 350: Loss = 0.0000\n",
      "Epoch 2, Step 360: Loss = 0.0000\n",
      "Epoch 2, Step 370: Loss = 0.0000\n",
      "Epoch 2, Step 380: Loss = 0.0000\n",
      "Epoch 2, Step 390: Loss = 0.0000\n",
      "Epoch 2, Step 400: Loss = 0.0967\n",
      "Epoch 2, Step 410: Loss = 0.0000\n",
      "Epoch 2, Step 420: Loss = 0.0000\n",
      "Epoch 2, Step 430: Loss = 0.0000\n",
      "Epoch 2, Step 440: Loss = 0.0000\n",
      "Epoch 2, Step 450: Loss = 0.0000\n",
      "Epoch 2, Step 460: Loss = 0.0000\n",
      "Epoch 2, Step 470: Loss = 0.0000\n",
      "Epoch 2, Step 480: Loss = 0.0000\n",
      "Epoch 2, Step 490: Loss = 0.0000\n",
      "Epoch 2, Step 500: Loss = 0.0000\n",
      "Epoch 2, Step 510: Loss = 0.0000\n",
      "Epoch 2, Step 520: Loss = 0.0000\n",
      "Epoch 2, Step 530: Loss = 0.0000\n",
      "Epoch 2, Step 540: Loss = 0.0000\n",
      "Epoch 2, Step 550: Loss = 0.0000\n",
      "Epoch 2, Step 560: Loss = 0.0000\n",
      "Epoch 2, Step 570: Loss = 0.0000\n",
      "Epoch 2, Step 580: Loss = 0.0000\n",
      "Epoch 2, Step 590: Loss = 0.0000\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Average Training Loss: 0.0110\n",
      "Validation Metrics:\n",
      "Val_loss: 0.0210\n",
      "Epoch 3, Step 0: Loss = 0.0000\n",
      "Epoch 3, Step 10: Loss = 0.0000\n",
      "Epoch 3, Step 20: Loss = 0.0000\n",
      "Epoch 3, Step 30: Loss = 0.0000\n",
      "Epoch 3, Step 40: Loss = 0.0000\n",
      "Epoch 3, Step 50: Loss = 0.0000\n",
      "Epoch 3, Step 60: Loss = 0.0000\n",
      "Epoch 3, Step 70: Loss = 0.0000\n",
      "Epoch 3, Step 80: Loss = 0.0000\n",
      "Epoch 3, Step 90: Loss = 0.0000\n",
      "Epoch 3, Step 100: Loss = 0.0000\n",
      "Epoch 3, Step 110: Loss = 0.0000\n",
      "Epoch 3, Step 120: Loss = 0.0000\n",
      "Epoch 3, Step 130: Loss = 0.0000\n",
      "Epoch 3, Step 140: Loss = 0.0000\n",
      "Epoch 3, Step 150: Loss = 0.0000\n",
      "Epoch 3, Step 160: Loss = 0.0000\n",
      "Epoch 3, Step 170: Loss = 0.0000\n",
      "Epoch 3, Step 180: Loss = 0.0000\n",
      "Epoch 3, Step 190: Loss = 0.0000\n",
      "Epoch 3, Step 200: Loss = 0.0000\n",
      "Epoch 3, Step 210: Loss = 0.0000\n",
      "Epoch 3, Step 220: Loss = 0.0000\n",
      "Epoch 3, Step 230: Loss = 0.0000\n",
      "Epoch 3, Step 240: Loss = 0.0000\n",
      "Epoch 3, Step 250: Loss = 0.0000\n",
      "Epoch 3, Step 260: Loss = 0.0000\n",
      "Epoch 3, Step 270: Loss = 0.0000\n",
      "Epoch 3, Step 280: Loss = 0.0000\n",
      "Epoch 3, Step 290: Loss = 0.0000\n",
      "Epoch 3, Step 300: Loss = 0.0000\n",
      "Epoch 3, Step 310: Loss = 0.0000\n",
      "Epoch 3, Step 320: Loss = 0.0000\n",
      "Epoch 3, Step 330: Loss = 0.0000\n",
      "Epoch 3, Step 340: Loss = 0.0000\n",
      "Epoch 3, Step 350: Loss = 0.0000\n",
      "Epoch 3, Step 360: Loss = 0.0000\n",
      "Epoch 3, Step 370: Loss = 0.0000\n",
      "Epoch 3, Step 380: Loss = 0.0000\n",
      "Epoch 3, Step 390: Loss = 0.0000\n",
      "Epoch 3, Step 400: Loss = 0.0116\n",
      "Epoch 3, Step 410: Loss = 0.0000\n",
      "Epoch 3, Step 420: Loss = 0.0000\n",
      "Epoch 3, Step 430: Loss = 0.0000\n",
      "Epoch 3, Step 440: Loss = 0.0000\n",
      "Epoch 3, Step 450: Loss = 0.0000\n",
      "Epoch 3, Step 460: Loss = 0.0000\n",
      "Epoch 3, Step 470: Loss = 0.0000\n",
      "Epoch 3, Step 480: Loss = 0.0000\n",
      "Epoch 3, Step 490: Loss = 0.0234\n",
      "Epoch 3, Step 500: Loss = 0.0000\n",
      "Epoch 3, Step 510: Loss = 0.0000\n",
      "Epoch 3, Step 520: Loss = 0.0000\n",
      "Epoch 3, Step 530: Loss = 0.0000\n",
      "Epoch 3, Step 540: Loss = 0.0000\n",
      "Epoch 3, Step 550: Loss = 0.0000\n",
      "Epoch 3, Step 560: Loss = 0.0000\n",
      "Epoch 3, Step 570: Loss = 0.0000\n",
      "Epoch 3, Step 580: Loss = 0.0000\n",
      "Epoch 3, Step 590: Loss = 0.0000\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Average Training Loss: 0.0024\n",
      "Validation Metrics:\n",
      "Val_loss: 0.0116\n",
      "Epoch 4, Step 0: Loss = 0.0000\n",
      "Epoch 4, Step 10: Loss = 0.0000\n",
      "Epoch 4, Step 20: Loss = 0.0000\n",
      "Epoch 4, Step 30: Loss = 0.0000\n",
      "Epoch 4, Step 40: Loss = 0.0000\n",
      "Epoch 4, Step 50: Loss = 0.0000\n",
      "Epoch 4, Step 60: Loss = 0.0000\n",
      "Epoch 4, Step 70: Loss = 0.0000\n",
      "Epoch 4, Step 80: Loss = 0.0000\n",
      "Epoch 4, Step 90: Loss = 0.0000\n",
      "Epoch 4, Step 100: Loss = 0.0000\n",
      "Epoch 4, Step 110: Loss = 0.0000\n",
      "Epoch 4, Step 120: Loss = 0.0000\n",
      "Epoch 4, Step 130: Loss = 0.0000\n",
      "Epoch 4, Step 140: Loss = 0.0000\n",
      "Epoch 4, Step 150: Loss = 0.0000\n",
      "Epoch 4, Step 160: Loss = 0.0000\n",
      "Epoch 4, Step 170: Loss = 0.0000\n",
      "Epoch 4, Step 180: Loss = 0.0000\n",
      "Epoch 4, Step 190: Loss = 0.0000\n",
      "Epoch 4, Step 200: Loss = 0.0000\n",
      "Epoch 4, Step 210: Loss = 0.0000\n",
      "Epoch 4, Step 220: Loss = 0.0000\n",
      "Epoch 4, Step 230: Loss = 0.0000\n",
      "Epoch 4, Step 240: Loss = 0.0000\n",
      "Epoch 4, Step 250: Loss = 0.0000\n",
      "Epoch 4, Step 260: Loss = 0.0000\n",
      "Epoch 4, Step 270: Loss = 0.0000\n",
      "Epoch 4, Step 280: Loss = 0.0000\n",
      "Epoch 4, Step 290: Loss = 0.0000\n",
      "Epoch 4, Step 300: Loss = 0.0000\n",
      "Epoch 4, Step 310: Loss = 0.0000\n",
      "Epoch 4, Step 320: Loss = 0.0000\n",
      "Epoch 4, Step 330: Loss = 0.0000\n",
      "Epoch 4, Step 340: Loss = 0.0000\n",
      "Epoch 4, Step 350: Loss = 0.0000\n",
      "Epoch 4, Step 360: Loss = 0.0000\n",
      "Epoch 4, Step 370: Loss = 0.0000\n",
      "Epoch 4, Step 380: Loss = 0.0000\n",
      "Epoch 4, Step 390: Loss = 0.0000\n",
      "Epoch 4, Step 400: Loss = 0.0000\n",
      "Epoch 4, Step 410: Loss = 0.0000\n",
      "Epoch 4, Step 420: Loss = 0.0000\n",
      "Epoch 4, Step 430: Loss = 0.0000\n",
      "Epoch 4, Step 440: Loss = 0.0000\n",
      "Epoch 4, Step 450: Loss = 0.0000\n",
      "Epoch 4, Step 460: Loss = 0.0000\n",
      "Epoch 4, Step 470: Loss = 0.0000\n",
      "Epoch 4, Step 480: Loss = 0.0000\n",
      "Epoch 4, Step 490: Loss = 0.0000\n",
      "Epoch 4, Step 500: Loss = 0.0000\n",
      "Epoch 4, Step 510: Loss = 0.0000\n",
      "Epoch 4, Step 520: Loss = 0.0000\n",
      "Epoch 4, Step 530: Loss = 0.0000\n",
      "Epoch 4, Step 540: Loss = 0.0000\n",
      "Epoch 4, Step 550: Loss = 0.0000\n",
      "Epoch 4, Step 560: Loss = 0.0000\n",
      "Epoch 4, Step 570: Loss = 0.0000\n",
      "Epoch 4, Step 580: Loss = 0.0000\n",
      "Epoch 4, Step 590: Loss = 0.0000\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Average Training Loss: 0.0005\n",
      "Validation Metrics:\n",
      "Val_loss: 0.0119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.0000\n",
      "Epoch 5, Step 10: Loss = 0.0000\n",
      "Epoch 5, Step 20: Loss = 0.0000\n",
      "Epoch 5, Step 30: Loss = 0.0000\n",
      "Epoch 5, Step 40: Loss = 0.0000\n",
      "Epoch 5, Step 50: Loss = 0.0000\n",
      "Epoch 5, Step 60: Loss = 0.0000\n",
      "Epoch 5, Step 70: Loss = 0.0203\n",
      "Epoch 5, Step 80: Loss = 0.0000\n",
      "Epoch 5, Step 90: Loss = 0.0000\n",
      "Epoch 5, Step 100: Loss = 0.0000\n",
      "Epoch 5, Step 110: Loss = 0.0000\n",
      "Epoch 5, Step 120: Loss = 0.0000\n",
      "Epoch 5, Step 130: Loss = 0.0000\n",
      "Epoch 5, Step 140: Loss = 0.0000\n",
      "Epoch 5, Step 150: Loss = 0.0000\n",
      "Epoch 5, Step 160: Loss = 0.0000\n",
      "Epoch 5, Step 170: Loss = 0.0000\n",
      "Epoch 5, Step 180: Loss = 0.0000\n",
      "Epoch 5, Step 190: Loss = 0.0000\n",
      "Epoch 5, Step 200: Loss = 0.0000\n",
      "Epoch 5, Step 210: Loss = 0.0000\n",
      "Epoch 5, Step 220: Loss = 0.0000\n",
      "Epoch 5, Step 230: Loss = 0.0000\n",
      "Epoch 5, Step 240: Loss = 0.0000\n",
      "Epoch 5, Step 250: Loss = 0.0000\n",
      "Epoch 5, Step 260: Loss = 0.0000\n",
      "Epoch 5, Step 270: Loss = 0.0000\n",
      "Epoch 5, Step 280: Loss = 0.0000\n",
      "Epoch 5, Step 290: Loss = 0.0000\n",
      "Epoch 5, Step 300: Loss = 0.0000\n",
      "Epoch 5, Step 310: Loss = 0.0000\n",
      "Epoch 5, Step 320: Loss = 0.0000\n",
      "Epoch 5, Step 330: Loss = 0.0000\n",
      "Epoch 5, Step 340: Loss = 0.0000\n",
      "Epoch 5, Step 350: Loss = 0.0000\n",
      "Epoch 5, Step 360: Loss = 0.0000\n",
      "Epoch 5, Step 370: Loss = 0.0000\n",
      "Epoch 5, Step 380: Loss = 0.0000\n",
      "Epoch 5, Step 390: Loss = 0.0000\n",
      "Epoch 5, Step 400: Loss = 0.0000\n",
      "Epoch 5, Step 410: Loss = 0.0000\n",
      "Epoch 5, Step 420: Loss = 0.0000\n",
      "Epoch 5, Step 430: Loss = 0.0000\n",
      "Epoch 5, Step 440: Loss = 0.0000\n",
      "Epoch 5, Step 450: Loss = 0.0000\n",
      "Epoch 5, Step 460: Loss = 0.0000\n",
      "Epoch 5, Step 470: Loss = 0.0000\n",
      "Epoch 5, Step 480: Loss = 0.0000\n",
      "Epoch 5, Step 490: Loss = 0.0000\n",
      "Epoch 5, Step 500: Loss = 0.0000\n",
      "Epoch 5, Step 510: Loss = 0.0000\n",
      "Epoch 5, Step 520: Loss = 0.0000\n",
      "Epoch 5, Step 530: Loss = 0.0000\n",
      "Epoch 5, Step 540: Loss = 0.0000\n",
      "Epoch 5, Step 550: Loss = 0.0000\n",
      "Epoch 5, Step 560: Loss = 0.0000\n",
      "Epoch 5, Step 570: Loss = 0.0000\n",
      "Epoch 5, Step 580: Loss = 0.0000\n",
      "Epoch 5, Step 590: Loss = 0.0000\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Average Training Loss: 0.0023\n",
      "Validation Metrics:\n",
      "Val_loss: 0.0065\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "# Set environment to use GPU\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '2' \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "# Dataset class with data cleaning\n",
    "class ContrastiveEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        # Clean each text field\n",
    "        emails_df['sender'] = emails_df['sender'].apply(clean_text)\n",
    "        emails_df['subject'] = emails_df['subject'].apply(clean_text)\n",
    "        emails_df['body'] = emails_df['body'].apply(clean_text)\n",
    "        \n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.ham_indices = self.emails_df[self.emails_df['label'] == 0].index.tolist()\n",
    "        self.phish_indices = self.emails_df[self.emails_df['label'] == 1].index.tolist()\n",
    "        if not (self.ham_indices and self.phish_indices):\n",
    "            raise ValueError(\"Dataset must contain examples of both classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def _get_random_email_idx(self, label):\n",
    "        indices = self.ham_indices if label == 0 else self.phish_indices\n",
    "        return np.random.choice(indices)\n",
    "\n",
    "    def _prepare_email_input(self, email):\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        tokenized_output = self.tokenizer(input_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        return tokenized_output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_email = self.emails_df.iloc[idx]\n",
    "        anchor_label = anchor_email['label']\n",
    "        positives, negatives = [], []\n",
    "\n",
    "        for _ in range(3):\n",
    "            pos_idx = self._get_random_email_idx(anchor_label)\n",
    "            neg_idx = self._get_random_email_idx(1 - anchor_label)\n",
    "            positives.append(self._prepare_email_input(self.emails_df.iloc[pos_idx]))\n",
    "            negatives.append(self._prepare_email_input(self.emails_df.iloc[neg_idx]))\n",
    "\n",
    "        anchor_inputs = self._prepare_email_input(anchor_email)\n",
    "        return {\n",
    "            'anchor_input_ids': anchor_inputs['input_ids'].squeeze(),\n",
    "            'anchor_attention_mask': anchor_inputs['attention_mask'].squeeze(),\n",
    "            'positive_input_ids': torch.stack([p['input_ids'].squeeze() for p in positives]),\n",
    "            'positive_attention_mask': torch.stack([p['attention_mask'].squeeze() for p in positives]),\n",
    "            'negative_input_ids': torch.stack([n['input_ids'].squeeze() for n in negatives]),\n",
    "            'negative_attention_mask': torch.stack([n['attention_mask'].squeeze() for n in negatives])\n",
    "        }\n",
    "# Load the model and tokenizer\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Set a default padding token if it doesn't exist\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else tokenizer.unk_token\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Load the BERT model for sequence classification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        config=model_config, \n",
    "        torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=5, margin=1.0, accumulation_steps=2):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    model = model.to(device).to(torch.float32)  # Use full precision initially to debug\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            # Remove the autocast context to stabilize loss calculation\n",
    "            anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "            positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "            negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "            positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "            negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "           \n",
    "            epsilon = 1e-6\n",
    "            loss = F.triplet_margin_loss(\n",
    "                anchor_embeddings.unsqueeze(1).expand(-1, 3, -1) + epsilon,\n",
    "                positive_embeddings + epsilon,\n",
    "                negative_embeddings + epsilon,\n",
    "                margin=margin\n",
    "            )\n",
    "            loss /= accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)  \n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        val_metrics = evaluate_model(model, val_loader, device, margin)\n",
    "        print_metrics(epoch, total_loss, len(train_loader), val_metrics)\n",
    "\n",
    "        if val_metrics['val_loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['val_loss']\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader, device, margin=1.0):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                # Compute embeddings for anchor, positive, and negative samples in parallel\n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                # Reshape positive and negative embeddings to match the batch structure (batch_size, 3, embedding_dim)\n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "                # Calculate the triplet loss for each triplet in the batch\n",
    "                val_loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "    return {'val_loss': avg_val_loss}\n",
    "def print_metrics(epoch, total_loss, num_batches, val_metrics):\n",
    "    \"\"\"\n",
    "    Prints the training and validation metrics for each epoch.\n",
    "    \n",
    "    Parameters:\n",
    "    - epoch (int): Current epoch number.\n",
    "    - total_loss (float): Total training loss for the epoch.\n",
    "    - num_batches (int): Number of batches in training data.\n",
    "    - val_metrics (dict): Dictionary containing validation metrics such as 'val_loss'.\n",
    "    \"\"\"\n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Ensure text is a string\n",
    "        text = \"\"\n",
    " \n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def main():\n",
    "    login(token=\"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\") \n",
    "    device = setup_environment()\n",
    "    model_name = 'bert-base-uncased' \n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/newdata_cleaned.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "\n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "    train_dataset = ContrastiveEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = ContrastiveEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 5\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "    best_model_state = train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/contrastive_bertuncased\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5259b381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d356eae1fb04307afcd449fa1476546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 0.6385\n",
      "Epoch 1, Step 10: Loss = 0.9131\n",
      "Epoch 1, Step 20: Loss = 0.6719\n",
      "Epoch 1, Step 30: Loss = 0.5644\n",
      "Epoch 1, Step 40: Loss = 0.9722\n",
      "Epoch 1, Step 50: Loss = 1.0099\n",
      "Epoch 1, Step 60: Loss = 0.7462\n",
      "Epoch 1, Step 70: Loss = 0.4694\n",
      "Epoch 1, Step 80: Loss = 0.6595\n",
      "Epoch 1, Step 90: Loss = 0.4440\n",
      "Epoch 1, Step 100: Loss = 0.5117\n",
      "Epoch 1, Step 110: Loss = 0.8599\n",
      "Epoch 1, Step 120: Loss = 0.4787\n",
      "Epoch 1, Step 130: Loss = 0.5803\n",
      "Epoch 1, Step 140: Loss = 1.0303\n",
      "Epoch 1, Step 150: Loss = 0.6111\n",
      "Epoch 1, Step 160: Loss = 0.8293\n",
      "Epoch 1, Step 170: Loss = 0.7493\n",
      "Epoch 1, Step 180: Loss = 0.2285\n",
      "Epoch 1, Step 190: Loss = 0.7167\n",
      "Epoch 1, Step 200: Loss = 0.7388\n",
      "Epoch 1, Step 210: Loss = 0.6272\n",
      "Epoch 1, Step 220: Loss = 0.6067\n",
      "Epoch 1, Step 230: Loss = 0.6489\n",
      "Epoch 1, Step 240: Loss = 0.3009\n",
      "Epoch 1, Step 250: Loss = 0.6325\n",
      "Epoch 1, Step 260: Loss = 0.6023\n",
      "Epoch 1, Step 270: Loss = 0.8579\n",
      "Epoch 1, Step 280: Loss = 0.5738\n",
      "Epoch 1, Step 290: Loss = 0.3541\n",
      "Epoch 1, Step 300: Loss = 1.0176\n",
      "Epoch 1, Step 310: Loss = 0.4390\n",
      "Epoch 1, Step 320: Loss = 0.5763\n",
      "Epoch 1, Step 330: Loss = 0.5710\n",
      "Epoch 1, Step 340: Loss = 0.9315\n",
      "Epoch 1, Step 350: Loss = 0.4590\n",
      "Epoch 1, Step 360: Loss = 0.9366\n",
      "Epoch 1, Step 370: Loss = 0.7535\n",
      "Epoch 1, Step 380: Loss = 0.5976\n",
      "Epoch 1, Step 390: Loss = 0.5458\n",
      "Epoch 1, Step 400: Loss = 0.4920\n",
      "Epoch 1, Step 410: Loss = 0.6428\n",
      "Epoch 1, Step 420: Loss = 0.5212\n",
      "Epoch 1, Step 430: Loss = 0.4242\n",
      "Epoch 1, Step 440: Loss = 0.9446\n",
      "Epoch 1, Step 450: Loss = 0.4330\n",
      "Epoch 1, Step 460: Loss = 0.6876\n",
      "Epoch 1, Step 470: Loss = 0.8786\n",
      "Epoch 1, Step 480: Loss = 0.7857\n",
      "Epoch 1, Step 490: Loss = 0.6196\n",
      "Epoch 1, Step 500: Loss = 0.3775\n",
      "Epoch 1, Step 510: Loss = 0.6678\n",
      "Epoch 1, Step 520: Loss = 0.4958\n",
      "Epoch 1, Step 530: Loss = 0.3641\n",
      "Epoch 1, Step 540: Loss = 0.4956\n",
      "Epoch 1, Step 550: Loss = 0.6603\n",
      "Epoch 1, Step 560: Loss = 0.7919\n",
      "Epoch 1, Step 570: Loss = 0.3646\n",
      "Epoch 1, Step 580: Loss = 0.7602\n",
      "Epoch 1, Step 590: Loss = 0.3983\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Average Training Loss: 1.3377\n",
      "Validation Metrics:\n",
      "Val_loss: 0.9547\n",
      "Epoch 2, Step 0: Loss = 0.5215\n",
      "Epoch 2, Step 10: Loss = 0.2333\n",
      "Epoch 2, Step 20: Loss = 0.4823\n",
      "Epoch 2, Step 30: Loss = 0.5598\n",
      "Epoch 2, Step 40: Loss = 0.6503\n",
      "Epoch 2, Step 50: Loss = 0.4868\n",
      "Epoch 2, Step 60: Loss = 0.7421\n",
      "Epoch 2, Step 70: Loss = 0.4623\n",
      "Epoch 2, Step 80: Loss = 0.2584\n",
      "Epoch 2, Step 90: Loss = 0.3612\n",
      "Epoch 2, Step 100: Loss = 0.4085\n",
      "Epoch 2, Step 110: Loss = 0.5252\n",
      "Epoch 2, Step 120: Loss = 0.3380\n",
      "Epoch 2, Step 130: Loss = 0.4127\n",
      "Epoch 2, Step 140: Loss = 0.2210\n",
      "Epoch 2, Step 150: Loss = 0.2453\n",
      "Epoch 2, Step 160: Loss = 0.0198\n",
      "Epoch 2, Step 170: Loss = 0.0543\n",
      "Epoch 2, Step 180: Loss = 0.2986\n",
      "Epoch 2, Step 190: Loss = 0.3238\n",
      "Epoch 2, Step 200: Loss = 0.1932\n",
      "Epoch 2, Step 210: Loss = 0.1951\n",
      "Epoch 2, Step 220: Loss = 0.7204\n",
      "Epoch 2, Step 230: Loss = 0.5108\n",
      "Epoch 2, Step 240: Loss = 0.3604\n",
      "Epoch 2, Step 250: Loss = 0.3645\n",
      "Epoch 2, Step 260: Loss = 0.3065\n",
      "Epoch 2, Step 270: Loss = 0.5337\n",
      "Epoch 2, Step 280: Loss = 0.1683\n",
      "Epoch 2, Step 290: Loss = 0.4832\n",
      "Epoch 2, Step 300: Loss = 0.1696\n",
      "Epoch 2, Step 310: Loss = 0.0588\n",
      "Epoch 2, Step 320: Loss = 0.2795\n",
      "Epoch 2, Step 330: Loss = 0.0372\n",
      "Epoch 2, Step 340: Loss = 0.0293\n",
      "Epoch 2, Step 350: Loss = 0.1185\n",
      "Epoch 2, Step 360: Loss = 0.2663\n",
      "Epoch 2, Step 370: Loss = 0.6052\n",
      "Epoch 2, Step 380: Loss = 0.3908\n",
      "Epoch 2, Step 390: Loss = 0.1747\n",
      "Epoch 2, Step 400: Loss = 0.0876\n",
      "Epoch 2, Step 410: Loss = 0.4094\n",
      "Epoch 2, Step 420: Loss = 0.0311\n",
      "Epoch 2, Step 430: Loss = 0.1831\n",
      "Epoch 2, Step 440: Loss = 0.2494\n",
      "Epoch 2, Step 450: Loss = 0.3807\n",
      "Epoch 2, Step 460: Loss = 0.1194\n",
      "Epoch 2, Step 470: Loss = 0.0512\n",
      "Epoch 2, Step 480: Loss = 0.3177\n",
      "Epoch 2, Step 490: Loss = 0.7571\n",
      "Epoch 2, Step 500: Loss = 0.3358\n",
      "Epoch 2, Step 510: Loss = 0.0285\n",
      "Epoch 2, Step 520: Loss = 0.2997\n",
      "Epoch 2, Step 530: Loss = 0.0227\n",
      "Epoch 2, Step 540: Loss = 0.1101\n",
      "Epoch 2, Step 550: Loss = 0.3229\n",
      "Epoch 2, Step 560: Loss = 0.1890\n",
      "Epoch 2, Step 570: Loss = 0.3665\n",
      "Epoch 2, Step 580: Loss = 0.3756\n",
      "Epoch 2, Step 590: Loss = 0.2104\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Average Training Loss: 0.6633\n",
      "Validation Metrics:\n",
      "Val_loss: 0.4518\n",
      "Epoch 3, Step 0: Loss = 0.6039\n",
      "Epoch 3, Step 10: Loss = 0.0452\n",
      "Epoch 3, Step 20: Loss = 0.0421\n",
      "Epoch 3, Step 30: Loss = 0.4665\n",
      "Epoch 3, Step 40: Loss = 0.1793\n",
      "Epoch 3, Step 50: Loss = 0.0947\n",
      "Epoch 3, Step 60: Loss = 0.0158\n",
      "Epoch 3, Step 70: Loss = 0.1616\n",
      "Epoch 3, Step 80: Loss = 0.0090\n",
      "Epoch 3, Step 90: Loss = 0.0246\n",
      "Epoch 3, Step 100: Loss = 0.1727\n",
      "Epoch 3, Step 110: Loss = 0.1815\n",
      "Epoch 3, Step 120: Loss = 0.0719\n",
      "Epoch 3, Step 130: Loss = 0.4157\n",
      "Epoch 3, Step 140: Loss = 0.0543\n",
      "Epoch 3, Step 150: Loss = 0.1595\n",
      "Epoch 3, Step 160: Loss = 0.1316\n",
      "Epoch 3, Step 170: Loss = 0.1186\n",
      "Epoch 3, Step 180: Loss = 0.3290\n",
      "Epoch 3, Step 190: Loss = 0.0722\n",
      "Epoch 3, Step 200: Loss = 0.7200\n",
      "Epoch 3, Step 210: Loss = 0.0532\n",
      "Epoch 3, Step 220: Loss = 0.3354\n",
      "Epoch 3, Step 230: Loss = 0.2681\n",
      "Epoch 3, Step 240: Loss = 0.4325\n",
      "Epoch 3, Step 250: Loss = 0.3032\n",
      "Epoch 3, Step 260: Loss = 0.3527\n",
      "Epoch 3, Step 270: Loss = 0.0683\n",
      "Epoch 3, Step 280: Loss = 0.2424\n",
      "Epoch 3, Step 290: Loss = 0.0564\n",
      "Epoch 3, Step 300: Loss = 0.1462\n",
      "Epoch 3, Step 310: Loss = 0.6503\n",
      "Epoch 3, Step 320: Loss = 0.2176\n",
      "Epoch 3, Step 330: Loss = 0.2952\n",
      "Epoch 3, Step 340: Loss = 0.2066\n",
      "Epoch 3, Step 350: Loss = 0.4438\n",
      "Epoch 3, Step 360: Loss = 0.3806\n",
      "Epoch 3, Step 370: Loss = 0.1542\n",
      "Epoch 3, Step 380: Loss = 0.0128\n",
      "Epoch 3, Step 390: Loss = 0.2199\n",
      "Epoch 3, Step 400: Loss = 0.1013\n",
      "Epoch 3, Step 410: Loss = 0.4830\n",
      "Epoch 3, Step 420: Loss = 0.2612\n",
      "Epoch 3, Step 430: Loss = 0.3298\n",
      "Epoch 3, Step 440: Loss = 0.3363\n",
      "Epoch 3, Step 450: Loss = 0.1430\n",
      "Epoch 3, Step 460: Loss = 0.0851\n",
      "Epoch 3, Step 470: Loss = 0.0319\n",
      "Epoch 3, Step 480: Loss = 0.5186\n",
      "Epoch 3, Step 490: Loss = 0.1446\n",
      "Epoch 3, Step 500: Loss = 0.0341\n",
      "Epoch 3, Step 510: Loss = 0.1724\n",
      "Epoch 3, Step 520: Loss = 0.1362\n",
      "Epoch 3, Step 530: Loss = 0.1760\n",
      "Epoch 3, Step 540: Loss = 0.1258\n",
      "Epoch 3, Step 550: Loss = 0.1618\n",
      "Epoch 3, Step 560: Loss = 0.1080\n",
      "Epoch 3, Step 570: Loss = 0.2129\n",
      "Epoch 3, Step 580: Loss = 0.2662\n",
      "Epoch 3, Step 590: Loss = 0.1491\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Average Training Loss: 0.4078\n",
      "Validation Metrics:\n",
      "Val_loss: 0.3826\n",
      "Epoch 4, Step 0: Loss = 0.3330\n",
      "Epoch 4, Step 10: Loss = 0.0950\n",
      "Epoch 4, Step 20: Loss = 0.2162\n",
      "Epoch 4, Step 30: Loss = 0.0381\n",
      "Epoch 4, Step 40: Loss = 0.0588\n",
      "Epoch 4, Step 50: Loss = 0.0000\n",
      "Epoch 4, Step 60: Loss = 0.2596\n",
      "Epoch 4, Step 70: Loss = 0.0526\n",
      "Epoch 4, Step 80: Loss = 0.4777\n",
      "Epoch 4, Step 90: Loss = 0.0000\n",
      "Epoch 4, Step 100: Loss = 0.4123\n",
      "Epoch 4, Step 110: Loss = 0.0551\n",
      "Epoch 4, Step 120: Loss = 0.2422\n",
      "Epoch 4, Step 130: Loss = 0.0641\n",
      "Epoch 4, Step 140: Loss = 0.0496\n",
      "Epoch 4, Step 150: Loss = 0.3493\n",
      "Epoch 4, Step 160: Loss = 0.2667\n",
      "Epoch 4, Step 170: Loss = 0.3427\n",
      "Epoch 4, Step 180: Loss = 0.1990\n",
      "Epoch 4, Step 190: Loss = 0.1165\n",
      "Epoch 4, Step 200: Loss = 0.2719\n",
      "Epoch 4, Step 210: Loss = 0.0024\n",
      "Epoch 4, Step 220: Loss = 0.1569\n",
      "Epoch 4, Step 230: Loss = 0.0026\n",
      "Epoch 4, Step 240: Loss = 0.3646\n",
      "Epoch 4, Step 250: Loss = 0.5893\n",
      "Epoch 4, Step 260: Loss = 0.1721\n",
      "Epoch 4, Step 270: Loss = 0.0032\n",
      "Epoch 4, Step 280: Loss = 0.5745\n",
      "Epoch 4, Step 290: Loss = 0.0000\n",
      "Epoch 4, Step 300: Loss = 0.0701\n",
      "Epoch 4, Step 310: Loss = 0.0331\n",
      "Epoch 4, Step 320: Loss = 0.0769\n",
      "Epoch 4, Step 330: Loss = 0.1939\n",
      "Epoch 4, Step 340: Loss = 0.0575\n",
      "Epoch 4, Step 350: Loss = 0.0075\n",
      "Epoch 4, Step 360: Loss = 0.1523\n",
      "Epoch 4, Step 370: Loss = 0.1900\n",
      "Epoch 4, Step 380: Loss = 0.0846\n",
      "Epoch 4, Step 390: Loss = 0.0265\n",
      "Epoch 4, Step 400: Loss = 0.0226\n",
      "Epoch 4, Step 410: Loss = 0.0000\n",
      "Epoch 4, Step 420: Loss = 0.4247\n",
      "Epoch 4, Step 430: Loss = 0.1649\n",
      "Epoch 4, Step 440: Loss = 0.3487\n",
      "Epoch 4, Step 450: Loss = 0.0386\n",
      "Epoch 4, Step 460: Loss = 0.6059\n",
      "Epoch 4, Step 470: Loss = 0.1268\n",
      "Epoch 4, Step 480: Loss = 0.2454\n",
      "Epoch 4, Step 490: Loss = 0.0561\n",
      "Epoch 4, Step 500: Loss = 0.0000\n",
      "Epoch 4, Step 510: Loss = 0.0725\n",
      "Epoch 4, Step 520: Loss = 0.0460\n",
      "Epoch 4, Step 530: Loss = 0.1968\n",
      "Epoch 4, Step 540: Loss = 0.1922\n",
      "Epoch 4, Step 550: Loss = 0.3494\n",
      "Epoch 4, Step 560: Loss = 0.0228\n",
      "Epoch 4, Step 570: Loss = 0.0000\n",
      "Epoch 4, Step 580: Loss = 0.6332\n",
      "Epoch 4, Step 590: Loss = 0.0000\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Average Training Loss: 0.3223\n",
      "Validation Metrics:\n",
      "Val_loss: 0.3357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.0934\n",
      "Epoch 5, Step 10: Loss = 0.4894\n",
      "Epoch 5, Step 20: Loss = 0.1160\n",
      "Epoch 5, Step 30: Loss = 0.2615\n",
      "Epoch 5, Step 40: Loss = 0.0000\n",
      "Epoch 5, Step 50: Loss = 0.0510\n",
      "Epoch 5, Step 60: Loss = 0.0264\n",
      "Epoch 5, Step 70: Loss = 0.0639\n",
      "Epoch 5, Step 80: Loss = 0.0746\n",
      "Epoch 5, Step 90: Loss = 0.0824\n",
      "Epoch 5, Step 100: Loss = 0.1135\n",
      "Epoch 5, Step 110: Loss = 0.0000\n",
      "Epoch 5, Step 120: Loss = 0.0672\n",
      "Epoch 5, Step 130: Loss = 0.2503\n",
      "Epoch 5, Step 140: Loss = 0.1025\n",
      "Epoch 5, Step 150: Loss = 0.1044\n",
      "Epoch 5, Step 160: Loss = 0.3419\n",
      "Epoch 5, Step 170: Loss = 0.0367\n",
      "Epoch 5, Step 180: Loss = 0.0329\n",
      "Epoch 5, Step 190: Loss = 0.0945\n",
      "Epoch 5, Step 200: Loss = 0.4274\n",
      "Epoch 5, Step 210: Loss = 0.0000\n",
      "Epoch 5, Step 220: Loss = 0.0000\n",
      "Epoch 5, Step 230: Loss = 0.0299\n",
      "Epoch 5, Step 240: Loss = 0.0000\n",
      "Epoch 5, Step 250: Loss = 0.0648\n",
      "Epoch 5, Step 260: Loss = 0.0033\n",
      "Epoch 5, Step 270: Loss = 0.1724\n",
      "Epoch 5, Step 280: Loss = 0.0720\n",
      "Epoch 5, Step 290: Loss = 0.0000\n",
      "Epoch 5, Step 300: Loss = 0.0496\n",
      "Epoch 5, Step 310: Loss = 0.1567\n",
      "Epoch 5, Step 320: Loss = 0.0886\n",
      "Epoch 5, Step 330: Loss = 0.0057\n",
      "Epoch 5, Step 340: Loss = 0.0423\n",
      "Epoch 5, Step 350: Loss = 0.1348\n",
      "Epoch 5, Step 360: Loss = 0.4543\n",
      "Epoch 5, Step 370: Loss = 0.1562\n",
      "Epoch 5, Step 380: Loss = 0.0000\n",
      "Epoch 5, Step 390: Loss = 0.0000\n",
      "Epoch 5, Step 400: Loss = 0.0106\n",
      "Epoch 5, Step 410: Loss = 0.1412\n",
      "Epoch 5, Step 420: Loss = 0.0370\n",
      "Epoch 5, Step 430: Loss = 0.1269\n",
      "Epoch 5, Step 440: Loss = 0.5110\n",
      "Epoch 5, Step 450: Loss = 0.1502\n",
      "Epoch 5, Step 460: Loss = 0.0021\n",
      "Epoch 5, Step 470: Loss = 0.2542\n",
      "Epoch 5, Step 480: Loss = 0.0000\n",
      "Epoch 5, Step 490: Loss = 0.0161\n",
      "Epoch 5, Step 500: Loss = 0.0785\n",
      "Epoch 5, Step 510: Loss = 0.1879\n",
      "Epoch 5, Step 520: Loss = 0.0314\n",
      "Epoch 5, Step 530: Loss = 0.3895\n",
      "Epoch 5, Step 540: Loss = 0.3581\n",
      "Epoch 5, Step 550: Loss = 0.4261\n",
      "Epoch 5, Step 560: Loss = 0.0498\n",
      "Epoch 5, Step 570: Loss = 0.1546\n",
      "Epoch 5, Step 580: Loss = 0.0000\n",
      "Epoch 5, Step 590: Loss = 0.0320\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Average Training Loss: 0.2880\n",
      "Validation Metrics:\n",
      "Val_loss: 0.3025\n",
      "Epoch 6, Step 0: Loss = 0.0517\n",
      "Epoch 6, Step 10: Loss = 0.0000\n",
      "Epoch 6, Step 20: Loss = 0.5906\n",
      "Epoch 6, Step 30: Loss = 0.0000\n",
      "Epoch 6, Step 40: Loss = 0.1099\n",
      "Epoch 6, Step 50: Loss = 0.0131\n",
      "Epoch 6, Step 60: Loss = 0.0452\n",
      "Epoch 6, Step 70: Loss = 0.0000\n",
      "Epoch 6, Step 80: Loss = 0.1395\n",
      "Epoch 6, Step 90: Loss = 0.0848\n",
      "Epoch 6, Step 100: Loss = 0.1080\n",
      "Epoch 6, Step 110: Loss = 0.1446\n",
      "Epoch 6, Step 120: Loss = 0.1043\n",
      "Epoch 6, Step 130: Loss = 0.4518\n",
      "Epoch 6, Step 140: Loss = 0.0000\n",
      "Epoch 6, Step 150: Loss = 0.0000\n",
      "Epoch 6, Step 160: Loss = 0.0733\n",
      "Epoch 6, Step 170: Loss = 0.0232\n",
      "Epoch 6, Step 180: Loss = 0.0153\n",
      "Epoch 6, Step 190: Loss = 0.0582\n",
      "Epoch 6, Step 200: Loss = 0.0000\n",
      "Epoch 6, Step 210: Loss = 0.1171\n",
      "Epoch 6, Step 220: Loss = 0.0666\n",
      "Epoch 6, Step 230: Loss = 0.0817\n",
      "Epoch 6, Step 240: Loss = 0.0759\n",
      "Epoch 6, Step 250: Loss = 0.0201\n",
      "Epoch 6, Step 260: Loss = 0.3242\n",
      "Epoch 6, Step 270: Loss = 0.1243\n",
      "Epoch 6, Step 280: Loss = 0.0000\n",
      "Epoch 6, Step 290: Loss = 0.0000\n",
      "Epoch 6, Step 300: Loss = 0.0384\n",
      "Epoch 6, Step 310: Loss = 0.0081\n",
      "Epoch 6, Step 320: Loss = 0.0585\n",
      "Epoch 6, Step 330: Loss = 0.2083\n",
      "Epoch 6, Step 340: Loss = 0.1030\n",
      "Epoch 6, Step 350: Loss = 0.2651\n",
      "Epoch 6, Step 360: Loss = 0.0206\n",
      "Epoch 6, Step 370: Loss = 0.0005\n",
      "Epoch 6, Step 380: Loss = 0.0000\n",
      "Epoch 6, Step 390: Loss = 0.0000\n",
      "Epoch 6, Step 400: Loss = 0.0977\n",
      "Epoch 6, Step 410: Loss = 0.3854\n",
      "Epoch 6, Step 420: Loss = 0.1884\n",
      "Epoch 6, Step 430: Loss = 0.0000\n",
      "Epoch 6, Step 440: Loss = 0.2514\n",
      "Epoch 6, Step 450: Loss = 0.2803\n",
      "Epoch 6, Step 460: Loss = 0.0234\n",
      "Epoch 6, Step 470: Loss = 0.2903\n",
      "Epoch 6, Step 480: Loss = 0.0314\n",
      "Epoch 6, Step 490: Loss = 0.0719\n",
      "Epoch 6, Step 500: Loss = 0.1329\n",
      "Epoch 6, Step 510: Loss = 0.0725\n",
      "Epoch 6, Step 520: Loss = 0.8364\n",
      "Epoch 6, Step 530: Loss = 0.0239\n",
      "Epoch 6, Step 540: Loss = 0.6386\n",
      "Epoch 6, Step 550: Loss = 0.0000\n",
      "Epoch 6, Step 560: Loss = 0.5881\n",
      "Epoch 6, Step 570: Loss = 0.0762\n",
      "Epoch 6, Step 580: Loss = 0.0000\n",
      "Epoch 6, Step 590: Loss = 0.0000\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Average Training Loss: 0.2566\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2855\n",
      "Epoch 7, Step 0: Loss = 0.1292\n",
      "Epoch 7, Step 10: Loss = 0.0028\n",
      "Epoch 7, Step 20: Loss = 0.0443\n",
      "Epoch 7, Step 30: Loss = 0.1245\n",
      "Epoch 7, Step 40: Loss = 0.1472\n",
      "Epoch 7, Step 50: Loss = 0.0077\n",
      "Epoch 7, Step 60: Loss = 0.2056\n",
      "Epoch 7, Step 70: Loss = 0.0000\n",
      "Epoch 7, Step 80: Loss = 0.0510\n",
      "Epoch 7, Step 90: Loss = 0.0700\n",
      "Epoch 7, Step 100: Loss = 0.2092\n",
      "Epoch 7, Step 110: Loss = 0.0000\n",
      "Epoch 7, Step 120: Loss = 0.0275\n",
      "Epoch 7, Step 130: Loss = 0.0000\n",
      "Epoch 7, Step 140: Loss = 0.0584\n",
      "Epoch 7, Step 150: Loss = 0.4471\n",
      "Epoch 7, Step 160: Loss = 0.0706\n",
      "Epoch 7, Step 170: Loss = 0.1774\n",
      "Epoch 7, Step 180: Loss = 0.2095\n",
      "Epoch 7, Step 190: Loss = 0.0144\n",
      "Epoch 7, Step 200: Loss = 0.0000\n",
      "Epoch 7, Step 210: Loss = 0.0000\n",
      "Epoch 7, Step 220: Loss = 0.0216\n",
      "Epoch 7, Step 230: Loss = 0.1957\n",
      "Epoch 7, Step 240: Loss = 0.0231\n",
      "Epoch 7, Step 250: Loss = 0.0000\n",
      "Epoch 7, Step 260: Loss = 0.0000\n",
      "Epoch 7, Step 270: Loss = 0.0000\n",
      "Epoch 7, Step 280: Loss = 0.1563\n",
      "Epoch 7, Step 290: Loss = 0.0102\n",
      "Epoch 7, Step 300: Loss = 0.2681\n",
      "Epoch 7, Step 310: Loss = 0.0567\n",
      "Epoch 7, Step 320: Loss = 0.0000\n",
      "Epoch 7, Step 330: Loss = 0.0486\n",
      "Epoch 7, Step 340: Loss = 0.0278\n",
      "Epoch 7, Step 350: Loss = 0.0000\n",
      "Epoch 7, Step 360: Loss = 0.0004\n",
      "Epoch 7, Step 370: Loss = 0.0454\n",
      "Epoch 7, Step 380: Loss = 0.6817\n",
      "Epoch 7, Step 390: Loss = 0.2457\n",
      "Epoch 7, Step 400: Loss = 0.0000\n",
      "Epoch 7, Step 410: Loss = 0.1075\n",
      "Epoch 7, Step 420: Loss = 0.5299\n",
      "Epoch 7, Step 430: Loss = 0.2162\n",
      "Epoch 7, Step 440: Loss = 0.0303\n",
      "Epoch 7, Step 450: Loss = 0.0333\n",
      "Epoch 7, Step 460: Loss = 0.1583\n",
      "Epoch 7, Step 470: Loss = 0.1389\n",
      "Epoch 7, Step 480: Loss = 0.0574\n",
      "Epoch 7, Step 490: Loss = 0.0945\n",
      "Epoch 7, Step 500: Loss = 0.0145\n",
      "Epoch 7, Step 510: Loss = 0.3862\n",
      "Epoch 7, Step 520: Loss = 0.0003\n",
      "Epoch 7, Step 530: Loss = 0.1567\n",
      "Epoch 7, Step 540: Loss = 0.3848\n",
      "Epoch 7, Step 550: Loss = 0.2423\n",
      "Epoch 7, Step 560: Loss = 0.2537\n",
      "Epoch 7, Step 570: Loss = 0.2484\n",
      "Epoch 7, Step 580: Loss = 0.3569\n",
      "Epoch 7, Step 590: Loss = 0.0333\n",
      "\n",
      "Epoch 7 Summary:\n",
      "Average Training Loss: 0.2353\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2880\n",
      "Epoch 8, Step 0: Loss = 0.0295\n",
      "Epoch 8, Step 10: Loss = 0.0311\n",
      "Epoch 8, Step 20: Loss = 0.0000\n",
      "Epoch 8, Step 30: Loss = 0.0000\n",
      "Epoch 8, Step 40: Loss = 0.0000\n",
      "Epoch 8, Step 50: Loss = 0.0902\n",
      "Epoch 8, Step 60: Loss = 0.0979\n",
      "Epoch 8, Step 70: Loss = 0.0000\n",
      "Epoch 8, Step 80: Loss = 0.1905\n",
      "Epoch 8, Step 90: Loss = 0.0558\n",
      "Epoch 8, Step 100: Loss = 0.0034\n",
      "Epoch 8, Step 110: Loss = 0.0284\n",
      "Epoch 8, Step 120: Loss = 0.2815\n",
      "Epoch 8, Step 130: Loss = 0.0000\n",
      "Epoch 8, Step 140: Loss = 0.0000\n",
      "Epoch 8, Step 150: Loss = 0.0467\n",
      "Epoch 8, Step 160: Loss = 0.0836\n",
      "Epoch 8, Step 170: Loss = 0.2153\n",
      "Epoch 8, Step 180: Loss = 0.5039\n",
      "Epoch 8, Step 190: Loss = 0.1511\n",
      "Epoch 8, Step 200: Loss = 0.1932\n",
      "Epoch 8, Step 210: Loss = 0.2178\n",
      "Epoch 8, Step 220: Loss = 0.0000\n",
      "Epoch 8, Step 230: Loss = 0.0252\n",
      "Epoch 8, Step 240: Loss = 0.0708\n",
      "Epoch 8, Step 250: Loss = 0.1334\n",
      "Epoch 8, Step 260: Loss = 0.0000\n",
      "Epoch 8, Step 270: Loss = 0.0096\n",
      "Epoch 8, Step 280: Loss = 0.1219\n",
      "Epoch 8, Step 290: Loss = 0.0402\n",
      "Epoch 8, Step 300: Loss = 0.0551\n",
      "Epoch 8, Step 310: Loss = 0.2228\n",
      "Epoch 8, Step 320: Loss = 0.0071\n",
      "Epoch 8, Step 330: Loss = 0.0000\n",
      "Epoch 8, Step 340: Loss = 0.0174\n",
      "Epoch 8, Step 350: Loss = 0.0366\n",
      "Epoch 8, Step 360: Loss = 0.0717\n",
      "Epoch 8, Step 370: Loss = 0.0195\n",
      "Epoch 8, Step 380: Loss = 0.2624\n",
      "Epoch 8, Step 390: Loss = 0.0123\n",
      "Epoch 8, Step 400: Loss = 0.0581\n",
      "Epoch 8, Step 410: Loss = 0.1262\n",
      "Epoch 8, Step 420: Loss = 0.0251\n",
      "Epoch 8, Step 430: Loss = 0.0899\n",
      "Epoch 8, Step 440: Loss = 0.5006\n",
      "Epoch 8, Step 450: Loss = 0.0273\n",
      "Epoch 8, Step 460: Loss = 0.2375\n",
      "Epoch 8, Step 470: Loss = 0.0000\n",
      "Epoch 8, Step 480: Loss = 0.1079\n",
      "Epoch 8, Step 490: Loss = 0.0036\n",
      "Epoch 8, Step 500: Loss = 0.1496\n",
      "Epoch 8, Step 510: Loss = 0.0000\n",
      "Epoch 8, Step 520: Loss = 0.4321\n",
      "Epoch 8, Step 530: Loss = 0.0627\n",
      "Epoch 8, Step 540: Loss = 0.0515\n",
      "Epoch 8, Step 550: Loss = 0.3263\n",
      "Epoch 8, Step 560: Loss = 0.0743\n",
      "Epoch 8, Step 570: Loss = 0.1046\n",
      "Epoch 8, Step 580: Loss = 0.5578\n",
      "Epoch 8, Step 590: Loss = 0.0138\n",
      "\n",
      "Epoch 8 Summary:\n",
      "Average Training Loss: 0.2279\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 0: Loss = 0.1083\n",
      "Epoch 9, Step 10: Loss = 0.1633\n",
      "Epoch 9, Step 20: Loss = 0.0363\n",
      "Epoch 9, Step 30: Loss = 0.2780\n",
      "Epoch 9, Step 40: Loss = 0.0840\n",
      "Epoch 9, Step 50: Loss = 0.0000\n",
      "Epoch 9, Step 60: Loss = 0.0172\n",
      "Epoch 9, Step 70: Loss = 0.2872\n",
      "Epoch 9, Step 80: Loss = 0.0733\n",
      "Epoch 9, Step 90: Loss = 0.0207\n",
      "Epoch 9, Step 100: Loss = 0.4088\n",
      "Epoch 9, Step 110: Loss = 0.0570\n",
      "Epoch 9, Step 120: Loss = 0.1450\n",
      "Epoch 9, Step 130: Loss = 0.0031\n",
      "Epoch 9, Step 140: Loss = 0.0000\n",
      "Epoch 9, Step 150: Loss = 0.1433\n",
      "Epoch 9, Step 160: Loss = 0.0643\n",
      "Epoch 9, Step 170: Loss = 0.0000\n",
      "Epoch 9, Step 180: Loss = 0.1204\n",
      "Epoch 9, Step 190: Loss = 0.0876\n",
      "Epoch 9, Step 200: Loss = 0.0206\n",
      "Epoch 9, Step 210: Loss = 0.0000\n",
      "Epoch 9, Step 220: Loss = 0.0000\n",
      "Epoch 9, Step 230: Loss = 0.0000\n",
      "Epoch 9, Step 240: Loss = 0.0000\n",
      "Epoch 9, Step 250: Loss = 0.0132\n",
      "Epoch 9, Step 260: Loss = 0.1766\n",
      "Epoch 9, Step 270: Loss = 0.1162\n",
      "Epoch 9, Step 280: Loss = 0.0949\n",
      "Epoch 9, Step 290: Loss = 0.9205\n",
      "Epoch 9, Step 300: Loss = 0.1037\n",
      "Epoch 9, Step 310: Loss = 0.0000\n",
      "Epoch 9, Step 320: Loss = 0.2051\n",
      "Epoch 9, Step 330: Loss = 0.0000\n",
      "Epoch 9, Step 340: Loss = 0.1614\n",
      "Epoch 9, Step 350: Loss = 0.0241\n",
      "Epoch 9, Step 360: Loss = 0.0000\n",
      "Epoch 9, Step 370: Loss = 0.0857\n",
      "Epoch 9, Step 380: Loss = 0.0792\n",
      "Epoch 9, Step 390: Loss = 0.0000\n",
      "Epoch 9, Step 400: Loss = 0.1593\n",
      "Epoch 9, Step 410: Loss = 0.0178\n",
      "Epoch 9, Step 420: Loss = 0.0000\n",
      "Epoch 9, Step 430: Loss = 0.2673\n",
      "Epoch 9, Step 440: Loss = 0.0000\n",
      "Epoch 9, Step 450: Loss = 0.0195\n",
      "Epoch 9, Step 460: Loss = 0.1603\n",
      "Epoch 9, Step 470: Loss = 0.1298\n",
      "Epoch 9, Step 480: Loss = 0.0000\n",
      "Epoch 9, Step 490: Loss = 0.2911\n",
      "Epoch 9, Step 500: Loss = 0.4521\n",
      "Epoch 9, Step 510: Loss = 0.0000\n",
      "Epoch 9, Step 520: Loss = 0.1403\n",
      "Epoch 9, Step 530: Loss = 0.0000\n",
      "Epoch 9, Step 540: Loss = 0.0726\n",
      "Epoch 9, Step 550: Loss = 0.1496\n",
      "Epoch 9, Step 560: Loss = 0.1616\n",
      "Epoch 9, Step 570: Loss = 0.0901\n",
      "Epoch 9, Step 580: Loss = 0.0000\n",
      "Epoch 9, Step 590: Loss = 0.0000\n",
      "\n",
      "Epoch 9 Summary:\n",
      "Average Training Loss: 0.2160\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2451\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    torch.cuda.set_device(0)  \n",
    "    return torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataset class with data cleaning\n",
    "class ContrastiveEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        # Clean each text field\n",
    "        emails_df['sender'] = emails_df['sender'].apply(clean_text)\n",
    "        emails_df['subject'] = emails_df['subject'].apply(clean_text)\n",
    "        emails_df['body'] = emails_df['body'].apply(clean_text)\n",
    "        \n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.ham_indices = self.emails_df[self.emails_df['label'] == 0].index.tolist()\n",
    "        self.phish_indices = self.emails_df[self.emails_df['label'] == 1].index.tolist()\n",
    "        if not (self.ham_indices and self.phish_indices):\n",
    "            raise ValueError(\"Dataset must contain examples of both classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def _get_random_email_idx(self, label):\n",
    "        indices = self.ham_indices if label == 0 else self.phish_indices\n",
    "        return np.random.choice(indices)\n",
    "\n",
    "    def _prepare_email_input(self, email):\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        tokenized_output = self.tokenizer(input_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        return tokenized_output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_email = self.emails_df.iloc[idx]\n",
    "        anchor_label = anchor_email['label']\n",
    "        positives, negatives = [], []\n",
    "\n",
    "        for _ in range(3):\n",
    "            pos_idx = self._get_random_email_idx(anchor_label)\n",
    "            neg_idx = self._get_random_email_idx(1 - anchor_label)\n",
    "            positives.append(self._prepare_email_input(self.emails_df.iloc[pos_idx]))\n",
    "            negatives.append(self._prepare_email_input(self.emails_df.iloc[neg_idx]))\n",
    "\n",
    "        anchor_inputs = self._prepare_email_input(anchor_email)\n",
    "        return {\n",
    "            'anchor_input_ids': anchor_inputs['input_ids'].squeeze(),\n",
    "            'anchor_attention_mask': anchor_inputs['attention_mask'].squeeze(),\n",
    "            'positive_input_ids': torch.stack([p['input_ids'].squeeze() for p in positives]),\n",
    "            'positive_attention_mask': torch.stack([p['attention_mask'].squeeze() for p in positives]),\n",
    "            'negative_input_ids': torch.stack([n['input_ids'].squeeze() for n in negatives]),\n",
    "            'negative_attention_mask': torch.stack([n['attention_mask'].squeeze() for n in negatives])\n",
    "        }\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, LlamaForSequenceClassification,AutoModelForSequenceClassification\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    # Use the fast tokenizer to avoid the slow/legacy mismatch\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)  \n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Load model config\n",
    "    model_config = AutoConfig.from_pretrained(model_name, num_labels=2, pad_token_id=tokenizer.pad_token_id, use_cache=False)\n",
    "\n",
    "    # Quantize base model\n",
    "    quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,             # if needed for this checkpoint\n",
    "    )\n",
    "\n",
    "    # Attach LoRA\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "    )\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9, margin=1.0, accumulation_steps=2):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    model = model.to(device).to(torch.float16)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                # Compute embeddings for anchor, positive, and negative samples in parallel\n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                # Reshape positive and negative embeddings to match the batch structure (batch_size, 3, embedding_dim)\n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "                # Calculate the triplet loss for each triplet in the batch\n",
    "                loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "                loss /= accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        val_metrics = evaluate_model(model, val_loader, device, margin)\n",
    "        print_metrics(epoch, total_loss, len(train_loader), val_metrics)\n",
    "\n",
    "        if val_metrics['val_loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['val_loss']\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model(model, val_loader, device, margin=1.0):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "               \n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                \n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "               \n",
    "                val_loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "    return {'val_loss': avg_val_loss}\n",
    "\n",
    "\n",
    "# Main function\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  \n",
    "        text = \"\"\n",
    "    \n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def print_metrics(epoch, total_loss, num_batches, val_metrics):\n",
    "    \n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "def main():\n",
    "    login(token=\"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\")\n",
    "    device = setup_environment()\n",
    "    model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/newdata_cleaned.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "\n",
    "   \n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "    train_dataset = ContrastiveEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = ContrastiveEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "    best_model_state = train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/contrastive_8B_Llama\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e082bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d511676f759c40abbcd18021d6d0c5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92df8c8b4f66467290b90f6a51f58127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e7f6134fef4211ba3420642c6e4fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/680 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82cb329a12d240ddaabfecd7c25596da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a889124d5144631998337b7d3b984fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d28d5908af471ea094d2c07b4655ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-000002.safetensors:   0%|          | 0.00/6.62G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb071b956db47508df971c7cc12be54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-000002.safetensors:   0%|          | 0.00/8.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b643ba4317a4e629d0bc1e10f070604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at deepseek-ai/DeepSeek-R1-Distill-Qwen-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 0.8028\n",
      "Epoch 1, Step 10: Loss = 0.6896\n",
      "Epoch 1, Step 20: Loss = 1.2010\n",
      "Epoch 1, Step 30: Loss = 0.6916\n",
      "Epoch 1, Step 40: Loss = 0.6402\n",
      "Epoch 1, Step 50: Loss = 0.9477\n",
      "Epoch 1, Step 60: Loss = 1.1248\n",
      "Epoch 1, Step 70: Loss = 0.9071\n",
      "Epoch 1, Step 80: Loss = 0.5568\n",
      "Epoch 1, Step 90: Loss = 1.3045\n",
      "Epoch 1, Step 100: Loss = 1.2882\n",
      "Epoch 1, Step 110: Loss = 0.6885\n",
      "Epoch 1, Step 120: Loss = 0.7105\n",
      "Epoch 1, Step 130: Loss = 0.7471\n",
      "Epoch 1, Step 140: Loss = 1.1494\n",
      "Epoch 1, Step 150: Loss = 0.6830\n",
      "Epoch 1, Step 160: Loss = 1.1418\n",
      "Epoch 1, Step 170: Loss = 1.0892\n",
      "Epoch 1, Step 180: Loss = 0.8536\n",
      "Epoch 1, Step 190: Loss = 1.0699\n",
      "Epoch 1, Step 200: Loss = 0.7271\n",
      "Epoch 1, Step 210: Loss = 1.3153\n",
      "Epoch 1, Step 220: Loss = 0.8392\n",
      "Epoch 1, Step 230: Loss = 0.7340\n",
      "Epoch 1, Step 240: Loss = 0.5714\n",
      "Epoch 1, Step 250: Loss = 0.6092\n",
      "Epoch 1, Step 260: Loss = 0.4712\n",
      "Epoch 1, Step 270: Loss = 0.6679\n",
      "Epoch 1, Step 280: Loss = 0.9576\n",
      "Epoch 1, Step 290: Loss = 0.9810\n",
      "Epoch 1, Step 300: Loss = 0.6566\n",
      "Epoch 1, Step 310: Loss = 0.8321\n",
      "Epoch 1, Step 320: Loss = 0.8272\n",
      "Epoch 1, Step 330: Loss = 0.9096\n",
      "Epoch 1, Step 340: Loss = 0.7731\n",
      "Epoch 1, Step 350: Loss = 0.6954\n",
      "Epoch 1, Step 360: Loss = 1.1668\n",
      "Epoch 1, Step 370: Loss = 1.0642\n",
      "Epoch 1, Step 380: Loss = 0.7664\n",
      "Epoch 1, Step 390: Loss = 0.9139\n",
      "Epoch 1, Step 400: Loss = 1.0166\n",
      "Epoch 1, Step 410: Loss = 0.9291\n",
      "Epoch 1, Step 420: Loss = 0.8400\n",
      "Epoch 1, Step 430: Loss = 0.8794\n",
      "Epoch 1, Step 440: Loss = 0.6870\n",
      "Epoch 1, Step 450: Loss = 0.4716\n",
      "Epoch 1, Step 460: Loss = 1.1412\n",
      "Epoch 1, Step 470: Loss = 0.5517\n",
      "Epoch 1, Step 480: Loss = 0.5737\n",
      "Epoch 1, Step 490: Loss = 0.7201\n",
      "Epoch 1, Step 500: Loss = 1.2427\n",
      "Epoch 1, Step 510: Loss = 0.7675\n",
      "Epoch 1, Step 520: Loss = 0.8291\n",
      "Epoch 1, Step 530: Loss = 0.5489\n",
      "Epoch 1, Step 540: Loss = 0.7039\n",
      "Epoch 1, Step 550: Loss = 0.6663\n",
      "Epoch 1, Step 560: Loss = 0.4871\n",
      "Epoch 1, Step 570: Loss = 0.6794\n",
      "Epoch 1, Step 580: Loss = 0.7136\n",
      "Epoch 1, Step 590: Loss = 0.3861\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Average Training Loss: 1.6310\n",
      "Validation Metrics:\n",
      "Val_loss: 1.3547\n",
      "Epoch 2, Step 0: Loss = 0.6979\n",
      "Epoch 2, Step 10: Loss = 1.0765\n",
      "Epoch 2, Step 20: Loss = 0.6934\n",
      "Epoch 2, Step 30: Loss = 0.6589\n",
      "Epoch 2, Step 40: Loss = 0.7093\n",
      "Epoch 2, Step 50: Loss = 0.4535\n",
      "Epoch 2, Step 60: Loss = 0.8546\n",
      "Epoch 2, Step 70: Loss = 1.4023\n",
      "Epoch 2, Step 80: Loss = 0.5157\n",
      "Epoch 2, Step 90: Loss = 0.1991\n",
      "Epoch 2, Step 100: Loss = 0.6927\n",
      "Epoch 2, Step 110: Loss = 0.5670\n",
      "Epoch 2, Step 120: Loss = 0.7820\n",
      "Epoch 2, Step 130: Loss = 0.5104\n",
      "Epoch 2, Step 140: Loss = 0.6472\n",
      "Epoch 2, Step 150: Loss = 0.8278\n",
      "Epoch 2, Step 160: Loss = 0.5741\n",
      "Epoch 2, Step 170: Loss = 0.4544\n",
      "Epoch 2, Step 180: Loss = 0.8039\n",
      "Epoch 2, Step 190: Loss = 0.4574\n",
      "Epoch 2, Step 200: Loss = 0.7063\n",
      "Epoch 2, Step 210: Loss = 0.7338\n",
      "Epoch 2, Step 220: Loss = 0.2674\n",
      "Epoch 2, Step 230: Loss = 0.5761\n",
      "Epoch 2, Step 240: Loss = 0.6210\n",
      "Epoch 2, Step 250: Loss = 0.5686\n",
      "Epoch 2, Step 260: Loss = 0.9780\n",
      "Epoch 2, Step 270: Loss = 0.5970\n",
      "Epoch 2, Step 280: Loss = 0.6974\n",
      "Epoch 2, Step 290: Loss = 0.3119\n",
      "Epoch 2, Step 300: Loss = 0.2142\n",
      "Epoch 2, Step 310: Loss = 0.6184\n",
      "Epoch 2, Step 320: Loss = 0.4291\n",
      "Epoch 2, Step 330: Loss = 0.5514\n",
      "Epoch 2, Step 340: Loss = 0.3747\n",
      "Epoch 2, Step 350: Loss = 0.9310\n",
      "Epoch 2, Step 360: Loss = 1.1318\n",
      "Epoch 2, Step 370: Loss = 0.2182\n",
      "Epoch 2, Step 380: Loss = 0.6259\n",
      "Epoch 2, Step 390: Loss = 0.5988\n",
      "Epoch 2, Step 400: Loss = 0.5774\n",
      "Epoch 2, Step 410: Loss = 0.5284\n",
      "Epoch 2, Step 420: Loss = 0.2227\n",
      "Epoch 2, Step 430: Loss = 0.6203\n",
      "Epoch 2, Step 440: Loss = 0.8640\n",
      "Epoch 2, Step 450: Loss = 0.4620\n",
      "Epoch 2, Step 460: Loss = 0.3229\n",
      "Epoch 2, Step 470: Loss = 0.6151\n",
      "Epoch 2, Step 480: Loss = 1.0955\n",
      "Epoch 2, Step 490: Loss = 0.3784\n",
      "Epoch 2, Step 500: Loss = 0.2375\n",
      "Epoch 2, Step 510: Loss = 0.4577\n",
      "Epoch 2, Step 520: Loss = 0.5546\n",
      "Epoch 2, Step 530: Loss = 0.1448\n",
      "Epoch 2, Step 540: Loss = 0.5682\n",
      "Epoch 2, Step 550: Loss = 0.1847\n",
      "Epoch 2, Step 560: Loss = 0.3725\n",
      "Epoch 2, Step 570: Loss = 0.9155\n",
      "Epoch 2, Step 580: Loss = 0.2791\n",
      "Epoch 2, Step 590: Loss = 0.6582\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Average Training Loss: 1.1345\n",
      "Validation Metrics:\n",
      "Val_loss: 0.7374\n",
      "Epoch 3, Step 0: Loss = 0.3745\n",
      "Epoch 3, Step 10: Loss = 1.1563\n",
      "Epoch 3, Step 20: Loss = 0.1497\n",
      "Epoch 3, Step 30: Loss = 0.3601\n",
      "Epoch 3, Step 40: Loss = 0.3573\n",
      "Epoch 3, Step 50: Loss = 0.5460\n",
      "Epoch 3, Step 60: Loss = 0.3525\n",
      "Epoch 3, Step 70: Loss = 0.4894\n",
      "Epoch 3, Step 80: Loss = 0.4318\n",
      "Epoch 3, Step 90: Loss = 0.4185\n",
      "Epoch 3, Step 100: Loss = 0.3064\n",
      "Epoch 3, Step 110: Loss = 0.6804\n",
      "Epoch 3, Step 120: Loss = 0.6796\n",
      "Epoch 3, Step 130: Loss = 0.6665\n",
      "Epoch 3, Step 140: Loss = 0.3983\n",
      "Epoch 3, Step 150: Loss = 0.4463\n",
      "Epoch 3, Step 160: Loss = 0.0258\n",
      "Epoch 3, Step 170: Loss = 0.4452\n",
      "Epoch 3, Step 180: Loss = 0.4200\n",
      "Epoch 3, Step 190: Loss = 0.3424\n",
      "Epoch 3, Step 200: Loss = 0.2079\n",
      "Epoch 3, Step 210: Loss = 0.1848\n",
      "Epoch 3, Step 220: Loss = 0.5479\n",
      "Epoch 3, Step 230: Loss = 0.5262\n",
      "Epoch 3, Step 240: Loss = 0.2975\n",
      "Epoch 3, Step 250: Loss = 0.3629\n",
      "Epoch 3, Step 260: Loss = 0.6690\n",
      "Epoch 3, Step 270: Loss = 0.2835\n",
      "Epoch 3, Step 280: Loss = 0.1164\n",
      "Epoch 3, Step 290: Loss = 0.6113\n",
      "Epoch 3, Step 300: Loss = 0.4504\n",
      "Epoch 3, Step 310: Loss = 0.3762\n",
      "Epoch 3, Step 320: Loss = 0.2328\n",
      "Epoch 3, Step 330: Loss = 0.3848\n",
      "Epoch 3, Step 340: Loss = 0.4763\n",
      "Epoch 3, Step 350: Loss = 0.2364\n",
      "Epoch 3, Step 360: Loss = 0.1577\n",
      "Epoch 3, Step 370: Loss = 0.4684\n",
      "Epoch 3, Step 380: Loss = 0.2343\n",
      "Epoch 3, Step 390: Loss = 0.2650\n",
      "Epoch 3, Step 400: Loss = 0.5435\n",
      "Epoch 3, Step 410: Loss = 0.6086\n",
      "Epoch 3, Step 420: Loss = 0.2594\n",
      "Epoch 3, Step 430: Loss = 0.1303\n",
      "Epoch 3, Step 440: Loss = 0.8008\n",
      "Epoch 3, Step 450: Loss = 0.4242\n",
      "Epoch 3, Step 460: Loss = 0.4001\n",
      "Epoch 3, Step 470: Loss = 0.3936\n",
      "Epoch 3, Step 480: Loss = 0.1578\n",
      "Epoch 3, Step 490: Loss = 0.3084\n",
      "Epoch 3, Step 500: Loss = 0.0492\n",
      "Epoch 3, Step 510: Loss = 0.1304\n",
      "Epoch 3, Step 520: Loss = 0.1569\n",
      "Epoch 3, Step 530: Loss = 0.4053\n",
      "Epoch 3, Step 540: Loss = 0.0641\n",
      "Epoch 3, Step 550: Loss = 0.5140\n",
      "Epoch 3, Step 560: Loss = 0.5454\n",
      "Epoch 3, Step 570: Loss = 0.4208\n",
      "Epoch 3, Step 580: Loss = 0.0613\n",
      "Epoch 3, Step 590: Loss = 0.6017\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Average Training Loss: 0.7490\n",
      "Validation Metrics:\n",
      "Val_loss: 0.5188\n",
      "Epoch 4, Step 0: Loss = 0.0378\n",
      "Epoch 4, Step 10: Loss = 0.4729\n",
      "Epoch 4, Step 20: Loss = 0.0683\n",
      "Epoch 4, Step 30: Loss = 0.2162\n",
      "Epoch 4, Step 40: Loss = 0.2090\n",
      "Epoch 4, Step 50: Loss = 0.3489\n",
      "Epoch 4, Step 60: Loss = 0.2820\n",
      "Epoch 4, Step 70: Loss = 0.3108\n",
      "Epoch 4, Step 80: Loss = 0.0246\n",
      "Epoch 4, Step 90: Loss = 0.4071\n",
      "Epoch 4, Step 100: Loss = 0.1546\n",
      "Epoch 4, Step 110: Loss = 0.1182\n",
      "Epoch 4, Step 120: Loss = 0.0994\n",
      "Epoch 4, Step 130: Loss = 0.6400\n",
      "Epoch 4, Step 140: Loss = 0.3120\n",
      "Epoch 4, Step 150: Loss = 0.1045\n",
      "Epoch 4, Step 160: Loss = 0.2073\n",
      "Epoch 4, Step 170: Loss = 0.0821\n",
      "Epoch 4, Step 180: Loss = 0.0613\n",
      "Epoch 4, Step 190: Loss = 0.4735\n",
      "Epoch 4, Step 200: Loss = 0.0308\n",
      "Epoch 4, Step 210: Loss = 0.2871\n",
      "Epoch 4, Step 220: Loss = 0.4672\n",
      "Epoch 4, Step 230: Loss = 0.4029\n",
      "Epoch 4, Step 240: Loss = 0.2361\n",
      "Epoch 4, Step 250: Loss = 0.2580\n",
      "Epoch 4, Step 260: Loss = 0.6958\n",
      "Epoch 4, Step 270: Loss = 0.2194\n",
      "Epoch 4, Step 280: Loss = 0.2764\n",
      "Epoch 4, Step 290: Loss = 0.4733\n",
      "Epoch 4, Step 300: Loss = 0.1117\n",
      "Epoch 4, Step 310: Loss = 0.0314\n",
      "Epoch 4, Step 320: Loss = 0.0250\n",
      "Epoch 4, Step 330: Loss = 0.2993\n",
      "Epoch 4, Step 340: Loss = 0.1274\n",
      "Epoch 4, Step 350: Loss = 0.1230\n",
      "Epoch 4, Step 360: Loss = 0.3429\n",
      "Epoch 4, Step 370: Loss = 0.0870\n",
      "Epoch 4, Step 380: Loss = 0.0773\n",
      "Epoch 4, Step 390: Loss = 0.1840\n",
      "Epoch 4, Step 400: Loss = 0.1835\n",
      "Epoch 4, Step 410: Loss = 0.0847\n",
      "Epoch 4, Step 420: Loss = 0.1179\n",
      "Epoch 4, Step 430: Loss = 0.2721\n",
      "Epoch 4, Step 440: Loss = 0.3946\n",
      "Epoch 4, Step 450: Loss = 0.2919\n",
      "Epoch 4, Step 460: Loss = 0.2932\n",
      "Epoch 4, Step 470: Loss = 0.0558\n",
      "Epoch 4, Step 480: Loss = 0.1869\n",
      "Epoch 4, Step 490: Loss = 0.0540\n",
      "Epoch 4, Step 500: Loss = 0.0063\n",
      "Epoch 4, Step 510: Loss = 0.0838\n",
      "Epoch 4, Step 520: Loss = 0.1289\n",
      "Epoch 4, Step 530: Loss = 0.1168\n",
      "Epoch 4, Step 540: Loss = 0.2908\n",
      "Epoch 4, Step 550: Loss = 0.2624\n",
      "Epoch 4, Step 560: Loss = 0.3920\n",
      "Epoch 4, Step 570: Loss = 0.0727\n",
      "Epoch 4, Step 580: Loss = 0.0282\n",
      "Epoch 4, Step 590: Loss = 0.5300\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Average Training Loss: 0.5789\n",
      "Validation Metrics:\n",
      "Val_loss: 0.4183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.5847\n",
      "Epoch 5, Step 10: Loss = 0.2536\n",
      "Epoch 5, Step 20: Loss = 0.1347\n",
      "Epoch 5, Step 30: Loss = 0.0988\n",
      "Epoch 5, Step 40: Loss = 0.1152\n",
      "Epoch 5, Step 50: Loss = 0.4080\n",
      "Epoch 5, Step 60: Loss = 0.1958\n",
      "Epoch 5, Step 70: Loss = 0.4006\n",
      "Epoch 5, Step 80: Loss = 0.2764\n",
      "Epoch 5, Step 90: Loss = 0.2463\n",
      "Epoch 5, Step 100: Loss = 0.4503\n",
      "Epoch 5, Step 110: Loss = 0.4649\n",
      "Epoch 5, Step 120: Loss = 0.3792\n",
      "Epoch 5, Step 130: Loss = 0.0900\n",
      "Epoch 5, Step 140: Loss = 0.1605\n",
      "Epoch 5, Step 150: Loss = 0.2922\n",
      "Epoch 5, Step 160: Loss = 0.1506\n",
      "Epoch 5, Step 170: Loss = 0.2528\n",
      "Epoch 5, Step 180: Loss = 0.1301\n",
      "Epoch 5, Step 190: Loss = 0.1761\n",
      "Epoch 5, Step 200: Loss = 0.2238\n",
      "Epoch 5, Step 210: Loss = 0.2018\n",
      "Epoch 5, Step 220: Loss = 0.0098\n",
      "Epoch 5, Step 230: Loss = 0.3342\n",
      "Epoch 5, Step 240: Loss = 0.4101\n",
      "Epoch 5, Step 250: Loss = 0.0187\n",
      "Epoch 5, Step 260: Loss = 0.3135\n",
      "Epoch 5, Step 270: Loss = 0.2022\n",
      "Epoch 5, Step 280: Loss = 0.3618\n",
      "Epoch 5, Step 290: Loss = 0.3848\n",
      "Epoch 5, Step 300: Loss = 0.0176\n",
      "Epoch 5, Step 310: Loss = 0.1450\n",
      "Epoch 5, Step 320: Loss = 0.0453\n",
      "Epoch 5, Step 330: Loss = 0.1594\n",
      "Epoch 5, Step 340: Loss = 0.7481\n",
      "Epoch 5, Step 350: Loss = 0.4860\n",
      "Epoch 5, Step 360: Loss = 0.1651\n",
      "Epoch 5, Step 370: Loss = 0.1876\n",
      "Epoch 5, Step 380: Loss = 0.1468\n",
      "Epoch 5, Step 390: Loss = 0.1833\n",
      "Epoch 5, Step 400: Loss = 0.1493\n",
      "Epoch 5, Step 410: Loss = 0.0536\n",
      "Epoch 5, Step 420: Loss = 0.1411\n",
      "Epoch 5, Step 430: Loss = 0.4346\n",
      "Epoch 5, Step 440: Loss = 0.3922\n",
      "Epoch 5, Step 450: Loss = 0.3248\n",
      "Epoch 5, Step 460: Loss = 0.4394\n",
      "Epoch 5, Step 470: Loss = 0.0768\n",
      "Epoch 5, Step 480: Loss = 0.2196\n",
      "Epoch 5, Step 490: Loss = 0.7647\n",
      "Epoch 5, Step 500: Loss = 0.0463\n",
      "Epoch 5, Step 510: Loss = 0.1455\n",
      "Epoch 5, Step 520: Loss = 0.1086\n",
      "Epoch 5, Step 530: Loss = 0.2856\n",
      "Epoch 5, Step 540: Loss = 0.1722\n",
      "Epoch 5, Step 550: Loss = 0.4019\n",
      "Epoch 5, Step 560: Loss = 0.0887\n",
      "Epoch 5, Step 570: Loss = 0.0421\n",
      "Epoch 5, Step 580: Loss = 0.0537\n",
      "Epoch 5, Step 590: Loss = 0.4499\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Average Training Loss: 0.4727\n",
      "Validation Metrics:\n",
      "Val_loss: 0.3843\n",
      "Epoch 6, Step 0: Loss = 0.2370\n",
      "Epoch 6, Step 10: Loss = 0.0335\n",
      "Epoch 6, Step 20: Loss = 0.1105\n",
      "Epoch 6, Step 30: Loss = 0.4669\n",
      "Epoch 6, Step 40: Loss = 0.6617\n",
      "Epoch 6, Step 50: Loss = 0.0728\n",
      "Epoch 6, Step 60: Loss = 0.2372\n",
      "Epoch 6, Step 70: Loss = 0.4244\n",
      "Epoch 6, Step 80: Loss = 0.1667\n",
      "Epoch 6, Step 90: Loss = 0.4599\n",
      "Epoch 6, Step 100: Loss = 0.4298\n",
      "Epoch 6, Step 110: Loss = 0.0572\n",
      "Epoch 6, Step 120: Loss = 0.5412\n",
      "Epoch 6, Step 130: Loss = 0.0510\n",
      "Epoch 6, Step 140: Loss = 0.0822\n",
      "Epoch 6, Step 150: Loss = 0.2118\n",
      "Epoch 6, Step 160: Loss = 0.3040\n",
      "Epoch 6, Step 170: Loss = 0.1914\n",
      "Epoch 6, Step 180: Loss = 0.1042\n",
      "Epoch 6, Step 190: Loss = 0.2502\n",
      "Epoch 6, Step 200: Loss = 0.2190\n",
      "Epoch 6, Step 210: Loss = 0.0586\n",
      "Epoch 6, Step 220: Loss = 0.2337\n",
      "Epoch 6, Step 230: Loss = 0.2347\n",
      "Epoch 6, Step 240: Loss = 0.0584\n",
      "Epoch 6, Step 250: Loss = 0.1682\n",
      "Epoch 6, Step 260: Loss = 0.2026\n",
      "Epoch 6, Step 270: Loss = 0.2434\n",
      "Epoch 6, Step 280: Loss = 0.3109\n",
      "Epoch 6, Step 290: Loss = 0.3212\n",
      "Epoch 6, Step 300: Loss = 0.0801\n",
      "Epoch 6, Step 310: Loss = 0.2245\n",
      "Epoch 6, Step 320: Loss = 0.3266\n",
      "Epoch 6, Step 330: Loss = 0.3041\n",
      "Epoch 6, Step 340: Loss = 0.3992\n",
      "Epoch 6, Step 350: Loss = 0.0788\n",
      "Epoch 6, Step 360: Loss = 0.1232\n",
      "Epoch 6, Step 370: Loss = 0.3465\n",
      "Epoch 6, Step 380: Loss = 0.0832\n",
      "Epoch 6, Step 390: Loss = 0.0000\n",
      "Epoch 6, Step 400: Loss = 0.4952\n",
      "Epoch 6, Step 410: Loss = 0.1923\n",
      "Epoch 6, Step 420: Loss = 0.0325\n",
      "Epoch 6, Step 430: Loss = 0.8861\n",
      "Epoch 6, Step 440: Loss = 0.5172\n",
      "Epoch 6, Step 450: Loss = 0.1260\n",
      "Epoch 6, Step 460: Loss = 0.3001\n",
      "Epoch 6, Step 470: Loss = 0.6174\n",
      "Epoch 6, Step 480: Loss = 0.1492\n",
      "Epoch 6, Step 490: Loss = 0.2104\n",
      "Epoch 6, Step 500: Loss = 0.1265\n",
      "Epoch 6, Step 510: Loss = 0.2366\n",
      "Epoch 6, Step 520: Loss = 0.0326\n",
      "Epoch 6, Step 530: Loss = 0.0446\n",
      "Epoch 6, Step 540: Loss = 0.0115\n",
      "Epoch 6, Step 550: Loss = 0.2642\n",
      "Epoch 6, Step 560: Loss = 0.4266\n",
      "Epoch 6, Step 570: Loss = 0.3825\n",
      "Epoch 6, Step 580: Loss = 0.0316\n",
      "Epoch 6, Step 590: Loss = 0.1519\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Average Training Loss: 0.4088\n",
      "Validation Metrics:\n",
      "Val_loss: 0.3499\n",
      "Epoch 7, Step 0: Loss = 0.0959\n",
      "Epoch 7, Step 10: Loss = 0.2045\n",
      "Epoch 7, Step 20: Loss = 0.6994\n",
      "Epoch 7, Step 30: Loss = 0.0717\n",
      "Epoch 7, Step 40: Loss = 0.4723\n",
      "Epoch 7, Step 50: Loss = 0.0281\n",
      "Epoch 7, Step 60: Loss = 0.2824\n",
      "Epoch 7, Step 70: Loss = 0.2859\n",
      "Epoch 7, Step 80: Loss = 0.0132\n",
      "Epoch 7, Step 90: Loss = 0.3023\n",
      "Epoch 7, Step 100: Loss = 0.3008\n",
      "Epoch 7, Step 110: Loss = 0.0182\n",
      "Epoch 7, Step 120: Loss = 0.2364\n",
      "Epoch 7, Step 130: Loss = 0.2793\n",
      "Epoch 7, Step 140: Loss = 0.0099\n",
      "Epoch 7, Step 150: Loss = 0.1994\n",
      "Epoch 7, Step 160: Loss = 0.0193\n",
      "Epoch 7, Step 170: Loss = 0.1833\n",
      "Epoch 7, Step 180: Loss = 0.0669\n",
      "Epoch 7, Step 190: Loss = 0.0645\n",
      "Epoch 7, Step 200: Loss = 0.0331\n",
      "Epoch 7, Step 210: Loss = 0.0412\n",
      "Epoch 7, Step 220: Loss = 0.0187\n",
      "Epoch 7, Step 230: Loss = 0.1492\n",
      "Epoch 7, Step 240: Loss = 0.1487\n",
      "Epoch 7, Step 250: Loss = 0.5682\n",
      "Epoch 7, Step 260: Loss = 0.3717\n",
      "Epoch 7, Step 270: Loss = 0.0645\n",
      "Epoch 7, Step 280: Loss = 0.1830\n",
      "Epoch 7, Step 290: Loss = 0.1109\n",
      "Epoch 7, Step 300: Loss = 0.0191\n",
      "Epoch 7, Step 310: Loss = 0.0664\n",
      "Epoch 7, Step 320: Loss = 0.1714\n",
      "Epoch 7, Step 330: Loss = 0.2644\n",
      "Epoch 7, Step 340: Loss = 0.0625\n",
      "Epoch 7, Step 350: Loss = 0.1146\n",
      "Epoch 7, Step 360: Loss = 0.1440\n",
      "Epoch 7, Step 370: Loss = 0.0566\n",
      "Epoch 7, Step 380: Loss = 0.1763\n",
      "Epoch 7, Step 390: Loss = 0.3543\n",
      "Epoch 7, Step 400: Loss = 0.1332\n",
      "Epoch 7, Step 410: Loss = 0.4123\n",
      "Epoch 7, Step 420: Loss = 0.1363\n",
      "Epoch 7, Step 430: Loss = 0.0836\n",
      "Epoch 7, Step 440: Loss = 0.0494\n",
      "Epoch 7, Step 450: Loss = 0.0788\n",
      "Epoch 7, Step 460: Loss = 0.1697\n",
      "Epoch 7, Step 470: Loss = 0.0586\n",
      "Epoch 7, Step 480: Loss = 0.5586\n",
      "Epoch 7, Step 490: Loss = 0.4206\n",
      "Epoch 7, Step 500: Loss = 0.0118\n",
      "Epoch 7, Step 510: Loss = 0.3430\n",
      "Epoch 7, Step 520: Loss = 0.2049\n",
      "Epoch 7, Step 530: Loss = 0.0411\n",
      "Epoch 7, Step 540: Loss = 0.0207\n",
      "Epoch 7, Step 550: Loss = 0.1247\n",
      "Epoch 7, Step 560: Loss = 0.3457\n",
      "Epoch 7, Step 570: Loss = 0.0000\n",
      "Epoch 7, Step 580: Loss = 0.1404\n",
      "Epoch 7, Step 590: Loss = 0.0143\n",
      "\n",
      "Epoch 7 Summary:\n",
      "Average Training Loss: 0.3803\n",
      "Validation Metrics:\n",
      "Val_loss: 0.3302\n",
      "Epoch 8, Step 0: Loss = 0.2085\n",
      "Epoch 8, Step 10: Loss = 0.4360\n",
      "Epoch 8, Step 20: Loss = 0.1033\n",
      "Epoch 8, Step 30: Loss = 0.2223\n",
      "Epoch 8, Step 40: Loss = 0.0434\n",
      "Epoch 8, Step 50: Loss = 0.0559\n",
      "Epoch 8, Step 60: Loss = 0.1497\n",
      "Epoch 8, Step 70: Loss = 0.2561\n",
      "Epoch 8, Step 80: Loss = 0.0648\n",
      "Epoch 8, Step 90: Loss = 0.3015\n",
      "Epoch 8, Step 100: Loss = 0.0592\n",
      "Epoch 8, Step 110: Loss = 0.2577\n",
      "Epoch 8, Step 120: Loss = 0.0260\n",
      "Epoch 8, Step 130: Loss = 0.2332\n",
      "Epoch 8, Step 140: Loss = 0.0233\n",
      "Epoch 8, Step 150: Loss = 0.1801\n",
      "Epoch 8, Step 160: Loss = 0.0932\n",
      "Epoch 8, Step 170: Loss = 0.2053\n",
      "Epoch 8, Step 180: Loss = 0.0061\n",
      "Epoch 8, Step 190: Loss = 0.0827\n",
      "Epoch 8, Step 200: Loss = 0.1191\n",
      "Epoch 8, Step 210: Loss = 0.1243\n",
      "Epoch 8, Step 220: Loss = 0.0000\n",
      "Epoch 8, Step 230: Loss = 0.1853\n",
      "Epoch 8, Step 240: Loss = 0.8092\n",
      "Epoch 8, Step 250: Loss = 0.0878\n",
      "Epoch 8, Step 260: Loss = 0.0000\n",
      "Epoch 8, Step 270: Loss = 0.1378\n",
      "Epoch 8, Step 280: Loss = 0.2404\n",
      "Epoch 8, Step 290: Loss = 0.0642\n",
      "Epoch 8, Step 300: Loss = 0.0000\n",
      "Epoch 8, Step 310: Loss = 0.5572\n",
      "Epoch 8, Step 320: Loss = 0.1890\n",
      "Epoch 8, Step 330: Loss = 0.1822\n",
      "Epoch 8, Step 340: Loss = 0.0808\n",
      "Epoch 8, Step 350: Loss = 0.0000\n",
      "Epoch 8, Step 360: Loss = 0.1501\n",
      "Epoch 8, Step 370: Loss = 0.4548\n",
      "Epoch 8, Step 380: Loss = 0.0380\n",
      "Epoch 8, Step 390: Loss = 0.0573\n",
      "Epoch 8, Step 400: Loss = 0.2052\n",
      "Epoch 8, Step 410: Loss = 0.0989\n",
      "Epoch 8, Step 420: Loss = 0.1087\n",
      "Epoch 8, Step 430: Loss = 0.0910\n",
      "Epoch 8, Step 440: Loss = 0.0792\n",
      "Epoch 8, Step 450: Loss = 0.3096\n",
      "Epoch 8, Step 460: Loss = 0.4068\n",
      "Epoch 8, Step 470: Loss = 0.0856\n",
      "Epoch 8, Step 480: Loss = 0.0633\n",
      "Epoch 8, Step 490: Loss = 0.0899\n",
      "Epoch 8, Step 500: Loss = 0.2156\n",
      "Epoch 8, Step 510: Loss = 0.1783\n",
      "Epoch 8, Step 520: Loss = 0.1517\n",
      "Epoch 8, Step 530: Loss = 0.0942\n",
      "Epoch 8, Step 540: Loss = 0.5387\n",
      "Epoch 8, Step 550: Loss = 0.1200\n",
      "Epoch 8, Step 560: Loss = 0.0089\n",
      "Epoch 8, Step 570: Loss = 0.1848\n",
      "Epoch 8, Step 580: Loss = 0.0315\n",
      "Epoch 8, Step 590: Loss = 0.3253\n",
      "\n",
      "Epoch 8 Summary:\n",
      "Average Training Loss: 0.3672\n",
      "Validation Metrics:\n",
      "Val_loss: 0.3237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 0: Loss = 0.5386\n",
      "Epoch 9, Step 10: Loss = 0.3143\n",
      "Epoch 9, Step 20: Loss = 0.5745\n",
      "Epoch 9, Step 30: Loss = 0.0539\n",
      "Epoch 9, Step 40: Loss = 0.0669\n",
      "Epoch 9, Step 50: Loss = 0.1700\n",
      "Epoch 9, Step 60: Loss = 0.1260\n",
      "Epoch 9, Step 70: Loss = 0.3125\n",
      "Epoch 9, Step 80: Loss = 0.1614\n",
      "Epoch 9, Step 90: Loss = 0.4260\n",
      "Epoch 9, Step 100: Loss = 0.5738\n",
      "Epoch 9, Step 110: Loss = 0.0813\n",
      "Epoch 9, Step 120: Loss = 0.3561\n",
      "Epoch 9, Step 130: Loss = 0.2631\n",
      "Epoch 9, Step 140: Loss = 0.0845\n",
      "Epoch 9, Step 150: Loss = 0.1350\n",
      "Epoch 9, Step 160: Loss = 0.1224\n",
      "Epoch 9, Step 170: Loss = 0.2205\n",
      "Epoch 9, Step 180: Loss = 0.2210\n",
      "Epoch 9, Step 190: Loss = 0.0603\n",
      "Epoch 9, Step 200: Loss = 0.0312\n",
      "Epoch 9, Step 210: Loss = 0.4600\n",
      "Epoch 9, Step 220: Loss = 0.0000\n",
      "Epoch 9, Step 230: Loss = 0.1804\n",
      "Epoch 9, Step 240: Loss = 0.2589\n",
      "Epoch 9, Step 250: Loss = 0.1767\n",
      "Epoch 9, Step 260: Loss = 0.0672\n",
      "Epoch 9, Step 270: Loss = 0.1853\n",
      "Epoch 9, Step 280: Loss = 0.1249\n",
      "Epoch 9, Step 290: Loss = 0.3505\n",
      "Epoch 9, Step 300: Loss = 0.0484\n",
      "Epoch 9, Step 310: Loss = 0.0976\n",
      "Epoch 9, Step 320: Loss = 0.0032\n",
      "Epoch 9, Step 330: Loss = 0.0780\n",
      "Epoch 9, Step 340: Loss = 0.1825\n",
      "Epoch 9, Step 350: Loss = 0.0619\n",
      "Epoch 9, Step 360: Loss = 0.0000\n",
      "Epoch 9, Step 370: Loss = 0.0569\n",
      "Epoch 9, Step 380: Loss = 0.1726\n",
      "Epoch 9, Step 390: Loss = 0.4777\n",
      "Epoch 9, Step 400: Loss = 0.0955\n",
      "Epoch 9, Step 410: Loss = 0.0437\n",
      "Epoch 9, Step 420: Loss = 0.1453\n",
      "Epoch 9, Step 430: Loss = 0.1500\n",
      "Epoch 9, Step 440: Loss = 0.1723\n",
      "Epoch 9, Step 450: Loss = 0.1669\n",
      "Epoch 9, Step 460: Loss = 0.2724\n",
      "Epoch 9, Step 470: Loss = 0.2135\n",
      "Epoch 9, Step 480: Loss = 0.1241\n",
      "Epoch 9, Step 490: Loss = 0.2066\n",
      "Epoch 9, Step 500: Loss = 0.2497\n",
      "Epoch 9, Step 510: Loss = 0.0455\n",
      "Epoch 9, Step 520: Loss = 0.4070\n",
      "Epoch 9, Step 530: Loss = 0.0927\n",
      "Epoch 9, Step 540: Loss = 0.0000\n",
      "Epoch 9, Step 550: Loss = 0.0617\n",
      "Epoch 9, Step 560: Loss = 0.4754\n",
      "Epoch 9, Step 570: Loss = 0.1869\n",
      "Epoch 9, Step 580: Loss = 0.0567\n",
      "Epoch 9, Step 590: Loss = 0.0851\n",
      "\n",
      "Epoch 9 Summary:\n",
      "Average Training Loss: 0.3457\n",
      "Validation Metrics:\n",
      "Val_loss: 0.3089\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    torch.cuda.set_device(0)  \n",
    "    return torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataset class with data cleaning\n",
    "class ContrastiveEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        # Clean each text field\n",
    "        emails_df['sender'] = emails_df['sender'].apply(clean_text)\n",
    "        emails_df['subject'] = emails_df['subject'].apply(clean_text)\n",
    "        emails_df['body'] = emails_df['body'].apply(clean_text)\n",
    "        \n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.ham_indices = self.emails_df[self.emails_df['label'] == 0].index.tolist()\n",
    "        self.phish_indices = self.emails_df[self.emails_df['label'] == 1].index.tolist()\n",
    "        if not (self.ham_indices and self.phish_indices):\n",
    "            raise ValueError(\"Dataset must contain examples of both classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def _get_random_email_idx(self, label):\n",
    "        indices = self.ham_indices if label == 0 else self.phish_indices\n",
    "        return np.random.choice(indices)\n",
    "\n",
    "    def _prepare_email_input(self, email):\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        tokenized_output = self.tokenizer(input_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        return tokenized_output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_email = self.emails_df.iloc[idx]\n",
    "        anchor_label = anchor_email['label']\n",
    "        positives, negatives = [], []\n",
    "\n",
    "        for _ in range(3):\n",
    "            pos_idx = self._get_random_email_idx(anchor_label)\n",
    "            neg_idx = self._get_random_email_idx(1 - anchor_label)\n",
    "            positives.append(self._prepare_email_input(self.emails_df.iloc[pos_idx]))\n",
    "            negatives.append(self._prepare_email_input(self.emails_df.iloc[neg_idx]))\n",
    "\n",
    "        anchor_inputs = self._prepare_email_input(anchor_email)\n",
    "        return {\n",
    "            'anchor_input_ids': anchor_inputs['input_ids'].squeeze(),\n",
    "            'anchor_attention_mask': anchor_inputs['attention_mask'].squeeze(),\n",
    "            'positive_input_ids': torch.stack([p['input_ids'].squeeze() for p in positives]),\n",
    "            'positive_attention_mask': torch.stack([p['attention_mask'].squeeze() for p in positives]),\n",
    "            'negative_input_ids': torch.stack([n['input_ids'].squeeze() for n in negatives]),\n",
    "            'negative_attention_mask': torch.stack([n['attention_mask'].squeeze() for n in negatives])\n",
    "        }\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, LlamaForSequenceClassification,AutoModelForSequenceClassification\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    # Use the fast tokenizer to avoid the slow/legacy mismatch\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)  \n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Load model config\n",
    "    model_config = AutoConfig.from_pretrained(model_name, num_labels=2, pad_token_id=tokenizer.pad_token_id, use_cache=False)\n",
    "\n",
    "    # Quantize base model\n",
    "    quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,             # if needed for this checkpoint\n",
    "    )\n",
    "\n",
    "    # Attach LoRA\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "    )\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9, margin=1.0, accumulation_steps=2):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    model = model.to(device).to(torch.float16)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                # Compute embeddings for anchor, positive, and negative samples in parallel\n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                # Reshape positive and negative embeddings to match the batch structure (batch_size, 3, embedding_dim)\n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "                # Calculate the triplet loss for each triplet in the batch\n",
    "                loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "                loss /= accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        val_metrics = evaluate_model(model, val_loader, device, margin)\n",
    "        print_metrics(epoch, total_loss, len(train_loader), val_metrics)\n",
    "\n",
    "        if val_metrics['val_loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['val_loss']\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model(model, val_loader, device, margin=1.0):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "               \n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                \n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "               \n",
    "                val_loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "    return {'val_loss': avg_val_loss}\n",
    "\n",
    "\n",
    "# Main function\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  \n",
    "        text = \"\"\n",
    "    \n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def print_metrics(epoch, total_loss, num_batches, val_metrics):\n",
    "    \n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "def main():\n",
    "    login(token=\"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\")\n",
    "    device = setup_environment()\n",
    "    model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/newdata_cleaned.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "\n",
    "   \n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "    train_dataset = ContrastiveEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = ContrastiveEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "    best_model_state = train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/contrastive_7B_DistillDeepSeek\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d46cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
