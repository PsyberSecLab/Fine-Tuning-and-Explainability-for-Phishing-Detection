{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5c499e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c99cf7d7b99423b92f94db8f708cf5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 0.7082\n",
      "Epoch 1, Step 10: Loss = 0.4468\n",
      "Epoch 1, Step 20: Loss = 0.7221\n",
      "Epoch 1, Step 30: Loss = 0.6960\n",
      "Epoch 1, Step 40: Loss = 0.5017\n",
      "Epoch 1, Step 50: Loss = 0.5816\n",
      "Epoch 1, Step 60: Loss = 0.7640\n",
      "Epoch 1, Step 70: Loss = 0.8839\n",
      "Epoch 1, Step 80: Loss = 0.6196\n",
      "Epoch 1, Step 90: Loss = 0.7089\n",
      "Epoch 1, Step 100: Loss = 0.5917\n",
      "Epoch 1, Step 110: Loss = 0.3528\n",
      "Epoch 1, Step 120: Loss = 0.8158\n",
      "Epoch 1, Step 130: Loss = 1.0465\n",
      "Epoch 1, Step 140: Loss = 0.6372\n",
      "Epoch 1, Step 150: Loss = 0.7199\n",
      "Epoch 1, Step 160: Loss = 0.4135\n",
      "Epoch 1, Step 170: Loss = 0.5287\n",
      "Epoch 1, Step 180: Loss = 0.8346\n",
      "Epoch 1, Step 190: Loss = 0.5695\n",
      "Epoch 1, Step 200: Loss = 0.6521\n",
      "Epoch 1, Step 210: Loss = 0.2859\n",
      "Epoch 1, Step 220: Loss = 0.4780\n",
      "Epoch 1, Step 230: Loss = 0.7337\n",
      "Epoch 1, Step 240: Loss = 0.6236\n",
      "Epoch 1, Step 250: Loss = 0.7992\n",
      "Epoch 1, Step 260: Loss = 0.7037\n",
      "Epoch 1, Step 270: Loss = 0.6601\n",
      "Epoch 1, Step 280: Loss = 0.5299\n",
      "Epoch 1, Step 290: Loss = 0.7128\n",
      "Epoch 1, Step 300: Loss = 0.5938\n",
      "Epoch 1, Step 310: Loss = 0.5378\n",
      "Epoch 1, Step 320: Loss = 0.2654\n",
      "Epoch 1, Step 330: Loss = 0.5118\n",
      "Epoch 1, Step 340: Loss = 0.2886\n",
      "Epoch 1, Step 350: Loss = 0.1754\n",
      "Epoch 1, Step 360: Loss = 0.3846\n",
      "Epoch 1, Step 370: Loss = 0.8480\n",
      "Epoch 1, Step 380: Loss = 0.1536\n",
      "Epoch 1, Step 390: Loss = 0.5252\n",
      "Epoch 1, Step 400: Loss = 0.3108\n",
      "Epoch 1, Step 410: Loss = 0.3910\n",
      "Epoch 1, Step 420: Loss = 0.3542\n",
      "Epoch 1, Step 430: Loss = 0.5061\n",
      "Epoch 1, Step 440: Loss = 0.2738\n",
      "Epoch 1, Step 450: Loss = 0.1672\n",
      "Epoch 1, Step 460: Loss = 0.5619\n",
      "Epoch 1, Step 470: Loss = 0.1773\n",
      "Epoch 1, Step 480: Loss = 0.3233\n",
      "Epoch 1, Step 490: Loss = 0.0504\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Average Training Loss: 1.0290\n",
      "Validation Metrics:\n",
      "Val_loss: 0.4752\n",
      "Epoch 2, Step 0: Loss = 0.6851\n",
      "Epoch 2, Step 10: Loss = 0.2214\n",
      "Epoch 2, Step 20: Loss = 0.4009\n",
      "Epoch 2, Step 30: Loss = 0.4570\n",
      "Epoch 2, Step 40: Loss = 0.2474\n",
      "Epoch 2, Step 50: Loss = 0.1100\n",
      "Epoch 2, Step 60: Loss = 0.0016\n",
      "Epoch 2, Step 70: Loss = 0.1388\n",
      "Epoch 2, Step 80: Loss = 0.1323\n",
      "Epoch 2, Step 90: Loss = 0.1206\n",
      "Epoch 2, Step 100: Loss = 0.2525\n",
      "Epoch 2, Step 110: Loss = 0.0181\n",
      "Epoch 2, Step 120: Loss = 0.0191\n",
      "Epoch 2, Step 130: Loss = 0.0490\n",
      "Epoch 2, Step 140: Loss = 0.1399\n",
      "Epoch 2, Step 150: Loss = 0.0645\n",
      "Epoch 2, Step 160: Loss = 0.0515\n",
      "Epoch 2, Step 170: Loss = 0.1595\n",
      "Epoch 2, Step 180: Loss = 0.2126\n",
      "Epoch 2, Step 190: Loss = 0.1286\n",
      "Epoch 2, Step 200: Loss = 0.2932\n",
      "Epoch 2, Step 210: Loss = 0.2023\n",
      "Epoch 2, Step 220: Loss = 0.1789\n",
      "Epoch 2, Step 230: Loss = 0.1579\n",
      "Epoch 2, Step 240: Loss = 0.0861\n",
      "Epoch 2, Step 250: Loss = 0.1879\n",
      "Epoch 2, Step 260: Loss = 0.1331\n",
      "Epoch 2, Step 270: Loss = 0.0586\n",
      "Epoch 2, Step 280: Loss = 0.0005\n",
      "Epoch 2, Step 290: Loss = 0.1103\n",
      "Epoch 2, Step 300: Loss = 0.1919\n",
      "Epoch 2, Step 310: Loss = 0.0597\n",
      "Epoch 2, Step 320: Loss = 0.3271\n",
      "Epoch 2, Step 330: Loss = 0.1832\n",
      "Epoch 2, Step 340: Loss = 0.0000\n",
      "Epoch 2, Step 350: Loss = 0.1302\n",
      "Epoch 2, Step 360: Loss = 0.0495\n",
      "Epoch 2, Step 370: Loss = 0.3410\n",
      "Epoch 2, Step 380: Loss = 0.0000\n",
      "Epoch 2, Step 390: Loss = 0.0770\n",
      "Epoch 2, Step 400: Loss = 0.1084\n",
      "Epoch 2, Step 410: Loss = 0.0228\n",
      "Epoch 2, Step 420: Loss = 0.0917\n",
      "Epoch 2, Step 430: Loss = 0.0000\n",
      "Epoch 2, Step 440: Loss = 0.0059\n",
      "Epoch 2, Step 450: Loss = 0.0000\n",
      "Epoch 2, Step 460: Loss = 0.0686\n",
      "Epoch 2, Step 470: Loss = 0.0422\n",
      "Epoch 2, Step 480: Loss = 0.0960\n",
      "Epoch 2, Step 490: Loss = 0.0272\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Average Training Loss: 0.3139\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2806\n",
      "Epoch 3, Step 0: Loss = 0.3488\n",
      "Epoch 3, Step 10: Loss = 0.2016\n",
      "Epoch 3, Step 20: Loss = 0.1012\n",
      "Epoch 3, Step 30: Loss = 0.2121\n",
      "Epoch 3, Step 40: Loss = 0.0933\n",
      "Epoch 3, Step 50: Loss = 0.0000\n",
      "Epoch 3, Step 60: Loss = 0.0075\n",
      "Epoch 3, Step 70: Loss = 0.0680\n",
      "Epoch 3, Step 80: Loss = 0.0557\n",
      "Epoch 3, Step 90: Loss = 0.0000\n",
      "Epoch 3, Step 100: Loss = 0.1534\n",
      "Epoch 3, Step 110: Loss = 0.0130\n",
      "Epoch 3, Step 120: Loss = 0.1619\n",
      "Epoch 3, Step 130: Loss = 0.0289\n",
      "Epoch 3, Step 140: Loss = 0.2245\n",
      "Epoch 3, Step 150: Loss = 0.0435\n",
      "Epoch 3, Step 160: Loss = 0.0186\n",
      "Epoch 3, Step 170: Loss = 0.1378\n",
      "Epoch 3, Step 180: Loss = 0.1691\n",
      "Epoch 3, Step 190: Loss = 0.0168\n",
      "Epoch 3, Step 200: Loss = 0.0000\n",
      "Epoch 3, Step 210: Loss = 0.4132\n",
      "Epoch 3, Step 220: Loss = 0.0000\n",
      "Epoch 3, Step 230: Loss = 0.0094\n",
      "Epoch 3, Step 240: Loss = 0.0856\n",
      "Epoch 3, Step 250: Loss = 0.0768\n",
      "Epoch 3, Step 260: Loss = 0.0039\n",
      "Epoch 3, Step 270: Loss = 0.0000\n",
      "Epoch 3, Step 280: Loss = 0.0502\n",
      "Epoch 3, Step 290: Loss = 0.0000\n",
      "Epoch 3, Step 300: Loss = 0.0000\n",
      "Epoch 3, Step 310: Loss = 0.0554\n",
      "Epoch 3, Step 320: Loss = 0.0768\n",
      "Epoch 3, Step 330: Loss = 0.0000\n",
      "Epoch 3, Step 340: Loss = 0.0051\n",
      "Epoch 3, Step 350: Loss = 0.0000\n",
      "Epoch 3, Step 360: Loss = 0.3308\n",
      "Epoch 3, Step 370: Loss = 0.3062\n",
      "Epoch 3, Step 380: Loss = 0.0451\n",
      "Epoch 3, Step 390: Loss = 0.0608\n",
      "Epoch 3, Step 400: Loss = 0.0000\n",
      "Epoch 3, Step 410: Loss = 0.1842\n",
      "Epoch 3, Step 420: Loss = 0.0000\n",
      "Epoch 3, Step 430: Loss = 0.1025\n",
      "Epoch 3, Step 440: Loss = 0.0452\n",
      "Epoch 3, Step 450: Loss = 0.0251\n",
      "Epoch 3, Step 460: Loss = 0.0556\n",
      "Epoch 3, Step 470: Loss = 0.2455\n",
      "Epoch 3, Step 480: Loss = 0.0147\n",
      "Epoch 3, Step 490: Loss = 0.0231\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Average Training Loss: 0.1939\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2061\n",
      "Epoch 4, Step 0: Loss = 0.0838\n",
      "Epoch 4, Step 10: Loss = 0.0902\n",
      "Epoch 4, Step 20: Loss = 0.4266\n",
      "Epoch 4, Step 30: Loss = 0.0000\n",
      "Epoch 4, Step 40: Loss = 0.0244\n",
      "Epoch 4, Step 50: Loss = 0.0560\n",
      "Epoch 4, Step 60: Loss = 0.0205\n",
      "Epoch 4, Step 70: Loss = 0.0000\n",
      "Epoch 4, Step 80: Loss = 0.1786\n",
      "Epoch 4, Step 90: Loss = 0.3192\n",
      "Epoch 4, Step 100: Loss = 0.0790\n",
      "Epoch 4, Step 110: Loss = 0.1571\n",
      "Epoch 4, Step 120: Loss = 0.0271\n",
      "Epoch 4, Step 130: Loss = 0.0288\n",
      "Epoch 4, Step 140: Loss = 0.1011\n",
      "Epoch 4, Step 150: Loss = 0.0099\n",
      "Epoch 4, Step 160: Loss = 0.0000\n",
      "Epoch 4, Step 170: Loss = 0.0299\n",
      "Epoch 4, Step 180: Loss = 0.0228\n",
      "Epoch 4, Step 190: Loss = 0.0912\n",
      "Epoch 4, Step 200: Loss = 0.1426\n",
      "Epoch 4, Step 210: Loss = 0.0000\n",
      "Epoch 4, Step 220: Loss = 0.0524\n",
      "Epoch 4, Step 230: Loss = 0.0000\n",
      "Epoch 4, Step 240: Loss = 0.0000\n",
      "Epoch 4, Step 250: Loss = 0.0000\n",
      "Epoch 4, Step 260: Loss = 0.0000\n",
      "Epoch 4, Step 270: Loss = 0.1753\n",
      "Epoch 4, Step 280: Loss = 0.1342\n",
      "Epoch 4, Step 290: Loss = 0.1637\n",
      "Epoch 4, Step 300: Loss = 0.0367\n",
      "Epoch 4, Step 310: Loss = 0.2511\n",
      "Epoch 4, Step 320: Loss = 0.0741\n",
      "Epoch 4, Step 330: Loss = 0.0293\n",
      "Epoch 4, Step 340: Loss = 0.0000\n",
      "Epoch 4, Step 350: Loss = 0.0753\n",
      "Epoch 4, Step 360: Loss = 0.0611\n",
      "Epoch 4, Step 370: Loss = 0.2832\n",
      "Epoch 4, Step 380: Loss = 0.1763\n",
      "Epoch 4, Step 390: Loss = 0.0890\n",
      "Epoch 4, Step 400: Loss = 0.0000\n",
      "Epoch 4, Step 410: Loss = 0.0000\n",
      "Epoch 4, Step 420: Loss = 0.0071\n",
      "Epoch 4, Step 430: Loss = 0.0858\n",
      "Epoch 4, Step 440: Loss = 0.0540\n",
      "Epoch 4, Step 450: Loss = 0.1820\n",
      "Epoch 4, Step 460: Loss = 0.0000\n",
      "Epoch 4, Step 470: Loss = 0.0000\n",
      "Epoch 4, Step 480: Loss = 0.2250\n",
      "Epoch 4, Step 490: Loss = 0.1131\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Average Training Loss: 0.1655\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1663\n",
      "Epoch 5, Step 0: Loss = 0.0000\n",
      "Epoch 5, Step 10: Loss = 0.0000\n",
      "Epoch 5, Step 20: Loss = 0.0886\n",
      "Epoch 5, Step 30: Loss = 0.0426\n",
      "Epoch 5, Step 40: Loss = 0.5222\n",
      "Epoch 5, Step 50: Loss = 0.0785\n",
      "Epoch 5, Step 60: Loss = 0.0624\n",
      "Epoch 5, Step 70: Loss = 0.0529\n",
      "Epoch 5, Step 80: Loss = 0.2127\n",
      "Epoch 5, Step 90: Loss = 0.1092\n",
      "Epoch 5, Step 100: Loss = 0.0000\n",
      "Epoch 5, Step 110: Loss = 0.0115\n",
      "Epoch 5, Step 120: Loss = 0.0508\n",
      "Epoch 5, Step 130: Loss = 0.1362\n",
      "Epoch 5, Step 140: Loss = 0.0106\n",
      "Epoch 5, Step 150: Loss = 0.0376\n",
      "Epoch 5, Step 160: Loss = 0.0630\n",
      "Epoch 5, Step 170: Loss = 0.2995\n",
      "Epoch 5, Step 180: Loss = 0.1800\n",
      "Epoch 5, Step 190: Loss = 0.0000\n",
      "Epoch 5, Step 200: Loss = 0.1137\n",
      "Epoch 5, Step 210: Loss = 0.0455\n",
      "Epoch 5, Step 220: Loss = 0.0054\n",
      "Epoch 5, Step 230: Loss = 0.0037\n",
      "Epoch 5, Step 240: Loss = 0.0573\n",
      "Epoch 5, Step 250: Loss = 0.0435\n",
      "Epoch 5, Step 260: Loss = 0.0000\n",
      "Epoch 5, Step 270: Loss = 0.1909\n",
      "Epoch 5, Step 280: Loss = 0.3206\n",
      "Epoch 5, Step 290: Loss = 0.0325\n",
      "Epoch 5, Step 300: Loss = 0.0000\n",
      "Epoch 5, Step 310: Loss = 0.0795\n",
      "Epoch 5, Step 320: Loss = 0.0466\n",
      "Epoch 5, Step 330: Loss = 0.0235\n",
      "Epoch 5, Step 340: Loss = 0.0000\n",
      "Epoch 5, Step 350: Loss = 0.0000\n",
      "Epoch 5, Step 360: Loss = 0.0148\n",
      "Epoch 5, Step 370: Loss = 0.0204\n",
      "Epoch 5, Step 380: Loss = 0.0000\n",
      "Epoch 5, Step 390: Loss = 0.0970\n",
      "Epoch 5, Step 400: Loss = 0.0000\n",
      "Epoch 5, Step 410: Loss = 0.1324\n",
      "Epoch 5, Step 420: Loss = 0.0000\n",
      "Epoch 5, Step 430: Loss = 0.0000\n",
      "Epoch 5, Step 440: Loss = 0.3959\n",
      "Epoch 5, Step 450: Loss = 0.0022\n",
      "Epoch 5, Step 460: Loss = 0.0447\n",
      "Epoch 5, Step 470: Loss = 0.1897\n",
      "Epoch 5, Step 480: Loss = 0.0000\n",
      "Epoch 5, Step 490: Loss = 0.0033\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Average Training Loss: 0.1416\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1423\n",
      "Epoch 6, Step 0: Loss = 0.1109\n",
      "Epoch 6, Step 10: Loss = 0.0631\n",
      "Epoch 6, Step 20: Loss = 0.0000\n",
      "Epoch 6, Step 30: Loss = 0.0582\n",
      "Epoch 6, Step 40: Loss = 0.2159\n",
      "Epoch 6, Step 50: Loss = 0.0667\n",
      "Epoch 6, Step 60: Loss = 0.0000\n",
      "Epoch 6, Step 70: Loss = 0.0000\n",
      "Epoch 6, Step 80: Loss = 0.0000\n",
      "Epoch 6, Step 90: Loss = 0.0000\n",
      "Epoch 6, Step 100: Loss = 0.2675\n",
      "Epoch 6, Step 110: Loss = 0.0000\n",
      "Epoch 6, Step 120: Loss = 0.0828\n",
      "Epoch 6, Step 130: Loss = 0.1232\n",
      "Epoch 6, Step 140: Loss = 0.2002\n",
      "Epoch 6, Step 150: Loss = 0.2924\n",
      "Epoch 6, Step 160: Loss = 0.0000\n",
      "Epoch 6, Step 170: Loss = 0.1376\n",
      "Epoch 6, Step 180: Loss = 0.1648\n",
      "Epoch 6, Step 190: Loss = 0.0297\n",
      "Epoch 6, Step 200: Loss = 0.0000\n",
      "Epoch 6, Step 210: Loss = 0.0219\n",
      "Epoch 6, Step 220: Loss = 0.0000\n",
      "Epoch 6, Step 230: Loss = 0.0089\n",
      "Epoch 6, Step 240: Loss = 0.0000\n",
      "Epoch 6, Step 250: Loss = 0.0071\n",
      "Epoch 6, Step 260: Loss = 0.0000\n",
      "Epoch 6, Step 270: Loss = 0.0954\n",
      "Epoch 6, Step 280: Loss = 0.0355\n",
      "Epoch 6, Step 290: Loss = 0.0984\n",
      "Epoch 6, Step 300: Loss = 0.2069\n",
      "Epoch 6, Step 310: Loss = 0.1076\n",
      "Epoch 6, Step 320: Loss = 0.0141\n",
      "Epoch 6, Step 330: Loss = 0.0541\n",
      "Epoch 6, Step 340: Loss = 0.0000\n",
      "Epoch 6, Step 350: Loss = 0.3596\n",
      "Epoch 6, Step 360: Loss = 0.0283\n",
      "Epoch 6, Step 370: Loss = 0.0000\n",
      "Epoch 6, Step 380: Loss = 0.0021\n",
      "Epoch 6, Step 390: Loss = 0.0213\n",
      "Epoch 6, Step 400: Loss = 0.0667\n",
      "Epoch 6, Step 410: Loss = 0.1179\n",
      "Epoch 6, Step 420: Loss = 0.0422\n",
      "Epoch 6, Step 430: Loss = 0.0369\n",
      "Epoch 6, Step 440: Loss = 0.0467\n",
      "Epoch 6, Step 450: Loss = 0.0562\n",
      "Epoch 6, Step 460: Loss = 0.0000\n",
      "Epoch 6, Step 470: Loss = 0.0000\n",
      "Epoch 6, Step 480: Loss = 0.0101\n",
      "Epoch 6, Step 490: Loss = 0.0462\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Average Training Loss: 0.1257\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1309\n",
      "Epoch 7, Step 0: Loss = 0.1863\n",
      "Epoch 7, Step 10: Loss = 0.0087\n",
      "Epoch 7, Step 20: Loss = 0.0178\n",
      "Epoch 7, Step 30: Loss = 0.0287\n",
      "Epoch 7, Step 40: Loss = 0.0159\n",
      "Epoch 7, Step 50: Loss = 0.4041\n",
      "Epoch 7, Step 60: Loss = 0.0000\n",
      "Epoch 7, Step 70: Loss = 0.0000\n",
      "Epoch 7, Step 80: Loss = 0.0136\n",
      "Epoch 7, Step 90: Loss = 0.0000\n",
      "Epoch 7, Step 100: Loss = 0.2056\n",
      "Epoch 7, Step 110: Loss = 0.0078\n",
      "Epoch 7, Step 120: Loss = 0.0000\n",
      "Epoch 7, Step 130: Loss = 0.2638\n",
      "Epoch 7, Step 140: Loss = 0.0053\n",
      "Epoch 7, Step 150: Loss = 0.0464\n",
      "Epoch 7, Step 160: Loss = 0.0879\n",
      "Epoch 7, Step 170: Loss = 0.0093\n",
      "Epoch 7, Step 180: Loss = 0.0354\n",
      "Epoch 7, Step 190: Loss = 0.0000\n",
      "Epoch 7, Step 200: Loss = 0.2214\n",
      "Epoch 7, Step 210: Loss = 0.0020\n",
      "Epoch 7, Step 220: Loss = 0.0453\n",
      "Epoch 7, Step 230: Loss = 0.0969\n",
      "Epoch 7, Step 240: Loss = 0.0251\n",
      "Epoch 7, Step 250: Loss = 0.0000\n",
      "Epoch 7, Step 260: Loss = 0.0049\n",
      "Epoch 7, Step 270: Loss = 0.4250\n",
      "Epoch 7, Step 280: Loss = 0.2867\n",
      "Epoch 7, Step 290: Loss = 0.0182\n",
      "Epoch 7, Step 300: Loss = 0.0670\n",
      "Epoch 7, Step 310: Loss = 0.0090\n",
      "Epoch 7, Step 320: Loss = 0.0869\n",
      "Epoch 7, Step 330: Loss = 0.0337\n",
      "Epoch 7, Step 340: Loss = 0.0895\n",
      "Epoch 7, Step 350: Loss = 0.0808\n",
      "Epoch 7, Step 360: Loss = 0.0000\n",
      "Epoch 7, Step 370: Loss = 0.0000\n",
      "Epoch 7, Step 380: Loss = 0.0026\n",
      "Epoch 7, Step 390: Loss = 0.0859\n",
      "Epoch 7, Step 400: Loss = 0.0000\n",
      "Epoch 7, Step 410: Loss = 0.0210\n",
      "Epoch 7, Step 420: Loss = 0.0343\n",
      "Epoch 7, Step 430: Loss = 0.0000\n",
      "Epoch 7, Step 440: Loss = 0.0041\n",
      "Epoch 7, Step 450: Loss = 0.0000\n",
      "Epoch 7, Step 460: Loss = 0.0561\n",
      "Epoch 7, Step 470: Loss = 0.0000\n",
      "Epoch 7, Step 480: Loss = 0.0191\n",
      "Epoch 7, Step 490: Loss = 0.0469\n",
      "\n",
      "Epoch 7 Summary:\n",
      "Average Training Loss: 0.1113\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1178\n",
      "Epoch 8, Step 0: Loss = 0.0116\n",
      "Epoch 8, Step 10: Loss = 0.0807\n",
      "Epoch 8, Step 20: Loss = 0.1081\n",
      "Epoch 8, Step 30: Loss = 0.0000\n",
      "Epoch 8, Step 40: Loss = 0.0340\n",
      "Epoch 8, Step 50: Loss = 0.0540\n",
      "Epoch 8, Step 60: Loss = 0.0658\n",
      "Epoch 8, Step 70: Loss = 0.0881\n",
      "Epoch 8, Step 80: Loss = 0.1413\n",
      "Epoch 8, Step 90: Loss = 0.0574\n",
      "Epoch 8, Step 100: Loss = 0.0000\n",
      "Epoch 8, Step 110: Loss = 0.0551\n",
      "Epoch 8, Step 120: Loss = 0.0000\n",
      "Epoch 8, Step 130: Loss = 0.0356\n",
      "Epoch 8, Step 140: Loss = 0.4138\n",
      "Epoch 8, Step 150: Loss = 0.0000\n",
      "Epoch 8, Step 160: Loss = 0.0363\n",
      "Epoch 8, Step 170: Loss = 0.0000\n",
      "Epoch 8, Step 180: Loss = 0.0000\n",
      "Epoch 8, Step 190: Loss = 0.0582\n",
      "Epoch 8, Step 200: Loss = 0.0115\n",
      "Epoch 8, Step 210: Loss = 0.0000\n",
      "Epoch 8, Step 220: Loss = 0.0204\n",
      "Epoch 8, Step 230: Loss = 0.0583\n",
      "Epoch 8, Step 240: Loss = 0.0000\n",
      "Epoch 8, Step 250: Loss = 0.0058\n",
      "Epoch 8, Step 260: Loss = 0.1418\n",
      "Epoch 8, Step 270: Loss = 0.0000\n",
      "Epoch 8, Step 280: Loss = 0.0868\n",
      "Epoch 8, Step 290: Loss = 0.0419\n",
      "Epoch 8, Step 300: Loss = 0.1615\n",
      "Epoch 8, Step 310: Loss = 0.0069\n",
      "Epoch 8, Step 320: Loss = 0.0426\n",
      "Epoch 8, Step 330: Loss = 0.0000\n",
      "Epoch 8, Step 340: Loss = 0.0000\n",
      "Epoch 8, Step 350: Loss = 0.0000\n",
      "Epoch 8, Step 360: Loss = 0.0000\n",
      "Epoch 8, Step 370: Loss = 0.0485\n",
      "Epoch 8, Step 380: Loss = 0.1021\n",
      "Epoch 8, Step 390: Loss = 0.0000\n",
      "Epoch 8, Step 400: Loss = 0.0937\n",
      "Epoch 8, Step 410: Loss = 0.0056\n",
      "Epoch 8, Step 420: Loss = 0.1488\n",
      "Epoch 8, Step 430: Loss = 0.0000\n",
      "Epoch 8, Step 440: Loss = 0.0099\n",
      "Epoch 8, Step 450: Loss = 0.1102\n",
      "Epoch 8, Step 460: Loss = 0.0719\n",
      "Epoch 8, Step 470: Loss = 0.0076\n",
      "Epoch 8, Step 480: Loss = 0.1489\n",
      "Epoch 8, Step 490: Loss = 0.0322\n",
      "\n",
      "Epoch 8 Summary:\n",
      "Average Training Loss: 0.1079\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1166\n",
      "Epoch 9, Step 0: Loss = 0.0806\n",
      "Epoch 9, Step 10: Loss = 0.0559\n",
      "Epoch 9, Step 20: Loss = 0.0071\n",
      "Epoch 9, Step 30: Loss = 0.1157\n",
      "Epoch 9, Step 40: Loss = 0.1978\n",
      "Epoch 9, Step 50: Loss = 0.0000\n",
      "Epoch 9, Step 60: Loss = 0.0378\n",
      "Epoch 9, Step 70: Loss = 0.0271\n",
      "Epoch 9, Step 80: Loss = 0.0520\n",
      "Epoch 9, Step 90: Loss = 0.0009\n",
      "Epoch 9, Step 100: Loss = 0.0000\n",
      "Epoch 9, Step 110: Loss = 0.0000\n",
      "Epoch 9, Step 120: Loss = 0.0000\n",
      "Epoch 9, Step 130: Loss = 0.2533\n",
      "Epoch 9, Step 140: Loss = 0.1582\n",
      "Epoch 9, Step 150: Loss = 0.0000\n",
      "Epoch 9, Step 160: Loss = 0.0039\n",
      "Epoch 9, Step 170: Loss = 0.0040\n",
      "Epoch 9, Step 180: Loss = 0.0224\n",
      "Epoch 9, Step 190: Loss = 0.2786\n",
      "Epoch 9, Step 200: Loss = 0.0407\n",
      "Epoch 9, Step 210: Loss = 0.0304\n",
      "Epoch 9, Step 220: Loss = 0.0000\n",
      "Epoch 9, Step 230: Loss = 0.0000\n",
      "Epoch 9, Step 240: Loss = 0.0600\n",
      "Epoch 9, Step 250: Loss = 0.0000\n",
      "Epoch 9, Step 260: Loss = 0.0049\n",
      "Epoch 9, Step 270: Loss = 0.0000\n",
      "Epoch 9, Step 280: Loss = 0.0844\n",
      "Epoch 9, Step 290: Loss = 0.0000\n",
      "Epoch 9, Step 300: Loss = 0.0762\n",
      "Epoch 9, Step 310: Loss = 0.0000\n",
      "Epoch 9, Step 320: Loss = 0.0207\n",
      "Epoch 9, Step 330: Loss = 0.0000\n",
      "Epoch 9, Step 340: Loss = 0.0954\n",
      "Epoch 9, Step 350: Loss = 0.0319\n",
      "Epoch 9, Step 360: Loss = 0.0471\n",
      "Epoch 9, Step 370: Loss = 0.1177\n",
      "Epoch 9, Step 380: Loss = 0.0155\n",
      "Epoch 9, Step 390: Loss = 0.0994\n",
      "Epoch 9, Step 400: Loss = 0.0159\n",
      "Epoch 9, Step 410: Loss = 0.0000\n",
      "Epoch 9, Step 420: Loss = 0.0755\n",
      "Epoch 9, Step 430: Loss = 0.0934\n",
      "Epoch 9, Step 440: Loss = 0.0000\n",
      "Epoch 9, Step 450: Loss = 0.0000\n",
      "Epoch 9, Step 460: Loss = 0.0566\n",
      "Epoch 9, Step 470: Loss = 0.0000\n",
      "Epoch 9, Step 480: Loss = 0.3168\n",
      "Epoch 9, Step 490: Loss = 0.0000\n",
      "\n",
      "Epoch 9 Summary:\n",
      "Average Training Loss: 0.1015\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1176\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    torch.cuda.set_device(0)  \n",
    "    return torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataset class with data cleaning\n",
    "class ContrastiveEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        # Clean each text field\n",
    "        emails_df['sender'] = emails_df['sender'].apply(clean_text)\n",
    "        emails_df['subject'] = emails_df['subject'].apply(clean_text)\n",
    "        emails_df['body'] = emails_df['body'].apply(clean_text)\n",
    "        \n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.ham_indices = self.emails_df[self.emails_df['label'] == 0].index.tolist()\n",
    "        self.phish_indices = self.emails_df[self.emails_df['label'] == 1].index.tolist()\n",
    "        if not (self.ham_indices and self.phish_indices):\n",
    "            raise ValueError(\"Dataset must contain examples of both classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def _get_random_email_idx(self, label):\n",
    "        indices = self.ham_indices if label == 0 else self.phish_indices\n",
    "        return np.random.choice(indices)\n",
    "\n",
    "    def _prepare_email_input(self, email):\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        tokenized_output = self.tokenizer(input_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        return tokenized_output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_email = self.emails_df.iloc[idx]\n",
    "        anchor_label = anchor_email['label']\n",
    "        positives, negatives = [], []\n",
    "\n",
    "        for _ in range(3):\n",
    "            pos_idx = self._get_random_email_idx(anchor_label)\n",
    "            neg_idx = self._get_random_email_idx(1 - anchor_label)\n",
    "            positives.append(self._prepare_email_input(self.emails_df.iloc[pos_idx]))\n",
    "            negatives.append(self._prepare_email_input(self.emails_df.iloc[neg_idx]))\n",
    "\n",
    "        anchor_inputs = self._prepare_email_input(anchor_email)\n",
    "        return {\n",
    "            'anchor_input_ids': anchor_inputs['input_ids'].squeeze(),\n",
    "            'anchor_attention_mask': anchor_inputs['attention_mask'].squeeze(),\n",
    "            'positive_input_ids': torch.stack([p['input_ids'].squeeze() for p in positives]),\n",
    "            'positive_attention_mask': torch.stack([p['attention_mask'].squeeze() for p in positives]),\n",
    "            'negative_input_ids': torch.stack([n['input_ids'].squeeze() for n in negatives]),\n",
    "            'negative_attention_mask': torch.stack([n['attention_mask'].squeeze() for n in negatives])\n",
    "        }\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model_config.use_cache = False\n",
    "\n",
    "   \n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        config=model_config, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"SEQ_CLS\")\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9, margin=1.0, accumulation_steps=2):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    model = model.to(device).to(torch.float16)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                # Compute embeddings for anchor, positive, and negative samples in parallel\n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                # Reshape positive and negative embeddings to match the batch structure (batch_size, 3, embedding_dim)\n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "                # Calculate the triplet loss for each triplet in the batch\n",
    "                loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "                loss /= accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        val_metrics = evaluate_model(model, val_loader, device, margin)\n",
    "        print_metrics(epoch, total_loss, len(train_loader), val_metrics)\n",
    "\n",
    "        if val_metrics['val_loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['val_loss']\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model(model, val_loader, device, margin=1.0):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "               \n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                \n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "               \n",
    "                val_loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "    return {'val_loss': avg_val_loss}\n",
    "\n",
    "\n",
    "# Main function\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  \n",
    "        text = \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def print_metrics(epoch, total_loss, num_batches, val_metrics):\n",
    "    \n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "def main():\n",
    "    \n",
    "    device = setup_environment()\n",
    "    model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/final_data.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "\n",
    "   \n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "    train_dataset = ContrastiveEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = ContrastiveEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "    best_model_state = train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/llama_7B_contrastive_classification_model_lora\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad7d7d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990fbc7bfc2944e39ad8c1e0adca1b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 0.4848\n",
      "Epoch 1, Step 10: Loss = 0.3876\n",
      "Epoch 1, Step 20: Loss = 0.4532\n",
      "Epoch 1, Step 30: Loss = 0.4432\n",
      "Epoch 1, Step 40: Loss = 0.4758\n",
      "Epoch 1, Step 50: Loss = 1.0357\n",
      "Epoch 1, Step 60: Loss = 0.6788\n",
      "Epoch 1, Step 70: Loss = 0.5073\n",
      "Epoch 1, Step 80: Loss = 0.7307\n",
      "Epoch 1, Step 90: Loss = 0.9666\n",
      "Epoch 1, Step 100: Loss = 0.7170\n",
      "Epoch 1, Step 110: Loss = 1.2472\n",
      "Epoch 1, Step 120: Loss = 0.6567\n",
      "Epoch 1, Step 130: Loss = 0.4748\n",
      "Epoch 1, Step 140: Loss = 0.4824\n",
      "Epoch 1, Step 150: Loss = 0.7874\n",
      "Epoch 1, Step 160: Loss = 0.9680\n",
      "Epoch 1, Step 170: Loss = 0.2350\n",
      "Epoch 1, Step 180: Loss = 0.4143\n",
      "Epoch 1, Step 190: Loss = 0.5848\n",
      "Epoch 1, Step 200: Loss = 0.8162\n",
      "Epoch 1, Step 210: Loss = 0.6087\n",
      "Epoch 1, Step 220: Loss = 0.2629\n",
      "Epoch 1, Step 230: Loss = 0.4542\n",
      "Epoch 1, Step 240: Loss = 0.5835\n",
      "Epoch 1, Step 250: Loss = 0.3362\n",
      "Epoch 1, Step 260: Loss = 0.5486\n",
      "Epoch 1, Step 270: Loss = 0.7951\n",
      "Epoch 1, Step 280: Loss = 0.6721\n",
      "Epoch 1, Step 290: Loss = 0.0943\n",
      "Epoch 1, Step 300: Loss = 0.5878\n",
      "Epoch 1, Step 310: Loss = 0.7058\n",
      "Epoch 1, Step 320: Loss = 0.3119\n",
      "Epoch 1, Step 330: Loss = 0.6012\n",
      "Epoch 1, Step 340: Loss = 0.6326\n",
      "Epoch 1, Step 350: Loss = 0.5621\n",
      "Epoch 1, Step 360: Loss = 0.2298\n",
      "Epoch 1, Step 370: Loss = 0.4004\n",
      "Epoch 1, Step 380: Loss = 0.5852\n",
      "Epoch 1, Step 390: Loss = 0.2530\n",
      "Epoch 1, Step 400: Loss = 0.2093\n",
      "Epoch 1, Step 410: Loss = 0.1821\n",
      "Epoch 1, Step 420: Loss = 0.1743\n",
      "Epoch 1, Step 430: Loss = 0.2689\n",
      "Epoch 1, Step 440: Loss = 0.3118\n",
      "Epoch 1, Step 450: Loss = 0.3554\n",
      "Epoch 1, Step 460: Loss = 0.4732\n",
      "Epoch 1, Step 470: Loss = 0.4248\n",
      "Epoch 1, Step 480: Loss = 0.0985\n",
      "Epoch 1, Step 490: Loss = 0.0200\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Average Training Loss: 1.1307\n",
      "Validation Metrics:\n",
      "Val_loss: 0.4770\n",
      "Epoch 2, Step 0: Loss = 0.2572\n",
      "Epoch 2, Step 10: Loss = 0.0856\n",
      "Epoch 2, Step 20: Loss = 0.1335\n",
      "Epoch 2, Step 30: Loss = 0.1900\n",
      "Epoch 2, Step 40: Loss = 0.0974\n",
      "Epoch 2, Step 50: Loss = 0.3712\n",
      "Epoch 2, Step 60: Loss = 0.1676\n",
      "Epoch 2, Step 70: Loss = 0.0853\n",
      "Epoch 2, Step 80: Loss = 0.1577\n",
      "Epoch 2, Step 90: Loss = 0.7640\n",
      "Epoch 2, Step 100: Loss = 0.3816\n",
      "Epoch 2, Step 110: Loss = 0.0000\n",
      "Epoch 2, Step 120: Loss = 0.0896\n",
      "Epoch 2, Step 130: Loss = 0.2988\n",
      "Epoch 2, Step 140: Loss = 0.2648\n",
      "Epoch 2, Step 150: Loss = 0.2956\n",
      "Epoch 2, Step 160: Loss = 0.3394\n",
      "Epoch 2, Step 170: Loss = 0.1231\n",
      "Epoch 2, Step 180: Loss = 0.0000\n",
      "Epoch 2, Step 190: Loss = 0.2146\n",
      "Epoch 2, Step 200: Loss = 0.0547\n",
      "Epoch 2, Step 210: Loss = 0.1700\n",
      "Epoch 2, Step 220: Loss = 0.3455\n",
      "Epoch 2, Step 230: Loss = 0.4203\n",
      "Epoch 2, Step 240: Loss = 0.0736\n",
      "Epoch 2, Step 250: Loss = 0.1711\n",
      "Epoch 2, Step 260: Loss = 0.3043\n",
      "Epoch 2, Step 270: Loss = 0.3508\n",
      "Epoch 2, Step 280: Loss = 0.1253\n",
      "Epoch 2, Step 290: Loss = 0.0239\n",
      "Epoch 2, Step 300: Loss = 0.0021\n",
      "Epoch 2, Step 310: Loss = 0.3391\n",
      "Epoch 2, Step 320: Loss = 0.6215\n",
      "Epoch 2, Step 330: Loss = 0.1083\n",
      "Epoch 2, Step 340: Loss = 0.2915\n",
      "Epoch 2, Step 350: Loss = 0.5473\n",
      "Epoch 2, Step 360: Loss = 0.0591\n",
      "Epoch 2, Step 370: Loss = 0.0142\n",
      "Epoch 2, Step 380: Loss = 0.0000\n",
      "Epoch 2, Step 390: Loss = 0.0063\n",
      "Epoch 2, Step 400: Loss = 0.2856\n",
      "Epoch 2, Step 410: Loss = 0.0000\n",
      "Epoch 2, Step 420: Loss = 0.0081\n",
      "Epoch 2, Step 430: Loss = 0.0518\n",
      "Epoch 2, Step 440: Loss = 0.0000\n",
      "Epoch 2, Step 450: Loss = 0.0205\n",
      "Epoch 2, Step 460: Loss = 0.0000\n",
      "Epoch 2, Step 470: Loss = 0.0012\n",
      "Epoch 2, Step 480: Loss = 0.0000\n",
      "Epoch 2, Step 490: Loss = 0.0257\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Average Training Loss: 0.3429\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2098\n",
      "Epoch 3, Step 0: Loss = 0.1203\n",
      "Epoch 3, Step 10: Loss = 0.0221\n",
      "Epoch 3, Step 20: Loss = 0.0001\n",
      "Epoch 3, Step 30: Loss = 0.1329\n",
      "Epoch 3, Step 40: Loss = 0.0543\n",
      "Epoch 3, Step 50: Loss = 0.0619\n",
      "Epoch 3, Step 60: Loss = 0.3187\n",
      "Epoch 3, Step 70: Loss = 0.0728\n",
      "Epoch 3, Step 80: Loss = 0.0010\n",
      "Epoch 3, Step 90: Loss = 0.1116\n",
      "Epoch 3, Step 100: Loss = 0.0000\n",
      "Epoch 3, Step 110: Loss = 0.0046\n",
      "Epoch 3, Step 120: Loss = 0.0596\n",
      "Epoch 3, Step 130: Loss = 0.0012\n",
      "Epoch 3, Step 140: Loss = 0.0111\n",
      "Epoch 3, Step 150: Loss = 0.0941\n",
      "Epoch 3, Step 160: Loss = 0.2125\n",
      "Epoch 3, Step 170: Loss = 0.0000\n",
      "Epoch 3, Step 180: Loss = 0.0044\n",
      "Epoch 3, Step 190: Loss = 0.0181\n",
      "Epoch 3, Step 200: Loss = 0.0047\n",
      "Epoch 3, Step 210: Loss = 0.0000\n",
      "Epoch 3, Step 220: Loss = 0.1364\n",
      "Epoch 3, Step 230: Loss = 0.3205\n",
      "Epoch 3, Step 240: Loss = 0.0000\n",
      "Epoch 3, Step 250: Loss = 0.0197\n",
      "Epoch 3, Step 260: Loss = 0.1147\n",
      "Epoch 3, Step 270: Loss = 0.0000\n",
      "Epoch 3, Step 280: Loss = 0.1988\n",
      "Epoch 3, Step 290: Loss = 0.0000\n",
      "Epoch 3, Step 300: Loss = 0.1568\n",
      "Epoch 3, Step 310: Loss = 0.0000\n",
      "Epoch 3, Step 320: Loss = 0.0000\n",
      "Epoch 3, Step 330: Loss = 0.0617\n",
      "Epoch 3, Step 340: Loss = 0.0743\n",
      "Epoch 3, Step 350: Loss = 0.5749\n",
      "Epoch 3, Step 360: Loss = 0.0763\n",
      "Epoch 3, Step 370: Loss = 0.0591\n",
      "Epoch 3, Step 380: Loss = 0.0194\n",
      "Epoch 3, Step 390: Loss = 0.1967\n",
      "Epoch 3, Step 400: Loss = 0.0279\n",
      "Epoch 3, Step 410: Loss = 0.2072\n",
      "Epoch 3, Step 420: Loss = 0.2243\n",
      "Epoch 3, Step 430: Loss = 0.0000\n",
      "Epoch 3, Step 440: Loss = 0.0000\n",
      "Epoch 3, Step 450: Loss = 0.0061\n",
      "Epoch 3, Step 460: Loss = 0.0665\n",
      "Epoch 3, Step 470: Loss = 0.0000\n",
      "Epoch 3, Step 480: Loss = 0.1570\n",
      "Epoch 3, Step 490: Loss = 0.0000\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Average Training Loss: 0.1814\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1499\n",
      "Epoch 4, Step 0: Loss = 0.3666\n",
      "Epoch 4, Step 10: Loss = 0.1884\n",
      "Epoch 4, Step 20: Loss = 0.0000\n",
      "Epoch 4, Step 30: Loss = 0.1665\n",
      "Epoch 4, Step 40: Loss = 0.0180\n",
      "Epoch 4, Step 50: Loss = 0.0889\n",
      "Epoch 4, Step 60: Loss = 0.0000\n",
      "Epoch 4, Step 70: Loss = 0.0000\n",
      "Epoch 4, Step 80: Loss = 0.1458\n",
      "Epoch 4, Step 90: Loss = 0.0652\n",
      "Epoch 4, Step 100: Loss = 0.0239\n",
      "Epoch 4, Step 110: Loss = 0.0369\n",
      "Epoch 4, Step 120: Loss = 0.0000\n",
      "Epoch 4, Step 130: Loss = 0.0000\n",
      "Epoch 4, Step 140: Loss = 0.2219\n",
      "Epoch 4, Step 150: Loss = 0.1147\n",
      "Epoch 4, Step 160: Loss = 0.0620\n",
      "Epoch 4, Step 170: Loss = 0.0640\n",
      "Epoch 4, Step 180: Loss = 0.0000\n",
      "Epoch 4, Step 190: Loss = 0.0272\n",
      "Epoch 4, Step 200: Loss = 0.1130\n",
      "Epoch 4, Step 210: Loss = 0.0095\n",
      "Epoch 4, Step 220: Loss = 0.0000\n",
      "Epoch 4, Step 230: Loss = 0.0361\n",
      "Epoch 4, Step 240: Loss = 0.1666\n",
      "Epoch 4, Step 250: Loss = 0.0000\n",
      "Epoch 4, Step 260: Loss = 0.3509\n",
      "Epoch 4, Step 270: Loss = 0.1020\n",
      "Epoch 4, Step 280: Loss = 0.0234\n",
      "Epoch 4, Step 290: Loss = 0.2845\n",
      "Epoch 4, Step 300: Loss = 0.0134\n",
      "Epoch 4, Step 310: Loss = 0.0063\n",
      "Epoch 4, Step 320: Loss = 0.0000\n",
      "Epoch 4, Step 330: Loss = 0.3623\n",
      "Epoch 4, Step 340: Loss = 0.0000\n",
      "Epoch 4, Step 350: Loss = 0.1195\n",
      "Epoch 4, Step 360: Loss = 0.0340\n",
      "Epoch 4, Step 370: Loss = 0.0289\n",
      "Epoch 4, Step 380: Loss = 0.0927\n",
      "Epoch 4, Step 390: Loss = 0.0191\n",
      "Epoch 4, Step 400: Loss = 0.0177\n",
      "Epoch 4, Step 410: Loss = 0.0797\n",
      "Epoch 4, Step 420: Loss = 0.4425\n",
      "Epoch 4, Step 430: Loss = 0.2888\n",
      "Epoch 4, Step 440: Loss = 0.0000\n",
      "Epoch 4, Step 450: Loss = 0.1397\n",
      "Epoch 4, Step 460: Loss = 0.0000\n",
      "Epoch 4, Step 470: Loss = 0.0000\n",
      "Epoch 4, Step 480: Loss = 0.0000\n",
      "Epoch 4, Step 490: Loss = 0.2730\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Average Training Loss: 0.1422\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1266\n",
      "Epoch 5, Step 0: Loss = 0.3169\n",
      "Epoch 5, Step 10: Loss = 0.0000\n",
      "Epoch 5, Step 20: Loss = 0.2794\n",
      "Epoch 5, Step 30: Loss = 0.0000\n",
      "Epoch 5, Step 40: Loss = 0.0673\n",
      "Epoch 5, Step 50: Loss = 0.0000\n",
      "Epoch 5, Step 60: Loss = 0.0000\n",
      "Epoch 5, Step 70: Loss = 0.0000\n",
      "Epoch 5, Step 80: Loss = 0.0855\n",
      "Epoch 5, Step 90: Loss = 0.1876\n",
      "Epoch 5, Step 100: Loss = 0.0000\n",
      "Epoch 5, Step 110: Loss = 0.0000\n",
      "Epoch 5, Step 120: Loss = 0.0000\n",
      "Epoch 5, Step 130: Loss = 0.0016\n",
      "Epoch 5, Step 140: Loss = 0.0000\n",
      "Epoch 5, Step 150: Loss = 0.0000\n",
      "Epoch 5, Step 160: Loss = 0.0000\n",
      "Epoch 5, Step 170: Loss = 0.0000\n",
      "Epoch 5, Step 180: Loss = 0.0692\n",
      "Epoch 5, Step 190: Loss = 0.0192\n",
      "Epoch 5, Step 200: Loss = 0.0085\n",
      "Epoch 5, Step 210: Loss = 0.0000\n",
      "Epoch 5, Step 220: Loss = 0.0000\n",
      "Epoch 5, Step 230: Loss = 0.0000\n",
      "Epoch 5, Step 240: Loss = 0.0000\n",
      "Epoch 5, Step 250: Loss = 0.0794\n",
      "Epoch 5, Step 260: Loss = 0.0777\n",
      "Epoch 5, Step 270: Loss = 0.0000\n",
      "Epoch 5, Step 280: Loss = 0.0000\n",
      "Epoch 5, Step 290: Loss = 0.0566\n",
      "Epoch 5, Step 300: Loss = 0.1372\n",
      "Epoch 5, Step 310: Loss = 0.0224\n",
      "Epoch 5, Step 320: Loss = 0.0000\n",
      "Epoch 5, Step 330: Loss = 0.0062\n",
      "Epoch 5, Step 340: Loss = 0.0898\n",
      "Epoch 5, Step 350: Loss = 0.0742\n",
      "Epoch 5, Step 360: Loss = 0.0477\n",
      "Epoch 5, Step 370: Loss = 0.0000\n",
      "Epoch 5, Step 380: Loss = 0.0552\n",
      "Epoch 5, Step 390: Loss = 0.0347\n",
      "Epoch 5, Step 400: Loss = 0.1382\n",
      "Epoch 5, Step 410: Loss = 0.0000\n",
      "Epoch 5, Step 420: Loss = 0.2756\n",
      "Epoch 5, Step 430: Loss = 0.0000\n",
      "Epoch 5, Step 440: Loss = 0.3392\n",
      "Epoch 5, Step 450: Loss = 0.1175\n",
      "Epoch 5, Step 460: Loss = 0.0000\n",
      "Epoch 5, Step 470: Loss = 0.0000\n",
      "Epoch 5, Step 480: Loss = 0.0381\n",
      "Epoch 5, Step 490: Loss = 0.0035\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Average Training Loss: 0.0983\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1093\n",
      "Epoch 6, Step 0: Loss = 0.0000\n",
      "Epoch 6, Step 10: Loss = 0.0583\n",
      "Epoch 6, Step 20: Loss = 0.0590\n",
      "Epoch 6, Step 30: Loss = 0.0000\n",
      "Epoch 6, Step 40: Loss = 0.0000\n",
      "Epoch 6, Step 50: Loss = 0.0264\n",
      "Epoch 6, Step 60: Loss = 0.0000\n",
      "Epoch 6, Step 70: Loss = 0.0000\n",
      "Epoch 6, Step 80: Loss = 0.0281\n",
      "Epoch 6, Step 90: Loss = 0.0000\n",
      "Epoch 6, Step 100: Loss = 0.0740\n",
      "Epoch 6, Step 110: Loss = 0.0357\n",
      "Epoch 6, Step 120: Loss = 0.2503\n",
      "Epoch 6, Step 130: Loss = 0.0937\n",
      "Epoch 6, Step 140: Loss = 0.0000\n",
      "Epoch 6, Step 150: Loss = 0.0000\n",
      "Epoch 6, Step 160: Loss = 0.0000\n",
      "Epoch 6, Step 170: Loss = 0.0000\n",
      "Epoch 6, Step 180: Loss = 0.0408\n",
      "Epoch 6, Step 190: Loss = 0.0000\n",
      "Epoch 6, Step 200: Loss = 0.1385\n",
      "Epoch 6, Step 210: Loss = 0.0000\n",
      "Epoch 6, Step 220: Loss = 0.0000\n",
      "Epoch 6, Step 230: Loss = 0.0498\n",
      "Epoch 6, Step 240: Loss = 0.0797\n",
      "Epoch 6, Step 250: Loss = 0.0951\n",
      "Epoch 6, Step 260: Loss = 0.2057\n",
      "Epoch 6, Step 270: Loss = 0.0000\n",
      "Epoch 6, Step 280: Loss = 0.0000\n",
      "Epoch 6, Step 290: Loss = 0.1505\n",
      "Epoch 6, Step 300: Loss = 0.0920\n",
      "Epoch 6, Step 310: Loss = 0.0103\n",
      "Epoch 6, Step 320: Loss = 0.0061\n",
      "Epoch 6, Step 330: Loss = 0.1283\n",
      "Epoch 6, Step 340: Loss = 0.0000\n",
      "Epoch 6, Step 350: Loss = 0.0000\n",
      "Epoch 6, Step 360: Loss = 0.0000\n",
      "Epoch 6, Step 370: Loss = 0.2077\n",
      "Epoch 6, Step 380: Loss = 0.1045\n",
      "Epoch 6, Step 390: Loss = 0.0000\n",
      "Epoch 6, Step 400: Loss = 0.0000\n",
      "Epoch 6, Step 410: Loss = 0.0000\n",
      "Epoch 6, Step 420: Loss = 0.0000\n",
      "Epoch 6, Step 430: Loss = 0.0727\n",
      "Epoch 6, Step 440: Loss = 0.1802\n",
      "Epoch 6, Step 450: Loss = 0.0123\n",
      "Epoch 6, Step 460: Loss = 0.0112\n",
      "Epoch 6, Step 470: Loss = 0.0227\n",
      "Epoch 6, Step 480: Loss = 0.0935\n",
      "Epoch 6, Step 490: Loss = 0.0000\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Average Training Loss: 0.0890\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1170\n",
      "Epoch 7, Step 0: Loss = 0.1014\n",
      "Epoch 7, Step 10: Loss = 0.0000\n",
      "Epoch 7, Step 20: Loss = 0.0000\n",
      "Epoch 7, Step 30: Loss = 0.0018\n",
      "Epoch 7, Step 40: Loss = 0.0151\n",
      "Epoch 7, Step 50: Loss = 0.0718\n",
      "Epoch 7, Step 60: Loss = 0.0000\n",
      "Epoch 7, Step 70: Loss = 0.0000\n",
      "Epoch 7, Step 80: Loss = 0.0000\n",
      "Epoch 7, Step 90: Loss = 0.0000\n",
      "Epoch 7, Step 100: Loss = 0.0000\n",
      "Epoch 7, Step 110: Loss = 0.0385\n",
      "Epoch 7, Step 120: Loss = 0.0000\n",
      "Epoch 7, Step 130: Loss = 0.0000\n",
      "Epoch 7, Step 140: Loss = 0.0217\n",
      "Epoch 7, Step 150: Loss = 0.0000\n",
      "Epoch 7, Step 160: Loss = 0.0000\n",
      "Epoch 7, Step 170: Loss = 0.1303\n",
      "Epoch 7, Step 180: Loss = 0.0743\n",
      "Epoch 7, Step 190: Loss = 0.0000\n",
      "Epoch 7, Step 200: Loss = 0.0000\n",
      "Epoch 7, Step 210: Loss = 0.0000\n",
      "Epoch 7, Step 220: Loss = 0.0000\n",
      "Epoch 7, Step 230: Loss = 0.0550\n",
      "Epoch 7, Step 240: Loss = 0.0000\n",
      "Epoch 7, Step 250: Loss = 0.0581\n",
      "Epoch 7, Step 260: Loss = 0.1304\n",
      "Epoch 7, Step 270: Loss = 0.0000\n",
      "Epoch 7, Step 280: Loss = 0.0000\n",
      "Epoch 7, Step 290: Loss = 0.0000\n",
      "Epoch 7, Step 300: Loss = 0.0851\n",
      "Epoch 7, Step 310: Loss = 0.0000\n",
      "Epoch 7, Step 320: Loss = 0.0000\n",
      "Epoch 7, Step 330: Loss = 0.0167\n",
      "Epoch 7, Step 340: Loss = 0.1637\n",
      "Epoch 7, Step 350: Loss = 0.0000\n",
      "Epoch 7, Step 360: Loss = 0.0000\n",
      "Epoch 7, Step 370: Loss = 0.0092\n",
      "Epoch 7, Step 380: Loss = 0.0927\n",
      "Epoch 7, Step 390: Loss = 0.0000\n",
      "Epoch 7, Step 400: Loss = 0.0000\n",
      "Epoch 7, Step 410: Loss = 0.0235\n",
      "Epoch 7, Step 420: Loss = 0.0000\n",
      "Epoch 7, Step 430: Loss = 0.0754\n",
      "Epoch 7, Step 440: Loss = 0.0000\n",
      "Epoch 7, Step 450: Loss = 0.0000\n",
      "Epoch 7, Step 460: Loss = 0.4385\n",
      "Epoch 7, Step 470: Loss = 0.0493\n",
      "Epoch 7, Step 480: Loss = 0.0000\n",
      "Epoch 7, Step 490: Loss = 0.0000\n",
      "\n",
      "Epoch 7 Summary:\n",
      "Average Training Loss: 0.0768\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1130\n",
      "Epoch 8, Step 0: Loss = 0.0000\n",
      "Epoch 8, Step 10: Loss = 0.0000\n",
      "Epoch 8, Step 20: Loss = 0.0000\n",
      "Epoch 8, Step 30: Loss = 0.0206\n",
      "Epoch 8, Step 40: Loss = 0.0000\n",
      "Epoch 8, Step 50: Loss = 0.1063\n",
      "Epoch 8, Step 60: Loss = 0.2503\n",
      "Epoch 8, Step 70: Loss = 0.0000\n",
      "Epoch 8, Step 80: Loss = 0.0000\n",
      "Epoch 8, Step 90: Loss = 0.0380\n",
      "Epoch 8, Step 100: Loss = 0.0000\n",
      "Epoch 8, Step 110: Loss = 0.0520\n",
      "Epoch 8, Step 120: Loss = 0.0429\n",
      "Epoch 8, Step 130: Loss = 0.1037\n",
      "Epoch 8, Step 140: Loss = 0.0000\n",
      "Epoch 8, Step 150: Loss = 0.0000\n",
      "Epoch 8, Step 160: Loss = 0.0174\n",
      "Epoch 8, Step 170: Loss = 0.0000\n",
      "Epoch 8, Step 180: Loss = 0.3405\n",
      "Epoch 8, Step 190: Loss = 0.2411\n",
      "Epoch 8, Step 200: Loss = 0.0000\n",
      "Epoch 8, Step 210: Loss = 0.0451\n",
      "Epoch 8, Step 220: Loss = 0.0000\n",
      "Epoch 8, Step 230: Loss = 0.0000\n",
      "Epoch 8, Step 240: Loss = 0.1948\n",
      "Epoch 8, Step 250: Loss = 0.0000\n",
      "Epoch 8, Step 260: Loss = 0.0000\n",
      "Epoch 8, Step 270: Loss = 0.0000\n",
      "Epoch 8, Step 280: Loss = 0.2823\n",
      "Epoch 8, Step 290: Loss = 0.0235\n",
      "Epoch 8, Step 300: Loss = 0.0000\n",
      "Epoch 8, Step 310: Loss = 0.0634\n",
      "Epoch 8, Step 320: Loss = 0.0809\n",
      "Epoch 8, Step 330: Loss = 0.0000\n",
      "Epoch 8, Step 340: Loss = 0.0000\n",
      "Epoch 8, Step 350: Loss = 0.0081\n",
      "Epoch 8, Step 360: Loss = 0.0508\n",
      "Epoch 8, Step 370: Loss = 0.0000\n",
      "Epoch 8, Step 380: Loss = 0.3707\n",
      "Epoch 8, Step 390: Loss = 0.0000\n",
      "Epoch 8, Step 400: Loss = 0.0358\n",
      "Epoch 8, Step 410: Loss = 0.0000\n",
      "Epoch 8, Step 420: Loss = 0.0000\n",
      "Epoch 8, Step 430: Loss = 0.0015\n",
      "Epoch 8, Step 440: Loss = 0.0000\n",
      "Epoch 8, Step 450: Loss = 0.1472\n",
      "Epoch 8, Step 460: Loss = 0.0000\n",
      "Epoch 8, Step 470: Loss = 0.0491\n",
      "Epoch 8, Step 480: Loss = 0.0000\n",
      "Epoch 8, Step 490: Loss = 0.0069\n",
      "\n",
      "Epoch 8 Summary:\n",
      "Average Training Loss: 0.0732\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1041\n",
      "Epoch 9, Step 0: Loss = 0.1415\n",
      "Epoch 9, Step 10: Loss = 0.0359\n",
      "Epoch 9, Step 20: Loss = 0.0046\n",
      "Epoch 9, Step 30: Loss = 0.0628\n",
      "Epoch 9, Step 40: Loss = 0.0000\n",
      "Epoch 9, Step 50: Loss = 0.0000\n",
      "Epoch 9, Step 60: Loss = 0.0000\n",
      "Epoch 9, Step 70: Loss = 0.0044\n",
      "Epoch 9, Step 80: Loss = 0.3296\n",
      "Epoch 9, Step 90: Loss = 0.0805\n",
      "Epoch 9, Step 100: Loss = 0.0042\n",
      "Epoch 9, Step 110: Loss = 0.0045\n",
      "Epoch 9, Step 120: Loss = 0.0204\n",
      "Epoch 9, Step 130: Loss = 0.0083\n",
      "Epoch 9, Step 140: Loss = 0.0068\n",
      "Epoch 9, Step 150: Loss = 0.0000\n",
      "Epoch 9, Step 160: Loss = 0.0000\n",
      "Epoch 9, Step 170: Loss = 0.0065\n",
      "Epoch 9, Step 180: Loss = 0.0222\n",
      "Epoch 9, Step 190: Loss = 0.0000\n",
      "Epoch 9, Step 200: Loss = 0.0000\n",
      "Epoch 9, Step 210: Loss = 0.2071\n",
      "Epoch 9, Step 220: Loss = 0.0000\n",
      "Epoch 9, Step 230: Loss = 0.0000\n",
      "Epoch 9, Step 240: Loss = 0.2146\n",
      "Epoch 9, Step 250: Loss = 0.0000\n",
      "Epoch 9, Step 260: Loss = 0.0000\n",
      "Epoch 9, Step 270: Loss = 0.0064\n",
      "Epoch 9, Step 280: Loss = 0.0000\n",
      "Epoch 9, Step 290: Loss = 0.0000\n",
      "Epoch 9, Step 300: Loss = 0.0000\n",
      "Epoch 9, Step 310: Loss = 0.0000\n",
      "Epoch 9, Step 320: Loss = 0.0000\n",
      "Epoch 9, Step 330: Loss = 0.0254\n",
      "Epoch 9, Step 340: Loss = 0.0093\n",
      "Epoch 9, Step 350: Loss = 0.0000\n",
      "Epoch 9, Step 360: Loss = 0.0000\n",
      "Epoch 9, Step 370: Loss = 0.0000\n",
      "Epoch 9, Step 380: Loss = 0.0010\n",
      "Epoch 9, Step 390: Loss = 0.0000\n",
      "Epoch 9, Step 400: Loss = 0.0000\n",
      "Epoch 9, Step 410: Loss = 0.0394\n",
      "Epoch 9, Step 420: Loss = 0.0000\n",
      "Epoch 9, Step 430: Loss = 0.0000\n",
      "Epoch 9, Step 440: Loss = 0.0000\n",
      "Epoch 9, Step 450: Loss = 0.0000\n",
      "Epoch 9, Step 460: Loss = 0.0000\n",
      "Epoch 9, Step 470: Loss = 0.0000\n",
      "Epoch 9, Step 480: Loss = 0.0000\n",
      "Epoch 9, Step 490: Loss = 0.0000\n",
      "\n",
      "Epoch 9 Summary:\n",
      "Average Training Loss: 0.0710\n",
      "Validation Metrics:\n",
      "Val_loss: 0.0866\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "# Set environment to use GPU 2 explicitly\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "\n",
    "\n",
    "class ContrastiveEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        # Clean each text field\n",
    "        emails_df['sender'] = emails_df['sender'].apply(clean_text)\n",
    "        emails_df['subject'] = emails_df['subject'].apply(clean_text)\n",
    "        emails_df['body'] = emails_df['body'].apply(clean_text)\n",
    "        \n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.ham_indices = self.emails_df[self.emails_df['label'] == 0].index.tolist()\n",
    "        self.phish_indices = self.emails_df[self.emails_df['label'] == 1].index.tolist()\n",
    "        if not (self.ham_indices and self.phish_indices):\n",
    "            raise ValueError(\"Dataset must contain examples of both classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def _get_random_email_idx(self, label):\n",
    "        indices = self.ham_indices if label == 0 else self.phish_indices\n",
    "        return np.random.choice(indices)\n",
    "\n",
    "    def _prepare_email_input(self, email):\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        tokenized_output = self.tokenizer(input_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        return tokenized_output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_email = self.emails_df.iloc[idx]\n",
    "        anchor_label = anchor_email['label']\n",
    "        positives, negatives = [], []\n",
    "\n",
    "        for _ in range(3):\n",
    "            pos_idx = self._get_random_email_idx(anchor_label)\n",
    "            neg_idx = self._get_random_email_idx(1 - anchor_label)\n",
    "            positives.append(self._prepare_email_input(self.emails_df.iloc[pos_idx]))\n",
    "            negatives.append(self._prepare_email_input(self.emails_df.iloc[neg_idx]))\n",
    "\n",
    "        anchor_inputs = self._prepare_email_input(anchor_email)\n",
    "        return {\n",
    "            'anchor_input_ids': anchor_inputs['input_ids'].squeeze(),\n",
    "            'anchor_attention_mask': anchor_inputs['attention_mask'].squeeze(),\n",
    "            'positive_input_ids': torch.stack([p['input_ids'].squeeze() for p in positives]),\n",
    "            'positive_attention_mask': torch.stack([p['attention_mask'].squeeze() for p in positives]),\n",
    "            'negative_input_ids': torch.stack([n['input_ids'].squeeze() for n in negatives]),\n",
    "            'negative_attention_mask': torch.stack([n['attention_mask'].squeeze() for n in negatives])\n",
    "        }\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model_config.use_cache = False\n",
    "\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        config=model_config, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"SEQ_CLS\")\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9, margin=1.0, accumulation_steps=2):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    model = model.to(device).to(torch.float16)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                # Compute embeddings for anchor, positive, and negative samples in parallel\n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                # Reshape positive and negative embeddings to match the batch structure (batch_size, 3, embedding_dim)\n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "                # Calculate the triplet loss for each triplet in the batch\n",
    "                loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "                loss /= accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        val_metrics = evaluate_model(model, val_loader, device, margin)\n",
    "        print_metrics(epoch, total_loss, len(train_loader), val_metrics)\n",
    "\n",
    "        if val_metrics['val_loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['val_loss']\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "           # best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def print_metrics(epoch, total_loss, num_batches, val_metrics):\n",
    "    \n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader, device, margin=1.0):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                # Compute embeddings for anchor, positive, and negative samples in parallel\n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                # Reshape positive and negative embeddings to match the batch structure (batch_size, 3, embedding_dim)\n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "                # Calculate the triplet loss for each triplet in the batch\n",
    "                val_loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "    return {'val_loss': avg_val_loss}\n",
    "\n",
    "\n",
    "# Main function\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Ensure text is a string\n",
    "        text = \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    #text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Main function modification for data cleaning\n",
    "def main():\n",
    "   \n",
    "    device = setup_environment()\n",
    "    model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/final_data.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "\n",
    "    # Ensure all relevant columns are strings to avoid errors\n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "    train_dataset = ContrastiveEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = ContrastiveEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)   \n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "   \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "        #best_model_state = train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "\n",
    "    best_model_state = train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/llama8BBBBBB_contrastive_classification_model_lora\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33c761b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2f6b7518514767978a4aa03646a2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at dreamgen/WizardLM-2-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 1.4031\n",
      "Epoch 1, Step 10: Loss = 1.0666\n",
      "Epoch 1, Step 20: Loss = 1.8342\n",
      "Epoch 1, Step 30: Loss = 2.4281\n",
      "Epoch 1, Step 40: Loss = 1.7325\n",
      "Epoch 1, Step 50: Loss = 2.1193\n",
      "Epoch 1, Step 60: Loss = 0.7690\n",
      "Epoch 1, Step 70: Loss = 1.4506\n",
      "Epoch 1, Step 80: Loss = 1.3137\n",
      "Epoch 1, Step 90: Loss = 1.5301\n",
      "Epoch 1, Step 100: Loss = 0.8923\n",
      "Epoch 1, Step 110: Loss = 1.3825\n",
      "Epoch 1, Step 120: Loss = 0.4773\n",
      "Epoch 1, Step 130: Loss = 1.4295\n",
      "Epoch 1, Step 140: Loss = 1.4777\n",
      "Epoch 1, Step 150: Loss = 1.1634\n",
      "Epoch 1, Step 160: Loss = 1.3524\n",
      "Epoch 1, Step 170: Loss = 1.7564\n",
      "Epoch 1, Step 180: Loss = 0.8237\n",
      "Epoch 1, Step 190: Loss = 1.4811\n",
      "Epoch 1, Step 200: Loss = 1.8455\n",
      "Epoch 1, Step 210: Loss = 2.0182\n",
      "Epoch 1, Step 220: Loss = 1.7140\n",
      "Epoch 1, Step 230: Loss = 1.4466\n",
      "Epoch 1, Step 240: Loss = 1.2749\n",
      "Epoch 1, Step 250: Loss = 1.7117\n",
      "Epoch 1, Step 260: Loss = 1.3477\n",
      "Epoch 1, Step 270: Loss = 1.0726\n",
      "Epoch 1, Step 280: Loss = 1.4518\n",
      "Epoch 1, Step 290: Loss = 1.7797\n",
      "Epoch 1, Step 300: Loss = 0.8473\n",
      "Epoch 1, Step 310: Loss = 0.9004\n",
      "Epoch 1, Step 320: Loss = 1.0572\n",
      "Epoch 1, Step 330: Loss = 1.1341\n",
      "Epoch 1, Step 340: Loss = 2.2217\n",
      "Epoch 1, Step 350: Loss = 1.1473\n",
      "Epoch 1, Step 360: Loss = 1.9542\n",
      "Epoch 1, Step 370: Loss = 0.7634\n",
      "Epoch 1, Step 380: Loss = 2.4740\n",
      "Epoch 1, Step 390: Loss = 0.4974\n",
      "Epoch 1, Step 400: Loss = 0.8563\n",
      "Epoch 1, Step 410: Loss = 1.5821\n",
      "Epoch 1, Step 420: Loss = 1.0919\n",
      "Epoch 1, Step 430: Loss = 2.0725\n",
      "Epoch 1, Step 440: Loss = 1.2242\n",
      "Epoch 1, Step 450: Loss = 0.9353\n",
      "Epoch 1, Step 460: Loss = 1.9847\n",
      "Epoch 1, Step 470: Loss = 1.5623\n",
      "Epoch 1, Step 480: Loss = 0.8096\n",
      "Epoch 1, Step 490: Loss = 0.2982\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Average Training Loss: 2.5705\n",
      "Validation Metrics:\n",
      "Val_loss: 1.9553\n",
      "Epoch 2, Step 0: Loss = 0.6673\n",
      "Epoch 2, Step 10: Loss = 0.5814\n",
      "Epoch 2, Step 20: Loss = 1.0690\n",
      "Epoch 2, Step 30: Loss = 0.6466\n",
      "Epoch 2, Step 40: Loss = 0.9060\n",
      "Epoch 2, Step 50: Loss = 1.0404\n",
      "Epoch 2, Step 60: Loss = 0.6685\n",
      "Epoch 2, Step 70: Loss = 0.2459\n",
      "Epoch 2, Step 80: Loss = 0.5912\n",
      "Epoch 2, Step 90: Loss = 1.0168\n",
      "Epoch 2, Step 100: Loss = 1.7356\n",
      "Epoch 2, Step 110: Loss = 2.2075\n",
      "Epoch 2, Step 120: Loss = 0.5813\n",
      "Epoch 2, Step 130: Loss = 0.7771\n",
      "Epoch 2, Step 140: Loss = 0.6415\n",
      "Epoch 2, Step 150: Loss = 1.1944\n",
      "Epoch 2, Step 160: Loss = 0.5745\n",
      "Epoch 2, Step 170: Loss = 1.3894\n",
      "Epoch 2, Step 180: Loss = 1.1365\n",
      "Epoch 2, Step 190: Loss = 0.6322\n",
      "Epoch 2, Step 200: Loss = 1.5047\n",
      "Epoch 2, Step 210: Loss = 0.9732\n",
      "Epoch 2, Step 220: Loss = 1.4338\n",
      "Epoch 2, Step 230: Loss = 0.8882\n",
      "Epoch 2, Step 240: Loss = 0.6242\n",
      "Epoch 2, Step 250: Loss = 0.5802\n",
      "Epoch 2, Step 260: Loss = 0.2097\n",
      "Epoch 2, Step 270: Loss = 1.0556\n",
      "Epoch 2, Step 280: Loss = 0.5895\n",
      "Epoch 2, Step 290: Loss = 0.7568\n",
      "Epoch 2, Step 300: Loss = 1.3248\n",
      "Epoch 2, Step 310: Loss = 0.6904\n",
      "Epoch 2, Step 320: Loss = 1.6474\n",
      "Epoch 2, Step 330: Loss = 0.2979\n",
      "Epoch 2, Step 340: Loss = 0.7147\n",
      "Epoch 2, Step 350: Loss = 0.4704\n",
      "Epoch 2, Step 360: Loss = 0.5750\n",
      "Epoch 2, Step 370: Loss = 0.7133\n",
      "Epoch 2, Step 380: Loss = 1.4999\n",
      "Epoch 2, Step 390: Loss = 1.0788\n",
      "Epoch 2, Step 400: Loss = 0.5088\n",
      "Epoch 2, Step 410: Loss = 0.1775\n",
      "Epoch 2, Step 420: Loss = 0.8229\n",
      "Epoch 2, Step 430: Loss = 0.8731\n",
      "Epoch 2, Step 440: Loss = 2.6666\n",
      "Epoch 2, Step 450: Loss = 2.0768\n",
      "Epoch 2, Step 460: Loss = 0.0551\n",
      "Epoch 2, Step 470: Loss = 0.1085\n",
      "Epoch 2, Step 480: Loss = 0.2826\n",
      "Epoch 2, Step 490: Loss = 0.5307\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Average Training Loss: 1.5922\n",
      "Validation Metrics:\n",
      "Val_loss: 1.1990\n",
      "Epoch 3, Step 0: Loss = 0.1460\n",
      "Epoch 3, Step 10: Loss = 0.3208\n",
      "Epoch 3, Step 20: Loss = 0.3816\n",
      "Epoch 3, Step 30: Loss = 0.1373\n",
      "Epoch 3, Step 40: Loss = 2.0921\n",
      "Epoch 3, Step 50: Loss = 0.4128\n",
      "Epoch 3, Step 60: Loss = 1.0840\n",
      "Epoch 3, Step 70: Loss = 0.4982\n",
      "Epoch 3, Step 80: Loss = 0.3054\n",
      "Epoch 3, Step 90: Loss = 0.5135\n",
      "Epoch 3, Step 100: Loss = 0.5835\n",
      "Epoch 3, Step 110: Loss = 1.5202\n",
      "Epoch 3, Step 120: Loss = 0.0631\n",
      "Epoch 3, Step 130: Loss = 0.0000\n",
      "Epoch 3, Step 140: Loss = 1.5822\n",
      "Epoch 3, Step 150: Loss = 0.0206\n",
      "Epoch 3, Step 160: Loss = 0.3924\n",
      "Epoch 3, Step 170: Loss = 0.4469\n",
      "Epoch 3, Step 180: Loss = 1.1728\n",
      "Epoch 3, Step 190: Loss = 0.8912\n",
      "Epoch 3, Step 200: Loss = 0.9528\n",
      "Epoch 3, Step 210: Loss = 0.1337\n",
      "Epoch 3, Step 220: Loss = 0.6693\n",
      "Epoch 3, Step 230: Loss = 0.5594\n",
      "Epoch 3, Step 240: Loss = 0.6558\n",
      "Epoch 3, Step 250: Loss = 1.0821\n",
      "Epoch 3, Step 260: Loss = 0.2041\n",
      "Epoch 3, Step 270: Loss = 0.3678\n",
      "Epoch 3, Step 280: Loss = 0.0928\n",
      "Epoch 3, Step 290: Loss = 0.4108\n",
      "Epoch 3, Step 300: Loss = 0.0920\n",
      "Epoch 3, Step 310: Loss = 0.0158\n",
      "Epoch 3, Step 320: Loss = 0.0803\n",
      "Epoch 3, Step 330: Loss = 0.0835\n",
      "Epoch 3, Step 340: Loss = 0.3504\n",
      "Epoch 3, Step 350: Loss = 0.7417\n",
      "Epoch 3, Step 360: Loss = 0.5686\n",
      "Epoch 3, Step 370: Loss = 0.1682\n",
      "Epoch 3, Step 380: Loss = 0.1408\n",
      "Epoch 3, Step 390: Loss = 0.3239\n",
      "Epoch 3, Step 400: Loss = 0.5132\n",
      "Epoch 3, Step 410: Loss = 1.5202\n",
      "Epoch 3, Step 420: Loss = 0.2853\n",
      "Epoch 3, Step 430: Loss = 0.3025\n",
      "Epoch 3, Step 440: Loss = 0.8232\n",
      "Epoch 3, Step 450: Loss = 0.2586\n",
      "Epoch 3, Step 460: Loss = 0.4008\n",
      "Epoch 3, Step 470: Loss = 0.1443\n",
      "Epoch 3, Step 480: Loss = 0.0609\n",
      "Epoch 3, Step 490: Loss = 0.0372\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Average Training Loss: 1.0540\n",
      "Validation Metrics:\n",
      "Val_loss: 0.8774\n",
      "Epoch 4, Step 0: Loss = 0.7441\n",
      "Epoch 4, Step 10: Loss = 0.5212\n",
      "Epoch 4, Step 20: Loss = 0.0000\n",
      "Epoch 4, Step 30: Loss = 0.9358\n",
      "Epoch 4, Step 40: Loss = 0.2700\n",
      "Epoch 4, Step 50: Loss = 0.4064\n",
      "Epoch 4, Step 60: Loss = 0.0632\n",
      "Epoch 4, Step 70: Loss = 0.2381\n",
      "Epoch 4, Step 80: Loss = 1.1851\n",
      "Epoch 4, Step 90: Loss = 0.3161\n",
      "Epoch 4, Step 100: Loss = 0.5593\n",
      "Epoch 4, Step 110: Loss = 0.2482\n",
      "Epoch 4, Step 120: Loss = 0.2307\n",
      "Epoch 4, Step 130: Loss = 0.2490\n",
      "Epoch 4, Step 140: Loss = 0.0000\n",
      "Epoch 4, Step 150: Loss = 0.4908\n",
      "Epoch 4, Step 160: Loss = 0.4765\n",
      "Epoch 4, Step 170: Loss = 1.2163\n",
      "Epoch 4, Step 180: Loss = 0.8149\n",
      "Epoch 4, Step 190: Loss = 0.0792\n",
      "Epoch 4, Step 200: Loss = 0.1972\n",
      "Epoch 4, Step 210: Loss = 0.2947\n",
      "Epoch 4, Step 220: Loss = 0.3304\n",
      "Epoch 4, Step 230: Loss = 0.9818\n",
      "Epoch 4, Step 240: Loss = 0.0000\n",
      "Epoch 4, Step 250: Loss = 0.1441\n",
      "Epoch 4, Step 260: Loss = 0.8182\n",
      "Epoch 4, Step 270: Loss = 0.6393\n",
      "Epoch 4, Step 280: Loss = 0.2555\n",
      "Epoch 4, Step 290: Loss = 0.3270\n",
      "Epoch 4, Step 300: Loss = 0.4559\n",
      "Epoch 4, Step 310: Loss = 0.1245\n",
      "Epoch 4, Step 320: Loss = 0.2570\n",
      "Epoch 4, Step 330: Loss = 0.8072\n",
      "Epoch 4, Step 340: Loss = 0.1430\n",
      "Epoch 4, Step 350: Loss = 0.7153\n",
      "Epoch 4, Step 360: Loss = 1.1322\n",
      "Epoch 4, Step 370: Loss = 0.3631\n",
      "Epoch 4, Step 380: Loss = 0.2528\n",
      "Epoch 4, Step 390: Loss = 0.9204\n",
      "Epoch 4, Step 400: Loss = 0.3846\n",
      "Epoch 4, Step 410: Loss = 0.1660\n",
      "Epoch 4, Step 420: Loss = 0.1617\n",
      "Epoch 4, Step 430: Loss = 0.2854\n",
      "Epoch 4, Step 440: Loss = 0.0000\n",
      "Epoch 4, Step 450: Loss = 0.2589\n",
      "Epoch 4, Step 460: Loss = 0.1288\n",
      "Epoch 4, Step 470: Loss = 0.9932\n",
      "Epoch 4, Step 480: Loss = 1.0636\n",
      "Epoch 4, Step 490: Loss = 0.1061\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Average Training Loss: 0.8345\n",
      "Validation Metrics:\n",
      "Val_loss: 0.7048\n",
      "Epoch 5, Step 0: Loss = 0.0000\n",
      "Epoch 5, Step 10: Loss = 0.0307\n",
      "Epoch 5, Step 20: Loss = 0.4400\n",
      "Epoch 5, Step 30: Loss = 0.0645\n",
      "Epoch 5, Step 40: Loss = 0.8744\n",
      "Epoch 5, Step 50: Loss = 0.1897\n",
      "Epoch 5, Step 60: Loss = 0.0000\n",
      "Epoch 5, Step 70: Loss = 0.1781\n",
      "Epoch 5, Step 80: Loss = 0.0390\n",
      "Epoch 5, Step 90: Loss = 0.2720\n",
      "Epoch 5, Step 100: Loss = 0.8187\n",
      "Epoch 5, Step 110: Loss = 0.2483\n",
      "Epoch 5, Step 120: Loss = 0.5049\n",
      "Epoch 5, Step 130: Loss = 0.6160\n",
      "Epoch 5, Step 140: Loss = 0.2613\n",
      "Epoch 5, Step 150: Loss = 0.0939\n",
      "Epoch 5, Step 160: Loss = 0.9095\n",
      "Epoch 5, Step 170: Loss = 1.5584\n",
      "Epoch 5, Step 180: Loss = 0.9244\n",
      "Epoch 5, Step 190: Loss = 0.0703\n",
      "Epoch 5, Step 200: Loss = 0.4558\n",
      "Epoch 5, Step 210: Loss = 0.3747\n",
      "Epoch 5, Step 220: Loss = 0.0219\n",
      "Epoch 5, Step 230: Loss = 0.1410\n",
      "Epoch 5, Step 240: Loss = 0.2907\n",
      "Epoch 5, Step 250: Loss = 0.8148\n",
      "Epoch 5, Step 260: Loss = 0.0151\n",
      "Epoch 5, Step 270: Loss = 0.0899\n",
      "Epoch 5, Step 280: Loss = 1.5878\n",
      "Epoch 5, Step 290: Loss = 0.9180\n",
      "Epoch 5, Step 300: Loss = 0.2175\n",
      "Epoch 5, Step 310: Loss = 0.2410\n",
      "Epoch 5, Step 320: Loss = 0.5358\n",
      "Epoch 5, Step 330: Loss = 0.8337\n",
      "Epoch 5, Step 340: Loss = 0.0000\n",
      "Epoch 5, Step 350: Loss = 0.2151\n",
      "Epoch 5, Step 360: Loss = 1.5521\n",
      "Epoch 5, Step 370: Loss = 0.0129\n",
      "Epoch 5, Step 380: Loss = 0.1151\n",
      "Epoch 5, Step 390: Loss = 0.0370\n",
      "Epoch 5, Step 400: Loss = 0.1558\n",
      "Epoch 5, Step 410: Loss = 0.0441\n",
      "Epoch 5, Step 420: Loss = 0.3319\n",
      "Epoch 5, Step 430: Loss = 0.6237\n",
      "Epoch 5, Step 440: Loss = 0.0000\n",
      "Epoch 5, Step 450: Loss = 0.9349\n",
      "Epoch 5, Step 460: Loss = 0.0045\n",
      "Epoch 5, Step 470: Loss = 0.4737\n",
      "Epoch 5, Step 480: Loss = 0.0000\n",
      "Epoch 5, Step 490: Loss = 0.5129\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Average Training Loss: 0.6910\n",
      "Validation Metrics:\n",
      "Val_loss: 0.6452\n",
      "Epoch 6, Step 0: Loss = 0.4799\n",
      "Epoch 6, Step 10: Loss = 0.3119\n",
      "Epoch 6, Step 20: Loss = 0.1053\n",
      "Epoch 6, Step 30: Loss = 0.1553\n",
      "Epoch 6, Step 40: Loss = 0.0000\n",
      "Epoch 6, Step 50: Loss = 0.0000\n",
      "Epoch 6, Step 60: Loss = 0.4549\n",
      "Epoch 6, Step 70: Loss = 0.6031\n",
      "Epoch 6, Step 80: Loss = 0.7267\n",
      "Epoch 6, Step 90: Loss = 1.0390\n",
      "Epoch 6, Step 100: Loss = 0.4291\n",
      "Epoch 6, Step 110: Loss = 0.2464\n",
      "Epoch 6, Step 120: Loss = 0.7659\n",
      "Epoch 6, Step 130: Loss = 0.1653\n",
      "Epoch 6, Step 140: Loss = 0.0345\n",
      "Epoch 6, Step 150: Loss = 0.2109\n",
      "Epoch 6, Step 160: Loss = 0.3991\n",
      "Epoch 6, Step 170: Loss = 0.4025\n",
      "Epoch 6, Step 180: Loss = 0.4423\n",
      "Epoch 6, Step 190: Loss = 0.4024\n",
      "Epoch 6, Step 200: Loss = 0.0140\n",
      "Epoch 6, Step 210: Loss = 1.2129\n",
      "Epoch 6, Step 220: Loss = 0.6739\n",
      "Epoch 6, Step 230: Loss = 0.4586\n",
      "Epoch 6, Step 240: Loss = 0.0569\n",
      "Epoch 6, Step 250: Loss = 0.5638\n",
      "Epoch 6, Step 260: Loss = 0.4760\n",
      "Epoch 6, Step 270: Loss = 0.3842\n",
      "Epoch 6, Step 280: Loss = 0.5298\n",
      "Epoch 6, Step 290: Loss = 2.0475\n",
      "Epoch 6, Step 300: Loss = 0.6298\n",
      "Epoch 6, Step 310: Loss = 0.2244\n",
      "Epoch 6, Step 320: Loss = 0.6138\n",
      "Epoch 6, Step 330: Loss = 0.0000\n",
      "Epoch 6, Step 340: Loss = 0.1770\n",
      "Epoch 6, Step 350: Loss = 0.0000\n",
      "Epoch 6, Step 360: Loss = 0.5520\n",
      "Epoch 6, Step 370: Loss = 0.3019\n",
      "Epoch 6, Step 380: Loss = 0.3614\n",
      "Epoch 6, Step 390: Loss = 0.0937\n",
      "Epoch 6, Step 400: Loss = 0.0000\n",
      "Epoch 6, Step 410: Loss = 0.1952\n",
      "Epoch 6, Step 420: Loss = 0.1639\n",
      "Epoch 6, Step 430: Loss = 0.4018\n",
      "Epoch 6, Step 440: Loss = 0.1963\n",
      "Epoch 6, Step 450: Loss = 0.2240\n",
      "Epoch 6, Step 460: Loss = 1.1233\n",
      "Epoch 6, Step 470: Loss = 0.2770\n",
      "Epoch 6, Step 480: Loss = 0.0000\n",
      "Epoch 6, Step 490: Loss = 0.5255\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Average Training Loss: 0.6082\n",
      "Validation Metrics:\n",
      "Val_loss: 0.5909\n",
      "Epoch 7, Step 0: Loss = 0.1870\n",
      "Epoch 7, Step 10: Loss = 0.4735\n",
      "Epoch 7, Step 20: Loss = 0.1202\n",
      "Epoch 7, Step 30: Loss = 1.2540\n",
      "Epoch 7, Step 40: Loss = 0.0000\n",
      "Epoch 7, Step 50: Loss = 0.2660\n",
      "Epoch 7, Step 60: Loss = 0.0000\n",
      "Epoch 7, Step 70: Loss = 0.0357\n",
      "Epoch 7, Step 80: Loss = 0.0347\n",
      "Epoch 7, Step 90: Loss = 0.1511\n",
      "Epoch 7, Step 100: Loss = 0.0000\n",
      "Epoch 7, Step 110: Loss = 0.0000\n",
      "Epoch 7, Step 120: Loss = 0.7074\n",
      "Epoch 7, Step 130: Loss = 0.0000\n",
      "Epoch 7, Step 140: Loss = 0.0000\n",
      "Epoch 7, Step 150: Loss = 0.5217\n",
      "Epoch 7, Step 160: Loss = 0.0000\n",
      "Epoch 7, Step 170: Loss = 0.1798\n",
      "Epoch 7, Step 180: Loss = 0.0000\n",
      "Epoch 7, Step 190: Loss = 0.0131\n",
      "Epoch 7, Step 200: Loss = 0.0024\n",
      "Epoch 7, Step 210: Loss = 0.3712\n",
      "Epoch 7, Step 220: Loss = 0.0000\n",
      "Epoch 7, Step 230: Loss = 0.1432\n",
      "Epoch 7, Step 240: Loss = 0.0000\n",
      "Epoch 7, Step 250: Loss = 0.3475\n",
      "Epoch 7, Step 260: Loss = 0.1789\n",
      "Epoch 7, Step 270: Loss = 0.3653\n",
      "Epoch 7, Step 280: Loss = 0.0793\n",
      "Epoch 7, Step 290: Loss = 0.3608\n",
      "Epoch 7, Step 300: Loss = 0.4093\n",
      "Epoch 7, Step 310: Loss = 0.0000\n",
      "Epoch 7, Step 320: Loss = 0.0000\n",
      "Epoch 7, Step 330: Loss = 0.1111\n",
      "Epoch 7, Step 340: Loss = 0.8789\n",
      "Epoch 7, Step 350: Loss = 0.0115\n",
      "Epoch 7, Step 360: Loss = 0.2470\n",
      "Epoch 7, Step 370: Loss = 0.3307\n",
      "Epoch 7, Step 380: Loss = 0.0000\n",
      "Epoch 7, Step 390: Loss = 0.0000\n",
      "Epoch 7, Step 400: Loss = 0.5004\n",
      "Epoch 7, Step 410: Loss = 0.0295\n",
      "Epoch 7, Step 420: Loss = 0.4707\n",
      "Epoch 7, Step 430: Loss = 0.3313\n",
      "Epoch 7, Step 440: Loss = 0.3326\n",
      "Epoch 7, Step 450: Loss = 0.2024\n",
      "Epoch 7, Step 460: Loss = 0.5083\n",
      "Epoch 7, Step 470: Loss = 0.0764\n",
      "Epoch 7, Step 480: Loss = 1.3152\n",
      "Epoch 7, Step 490: Loss = 0.2396\n",
      "\n",
      "Epoch 7 Summary:\n",
      "Average Training Loss: 0.5572\n",
      "Validation Metrics:\n",
      "Val_loss: 0.6105\n",
      "Epoch 8, Step 0: Loss = 0.7702\n",
      "Epoch 8, Step 10: Loss = 0.0000\n",
      "Epoch 8, Step 20: Loss = 0.0000\n",
      "Epoch 8, Step 30: Loss = 0.8546\n",
      "Epoch 8, Step 40: Loss = 0.0455\n",
      "Epoch 8, Step 50: Loss = 0.2678\n",
      "Epoch 8, Step 60: Loss = 0.0816\n",
      "Epoch 8, Step 70: Loss = 0.1243\n",
      "Epoch 8, Step 80: Loss = 0.0000\n",
      "Epoch 8, Step 90: Loss = 0.0000\n",
      "Epoch 8, Step 100: Loss = 0.0479\n",
      "Epoch 8, Step 110: Loss = 0.3620\n",
      "Epoch 8, Step 120: Loss = 0.0631\n",
      "Epoch 8, Step 130: Loss = 0.0781\n",
      "Epoch 8, Step 140: Loss = 0.0340\n",
      "Epoch 8, Step 150: Loss = 0.3621\n",
      "Epoch 8, Step 160: Loss = 0.0000\n",
      "Epoch 8, Step 170: Loss = 0.0000\n",
      "Epoch 8, Step 180: Loss = 0.0458\n",
      "Epoch 8, Step 190: Loss = 0.1326\n",
      "Epoch 8, Step 200: Loss = 0.1745\n",
      "Epoch 8, Step 210: Loss = 0.3306\n",
      "Epoch 8, Step 220: Loss = 0.3083\n",
      "Epoch 8, Step 230: Loss = 0.0000\n",
      "Epoch 8, Step 240: Loss = 0.0000\n",
      "Epoch 8, Step 250: Loss = 0.0000\n",
      "Epoch 8, Step 260: Loss = 0.4682\n",
      "Epoch 8, Step 270: Loss = 0.3214\n",
      "Epoch 8, Step 280: Loss = 0.4326\n",
      "Epoch 8, Step 290: Loss = 1.2129\n",
      "Epoch 8, Step 300: Loss = 0.3494\n",
      "Epoch 8, Step 310: Loss = 0.0807\n",
      "Epoch 8, Step 320: Loss = 0.0289\n",
      "Epoch 8, Step 330: Loss = 0.0000\n",
      "Epoch 8, Step 340: Loss = 0.2162\n",
      "Epoch 8, Step 350: Loss = 0.1770\n",
      "Epoch 8, Step 360: Loss = 0.0000\n",
      "Epoch 8, Step 370: Loss = 0.0000\n",
      "Epoch 8, Step 380: Loss = 1.0730\n",
      "Epoch 8, Step 390: Loss = 0.0000\n",
      "Epoch 8, Step 400: Loss = 0.0526\n",
      "Epoch 8, Step 410: Loss = 0.0000\n",
      "Epoch 8, Step 420: Loss = 1.1077\n",
      "Epoch 8, Step 430: Loss = 0.2954\n",
      "Epoch 8, Step 440: Loss = 0.0000\n",
      "Epoch 8, Step 450: Loss = 0.0000\n",
      "Epoch 8, Step 460: Loss = 0.0000\n",
      "Epoch 8, Step 470: Loss = 0.0808\n",
      "Epoch 8, Step 480: Loss = 0.5368\n",
      "Epoch 8, Step 490: Loss = 0.0538\n",
      "\n",
      "Epoch 8 Summary:\n",
      "Average Training Loss: 0.5053\n",
      "Validation Metrics:\n",
      "Val_loss: 0.5835\n",
      "Epoch 9, Step 0: Loss = 0.0735\n",
      "Epoch 9, Step 10: Loss = 0.0866\n",
      "Epoch 9, Step 20: Loss = 0.0000\n",
      "Epoch 9, Step 30: Loss = 0.2095\n",
      "Epoch 9, Step 40: Loss = 0.1206\n",
      "Epoch 9, Step 50: Loss = 0.6422\n",
      "Epoch 9, Step 60: Loss = 0.0290\n",
      "Epoch 9, Step 70: Loss = 0.2450\n",
      "Epoch 9, Step 80: Loss = 0.0365\n",
      "Epoch 9, Step 90: Loss = 0.1337\n",
      "Epoch 9, Step 100: Loss = 0.0968\n",
      "Epoch 9, Step 110: Loss = 0.0512\n",
      "Epoch 9, Step 120: Loss = 0.0000\n",
      "Epoch 9, Step 130: Loss = 0.2011\n",
      "Epoch 9, Step 140: Loss = 0.1804\n",
      "Epoch 9, Step 150: Loss = 0.0082\n",
      "Epoch 9, Step 160: Loss = 0.0915\n",
      "Epoch 9, Step 170: Loss = 0.0000\n",
      "Epoch 9, Step 180: Loss = 0.0000\n",
      "Epoch 9, Step 190: Loss = 0.0000\n",
      "Epoch 9, Step 200: Loss = 0.1808\n",
      "Epoch 9, Step 210: Loss = 0.0649\n",
      "Epoch 9, Step 220: Loss = 0.0587\n",
      "Epoch 9, Step 230: Loss = 0.0428\n",
      "Epoch 9, Step 240: Loss = 0.0089\n",
      "Epoch 9, Step 250: Loss = 0.4888\n",
      "Epoch 9, Step 260: Loss = 0.0219\n",
      "Epoch 9, Step 270: Loss = 0.0000\n",
      "Epoch 9, Step 280: Loss = 0.4704\n",
      "Epoch 9, Step 290: Loss = 0.0000\n",
      "Epoch 9, Step 300: Loss = 0.0186\n",
      "Epoch 9, Step 310: Loss = 0.0219\n",
      "Epoch 9, Step 320: Loss = 0.2078\n",
      "Epoch 9, Step 330: Loss = 0.0150\n",
      "Epoch 9, Step 340: Loss = 0.2769\n",
      "Epoch 9, Step 350: Loss = 0.0223\n",
      "Epoch 9, Step 360: Loss = 0.3905\n",
      "Epoch 9, Step 370: Loss = 0.0000\n",
      "Epoch 9, Step 380: Loss = 0.3901\n",
      "Epoch 9, Step 390: Loss = 0.0029\n",
      "Epoch 9, Step 400: Loss = 0.2971\n",
      "Epoch 9, Step 410: Loss = 0.2601\n",
      "Epoch 9, Step 420: Loss = 0.7338\n",
      "Epoch 9, Step 430: Loss = 0.0000\n",
      "Epoch 9, Step 440: Loss = 0.0369\n",
      "Epoch 9, Step 450: Loss = 0.1183\n",
      "Epoch 9, Step 460: Loss = 0.0649\n",
      "Epoch 9, Step 470: Loss = 0.0207\n",
      "Epoch 9, Step 480: Loss = 0.0000\n",
      "Epoch 9, Step 490: Loss = 0.0000\n",
      "\n",
      "Epoch 9 Summary:\n",
      "Average Training Loss: 0.4531\n",
      "Validation Metrics:\n",
      "Val_loss: 0.4685\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import traceback\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Replace LlamaForSequenceClassification with AutoModelForSequenceClassification\n",
    "\n",
    "# Set environment to use GPU 2 explicitly\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "\n",
    "\n",
    "class ContrastiveEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        # Clean each text field\n",
    "        emails_df['sender'] = emails_df['sender'].apply(clean_text)\n",
    "        emails_df['subject'] = emails_df['subject'].apply(clean_text)\n",
    "        emails_df['body'] = emails_df['body'].apply(clean_text)\n",
    "        \n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.ham_indices = self.emails_df[self.emails_df['label'] == 0].index.tolist()\n",
    "        self.phish_indices = self.emails_df[self.emails_df['label'] == 1].index.tolist()\n",
    "        if not (self.ham_indices and self.phish_indices):\n",
    "            raise ValueError(\"Dataset must contain examples of both classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def _get_random_email_idx(self, label):\n",
    "        indices = self.ham_indices if label == 0 else self.phish_indices\n",
    "        return np.random.choice(indices)\n",
    "\n",
    "    def _prepare_email_input(self, email):\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        tokenized_output = self.tokenizer(input_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        return tokenized_output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_email = self.emails_df.iloc[idx]\n",
    "        anchor_label = anchor_email['label']\n",
    "        positives, negatives = [], []\n",
    "\n",
    "        for _ in range(3):\n",
    "            pos_idx = self._get_random_email_idx(anchor_label)\n",
    "            neg_idx = self._get_random_email_idx(1 - anchor_label)\n",
    "            positives.append(self._prepare_email_input(self.emails_df.iloc[pos_idx]))\n",
    "            negatives.append(self._prepare_email_input(self.emails_df.iloc[neg_idx]))\n",
    "\n",
    "        anchor_inputs = self._prepare_email_input(anchor_email)\n",
    "        return {\n",
    "            'anchor_input_ids': anchor_inputs['input_ids'].squeeze(),\n",
    "            'anchor_attention_mask': anchor_inputs['attention_mask'].squeeze(),\n",
    "            'positive_input_ids': torch.stack([p['input_ids'].squeeze() for p in positives]),\n",
    "            'positive_attention_mask': torch.stack([p['attention_mask'].squeeze() for p in positives]),\n",
    "            'negative_input_ids': torch.stack([n['input_ids'].squeeze() for n in negatives]),\n",
    "            'negative_attention_mask': torch.stack([n['attention_mask'].squeeze() for n in negatives])\n",
    "        }\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model_config.use_cache = False\n",
    "\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=model_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "\n",
    "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"SEQ_CLS\")\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=9, margin=1.0, accumulation_steps=2):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    model = model.to(device).to(torch.float16)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                # Compute embeddings for anchor, positive, and negative samples in parallel\n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                # Reshape positive and negative embeddings to match the batch structure (batch_size, 3, embedding_dim)\n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "                # Calculate the triplet loss for each triplet in the batch\n",
    "                loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "                loss /= accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        val_metrics = evaluate_model(model, val_loader, device, margin)\n",
    "        print_metrics(epoch, total_loss, len(train_loader), val_metrics)\n",
    "\n",
    "        if val_metrics['val_loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['val_loss']\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "           # best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def print_metrics(epoch, total_loss, num_batches, val_metrics):\n",
    "    \n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader, device, margin=1.0):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                # Compute embeddings for anchor, positive, and negative samples in parallel\n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                # Reshape positive and negative embeddings to match the batch structure (batch_size, 3, embedding_dim)\n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "                # Calculate the triplet loss for each triplet in the batch\n",
    "                val_loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "    return {'val_loss': avg_val_loss}\n",
    "\n",
    "\n",
    "# Main function\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Ensure text is a string\n",
    "        text = \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    #text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Main function modification for data cleaning\n",
    "def main():\n",
    "  \n",
    "    device = setup_environment()\n",
    "    model_name = 'dreamgen/WizardLM-2-7B'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/final_data.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "\n",
    "    # Ensure all relevant columns are strings to avoid errors\n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "    train_dataset = ContrastiveEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = ContrastiveEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)   \n",
    "    num_epochs = 9\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "   \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "        #best_model_state = train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "\n",
    "    best_model_state = train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/wiz_contrastive_classification_model_lora\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfcda21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854d260ceebd42c39dcc6dc6a95db08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 0.5966\n",
      "Epoch 1, Step 10: Loss = 0.4484\n",
      "Epoch 1, Step 20: Loss = 0.4604\n",
      "Epoch 1, Step 30: Loss = 0.5192\n",
      "Epoch 1, Step 40: Loss = 0.2952\n",
      "Epoch 1, Step 50: Loss = 0.2056\n",
      "Epoch 1, Step 60: Loss = 0.3994\n",
      "Epoch 1, Step 70: Loss = 0.4703\n",
      "Epoch 1, Step 80: Loss = 0.1773\n",
      "Epoch 1, Step 90: Loss = 0.5351\n",
      "Epoch 1, Step 100: Loss = 0.4803\n",
      "Epoch 1, Step 110: Loss = 0.3882\n",
      "Epoch 1, Step 120: Loss = 0.8287\n",
      "Epoch 1, Step 130: Loss = 0.4020\n",
      "Epoch 1, Step 140: Loss = 0.3895\n",
      "Epoch 1, Step 150: Loss = 0.3187\n",
      "Epoch 1, Step 160: Loss = 0.2593\n",
      "Epoch 1, Step 170: Loss = 0.4572\n",
      "Epoch 1, Step 180: Loss = 0.5183\n",
      "Epoch 1, Step 190: Loss = 0.5409\n",
      "Epoch 1, Step 200: Loss = 0.2555\n",
      "Epoch 1, Step 210: Loss = 0.4181\n",
      "Epoch 1, Step 220: Loss = 0.5730\n",
      "Epoch 1, Step 230: Loss = 0.4346\n",
      "Epoch 1, Step 240: Loss = 0.1880\n",
      "Epoch 1, Step 250: Loss = 0.5279\n",
      "Epoch 1, Step 260: Loss = 0.3217\n",
      "Epoch 1, Step 270: Loss = 0.2363\n",
      "Epoch 1, Step 280: Loss = 0.0875\n",
      "Epoch 1, Step 290: Loss = 0.5618\n",
      "Epoch 1, Step 300: Loss = 0.3012\n",
      "Epoch 1, Step 310: Loss = 0.2993\n",
      "Epoch 1, Step 320: Loss = 0.4836\n",
      "Epoch 1, Step 330: Loss = 0.1613\n",
      "Epoch 1, Step 340: Loss = 0.2165\n",
      "Epoch 1, Step 350: Loss = 0.5667\n",
      "Epoch 1, Step 360: Loss = 0.6110\n",
      "Epoch 1, Step 370: Loss = 0.1390\n",
      "Epoch 1, Step 380: Loss = 0.2535\n",
      "Epoch 1, Step 390: Loss = 0.1806\n",
      "Epoch 1, Step 400: Loss = 0.1535\n",
      "Epoch 1, Step 410: Loss = 0.3919\n",
      "Epoch 1, Step 420: Loss = 0.1881\n",
      "Epoch 1, Step 430: Loss = 0.0479\n",
      "Epoch 1, Step 440: Loss = 0.0456\n",
      "Epoch 1, Step 450: Loss = 0.1982\n",
      "Epoch 1, Step 460: Loss = 0.0805\n",
      "Epoch 1, Step 470: Loss = 0.0290\n",
      "Epoch 1, Step 480: Loss = 0.2230\n",
      "Epoch 1, Step 490: Loss = 0.0353\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Average Training Loss: 0.7297\n",
      "Validation Metrics:\n",
      "Val_loss: 0.4625\n",
      "Epoch 2, Step 0: Loss = 0.3337\n",
      "Epoch 2, Step 10: Loss = 0.2842\n",
      "Epoch 2, Step 20: Loss = 0.0200\n",
      "Epoch 2, Step 30: Loss = 0.3255\n",
      "Epoch 2, Step 40: Loss = 0.0771\n",
      "Epoch 2, Step 50: Loss = 0.3441\n",
      "Epoch 2, Step 60: Loss = 0.0478\n",
      "Epoch 2, Step 70: Loss = 0.3883\n",
      "Epoch 2, Step 80: Loss = 0.0325\n",
      "Epoch 2, Step 90: Loss = 0.0854\n",
      "Epoch 2, Step 100: Loss = 0.2862\n",
      "Epoch 2, Step 110: Loss = 0.3761\n",
      "Epoch 2, Step 120: Loss = 0.1435\n",
      "Epoch 2, Step 130: Loss = 0.5213\n",
      "Epoch 2, Step 140: Loss = 0.0000\n",
      "Epoch 2, Step 150: Loss = 0.0336\n",
      "Epoch 2, Step 160: Loss = 0.0000\n",
      "Epoch 2, Step 170: Loss = 0.1962\n",
      "Epoch 2, Step 180: Loss = 0.0941\n",
      "Epoch 2, Step 190: Loss = 0.0569\n",
      "Epoch 2, Step 200: Loss = 0.2724\n",
      "Epoch 2, Step 210: Loss = 0.2298\n",
      "Epoch 2, Step 220: Loss = 0.0012\n",
      "Epoch 2, Step 230: Loss = 0.0342\n",
      "Epoch 2, Step 240: Loss = 0.3128\n",
      "Epoch 2, Step 250: Loss = 0.0034\n",
      "Epoch 2, Step 260: Loss = 0.4137\n",
      "Epoch 2, Step 270: Loss = 0.3563\n",
      "Epoch 2, Step 280: Loss = 0.1204\n",
      "Epoch 2, Step 290: Loss = 0.2834\n",
      "Epoch 2, Step 300: Loss = 0.0565\n",
      "Epoch 2, Step 310: Loss = 0.0903\n",
      "Epoch 2, Step 320: Loss = 0.1629\n",
      "Epoch 2, Step 330: Loss = 0.0143\n",
      "Epoch 2, Step 340: Loss = 0.1015\n",
      "Epoch 2, Step 350: Loss = 0.5270\n",
      "Epoch 2, Step 360: Loss = 0.4762\n",
      "Epoch 2, Step 370: Loss = 0.3186\n",
      "Epoch 2, Step 380: Loss = 0.1062\n",
      "Epoch 2, Step 390: Loss = 0.1699\n",
      "Epoch 2, Step 400: Loss = 0.2106\n",
      "Epoch 2, Step 410: Loss = 0.1085\n",
      "Epoch 2, Step 420: Loss = 0.0000\n",
      "Epoch 2, Step 430: Loss = 0.1646\n",
      "Epoch 2, Step 440: Loss = 0.0655\n",
      "Epoch 2, Step 450: Loss = 0.3652\n",
      "Epoch 2, Step 460: Loss = 0.1155\n",
      "Epoch 2, Step 470: Loss = 0.2495\n",
      "Epoch 2, Step 480: Loss = 0.0264\n",
      "Epoch 2, Step 490: Loss = 0.0000\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Average Training Loss: 0.3473\n",
      "Validation Metrics:\n",
      "Val_loss: 0.3170\n",
      "Epoch 3, Step 0: Loss = 0.0000\n",
      "Epoch 3, Step 10: Loss = 0.0180\n",
      "Epoch 3, Step 20: Loss = 0.0330\n",
      "Epoch 3, Step 30: Loss = 0.2315\n",
      "Epoch 3, Step 40: Loss = 0.1057\n",
      "Epoch 3, Step 50: Loss = 0.1023\n",
      "Epoch 3, Step 60: Loss = 0.0138\n",
      "Epoch 3, Step 70: Loss = 0.1345\n",
      "Epoch 3, Step 80: Loss = 0.1988\n",
      "Epoch 3, Step 90: Loss = 0.3805\n",
      "Epoch 3, Step 100: Loss = 0.0151\n",
      "Epoch 3, Step 110: Loss = 0.0204\n",
      "Epoch 3, Step 120: Loss = 0.0101\n",
      "Epoch 3, Step 130: Loss = 0.0497\n",
      "Epoch 3, Step 140: Loss = 0.0373\n",
      "Epoch 3, Step 150: Loss = 0.1032\n",
      "Epoch 3, Step 160: Loss = 0.0000\n",
      "Epoch 3, Step 170: Loss = 0.0996\n",
      "Epoch 3, Step 180: Loss = 0.0412\n",
      "Epoch 3, Step 190: Loss = 0.0784\n",
      "Epoch 3, Step 200: Loss = 0.0000\n",
      "Epoch 3, Step 210: Loss = 0.1746\n",
      "Epoch 3, Step 220: Loss = 0.0607\n",
      "Epoch 3, Step 230: Loss = 0.1908\n",
      "Epoch 3, Step 240: Loss = 0.2407\n",
      "Epoch 3, Step 250: Loss = 0.0000\n",
      "Epoch 3, Step 260: Loss = 0.0088\n",
      "Epoch 3, Step 270: Loss = 0.2094\n",
      "Epoch 3, Step 280: Loss = 0.0000\n",
      "Epoch 3, Step 290: Loss = 0.1180\n",
      "Epoch 3, Step 300: Loss = 0.0592\n",
      "Epoch 3, Step 310: Loss = 0.1881\n",
      "Epoch 3, Step 320: Loss = 0.2578\n",
      "Epoch 3, Step 330: Loss = 0.0000\n",
      "Epoch 3, Step 340: Loss = 0.1359\n",
      "Epoch 3, Step 350: Loss = 0.1036\n",
      "Epoch 3, Step 360: Loss = 0.0914\n",
      "Epoch 3, Step 370: Loss = 0.2536\n",
      "Epoch 3, Step 380: Loss = 0.1104\n",
      "Epoch 3, Step 390: Loss = 0.0239\n",
      "Epoch 3, Step 400: Loss = 0.0850\n",
      "Epoch 3, Step 410: Loss = 0.0000\n",
      "Epoch 3, Step 420: Loss = 0.1603\n",
      "Epoch 3, Step 430: Loss = 0.0353\n",
      "Epoch 3, Step 440: Loss = 0.0000\n",
      "Epoch 3, Step 450: Loss = 0.1391\n",
      "Epoch 3, Step 460: Loss = 0.1587\n",
      "Epoch 3, Step 470: Loss = 0.0243\n",
      "Epoch 3, Step 480: Loss = 0.0140\n",
      "Epoch 3, Step 490: Loss = 0.1395\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Average Training Loss: 0.2509\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2412\n",
      "Epoch 4, Step 0: Loss = 0.0000\n",
      "Epoch 4, Step 10: Loss = 0.0993\n",
      "Epoch 4, Step 20: Loss = 0.2679\n",
      "Epoch 4, Step 30: Loss = 0.2827\n",
      "Epoch 4, Step 40: Loss = 0.1385\n",
      "Epoch 4, Step 50: Loss = 0.1143\n",
      "Epoch 4, Step 60: Loss = 0.0937\n",
      "Epoch 4, Step 70: Loss = 0.0403\n",
      "Epoch 4, Step 80: Loss = 0.0486\n",
      "Epoch 4, Step 90: Loss = 0.0996\n",
      "Epoch 4, Step 100: Loss = 0.0080\n",
      "Epoch 4, Step 110: Loss = 0.0000\n",
      "Epoch 4, Step 120: Loss = 0.0231\n",
      "Epoch 4, Step 130: Loss = 0.1076\n",
      "Epoch 4, Step 140: Loss = 0.1407\n",
      "Epoch 4, Step 150: Loss = 0.1566\n",
      "Epoch 4, Step 160: Loss = 0.0000\n",
      "Epoch 4, Step 170: Loss = 0.1939\n",
      "Epoch 4, Step 180: Loss = 0.0661\n",
      "Epoch 4, Step 190: Loss = 0.1378\n",
      "Epoch 4, Step 200: Loss = 0.1137\n",
      "Epoch 4, Step 210: Loss = 0.1291\n",
      "Epoch 4, Step 220: Loss = 0.1777\n",
      "Epoch 4, Step 230: Loss = 0.1387\n",
      "Epoch 4, Step 240: Loss = 0.0103\n",
      "Epoch 4, Step 250: Loss = 0.4322\n",
      "Epoch 4, Step 260: Loss = 0.0168\n",
      "Epoch 4, Step 270: Loss = 0.0000\n",
      "Epoch 4, Step 280: Loss = 0.0091\n",
      "Epoch 4, Step 290: Loss = 0.0000\n",
      "Epoch 4, Step 300: Loss = 0.1202\n",
      "Epoch 4, Step 310: Loss = 0.0201\n",
      "Epoch 4, Step 320: Loss = 0.0097\n",
      "Epoch 4, Step 330: Loss = 0.0532\n",
      "Epoch 4, Step 340: Loss = 0.0781\n",
      "Epoch 4, Step 350: Loss = 0.0839\n",
      "Epoch 4, Step 360: Loss = 0.0362\n",
      "Epoch 4, Step 370: Loss = 0.0574\n",
      "Epoch 4, Step 380: Loss = 0.2826\n",
      "Epoch 4, Step 390: Loss = 0.0000\n",
      "Epoch 4, Step 400: Loss = 0.0005\n",
      "Epoch 4, Step 410: Loss = 0.0364\n",
      "Epoch 4, Step 420: Loss = 0.1825\n",
      "Epoch 4, Step 430: Loss = 0.0000\n",
      "Epoch 4, Step 440: Loss = 0.0000\n",
      "Epoch 4, Step 450: Loss = 0.0009\n",
      "Epoch 4, Step 460: Loss = 0.3872\n",
      "Epoch 4, Step 470: Loss = 0.0403\n",
      "Epoch 4, Step 480: Loss = 0.1074\n",
      "Epoch 4, Step 490: Loss = 0.0308\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Average Training Loss: 0.1957\n",
      "Validation Metrics:\n",
      "Val_loss: 0.2190\n",
      "Epoch 5, Step 0: Loss = 0.0000\n",
      "Epoch 5, Step 10: Loss = 0.4767\n",
      "Epoch 5, Step 20: Loss = 0.0129\n",
      "Epoch 5, Step 30: Loss = 0.0000\n",
      "Epoch 5, Step 40: Loss = 0.0454\n",
      "Epoch 5, Step 50: Loss = 0.0365\n",
      "Epoch 5, Step 60: Loss = 0.0198\n",
      "Epoch 5, Step 70: Loss = 0.0229\n",
      "Epoch 5, Step 80: Loss = 0.0551\n",
      "Epoch 5, Step 90: Loss = 0.0000\n",
      "Epoch 5, Step 100: Loss = 0.0000\n",
      "Epoch 5, Step 110: Loss = 0.1303\n",
      "Epoch 5, Step 120: Loss = 0.0000\n",
      "Epoch 5, Step 130: Loss = 0.0442\n",
      "Epoch 5, Step 140: Loss = 0.0666\n",
      "Epoch 5, Step 150: Loss = 0.1740\n",
      "Epoch 5, Step 160: Loss = 0.0000\n",
      "Epoch 5, Step 170: Loss = 0.0000\n",
      "Epoch 5, Step 180: Loss = 0.0079\n",
      "Epoch 5, Step 190: Loss = 0.0250\n",
      "Epoch 5, Step 200: Loss = 0.0370\n",
      "Epoch 5, Step 210: Loss = 0.0925\n",
      "Epoch 5, Step 220: Loss = 0.0143\n",
      "Epoch 5, Step 230: Loss = 0.0516\n",
      "Epoch 5, Step 240: Loss = 0.0000\n",
      "Epoch 5, Step 250: Loss = 0.1967\n",
      "Epoch 5, Step 260: Loss = 0.1548\n",
      "Epoch 5, Step 270: Loss = 0.0000\n",
      "Epoch 5, Step 280: Loss = 0.1623\n",
      "Epoch 5, Step 290: Loss = 0.0000\n",
      "Epoch 5, Step 300: Loss = 0.0465\n",
      "Epoch 5, Step 310: Loss = 0.0000\n",
      "Epoch 5, Step 320: Loss = 0.0281\n",
      "Epoch 5, Step 330: Loss = 0.0000\n",
      "Epoch 5, Step 340: Loss = 0.0000\n",
      "Epoch 5, Step 350: Loss = 0.0433\n",
      "Epoch 5, Step 360: Loss = 0.3482\n",
      "Epoch 5, Step 370: Loss = 0.2405\n",
      "Epoch 5, Step 380: Loss = 0.0000\n",
      "Epoch 5, Step 390: Loss = 0.0174\n",
      "Epoch 5, Step 400: Loss = 0.0290\n",
      "Epoch 5, Step 410: Loss = 0.3593\n",
      "Epoch 5, Step 420: Loss = 0.0643\n",
      "Epoch 5, Step 430: Loss = 0.0197\n",
      "Epoch 5, Step 440: Loss = 0.1739\n",
      "Epoch 5, Step 450: Loss = 0.0538\n",
      "Epoch 5, Step 460: Loss = 0.4640\n",
      "Epoch 5, Step 470: Loss = 0.0000\n",
      "Epoch 5, Step 480: Loss = 0.0981\n",
      "Epoch 5, Step 490: Loss = 0.0000\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Average Training Loss: 0.1853\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1783\n",
      "Epoch 6, Step 0: Loss = 0.2441\n",
      "Epoch 6, Step 10: Loss = 0.1010\n",
      "Epoch 6, Step 20: Loss = 0.0573\n",
      "Epoch 6, Step 30: Loss = 0.3523\n",
      "Epoch 6, Step 40: Loss = 0.0667\n",
      "Epoch 6, Step 50: Loss = 0.1370\n",
      "Epoch 6, Step 60: Loss = 0.0065\n",
      "Epoch 6, Step 70: Loss = 0.0836\n",
      "Epoch 6, Step 80: Loss = 0.0219\n",
      "Epoch 6, Step 90: Loss = 0.0479\n",
      "Epoch 6, Step 100: Loss = 0.0773\n",
      "Epoch 6, Step 110: Loss = 0.0000\n",
      "Epoch 6, Step 120: Loss = 0.0000\n",
      "Epoch 6, Step 130: Loss = 0.0000\n",
      "Epoch 6, Step 140: Loss = 0.0678\n",
      "Epoch 6, Step 150: Loss = 0.0000\n",
      "Epoch 6, Step 160: Loss = 0.1380\n",
      "Epoch 6, Step 170: Loss = 0.0510\n",
      "Epoch 6, Step 180: Loss = 0.0104\n",
      "Epoch 6, Step 190: Loss = 0.1585\n",
      "Epoch 6, Step 200: Loss = 0.3468\n",
      "Epoch 6, Step 210: Loss = 0.0000\n",
      "Epoch 6, Step 220: Loss = 0.0136\n",
      "Epoch 6, Step 230: Loss = 0.0000\n",
      "Epoch 6, Step 240: Loss = 0.0183\n",
      "Epoch 6, Step 250: Loss = 0.0000\n",
      "Epoch 6, Step 260: Loss = 0.1419\n",
      "Epoch 6, Step 270: Loss = 0.1398\n",
      "Epoch 6, Step 280: Loss = 0.0355\n",
      "Epoch 6, Step 290: Loss = 0.0000\n",
      "Epoch 6, Step 300: Loss = 0.0000\n",
      "Epoch 6, Step 310: Loss = 0.0966\n",
      "Epoch 6, Step 320: Loss = 0.1304\n",
      "Epoch 6, Step 330: Loss = 0.3114\n",
      "Epoch 6, Step 340: Loss = 0.0172\n",
      "Epoch 6, Step 350: Loss = 0.0245\n",
      "Epoch 6, Step 360: Loss = 0.0400\n",
      "Epoch 6, Step 370: Loss = 0.1368\n",
      "Epoch 6, Step 380: Loss = 0.0760\n",
      "Epoch 6, Step 390: Loss = 0.0303\n",
      "Epoch 6, Step 400: Loss = 0.0740\n",
      "Epoch 6, Step 410: Loss = 0.0000\n",
      "Epoch 6, Step 420: Loss = 0.0225\n",
      "Epoch 6, Step 430: Loss = 0.1820\n",
      "Epoch 6, Step 440: Loss = 0.2155\n",
      "Epoch 6, Step 450: Loss = 0.0605\n",
      "Epoch 6, Step 460: Loss = 0.3067\n",
      "Epoch 6, Step 470: Loss = 0.0101\n",
      "Epoch 6, Step 480: Loss = 0.0027\n",
      "Epoch 6, Step 490: Loss = 0.0105\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Average Training Loss: 0.1525\n",
      "Validation Metrics:\n",
      "Val_loss: 0.1940\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    torch.cuda.set_device(0)  \n",
    "    return torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataset class with data cleaning\n",
    "class ContrastiveEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        # Clean each text field\n",
    "        emails_df['sender'] = emails_df['sender'].apply(clean_text)\n",
    "        emails_df['subject'] = emails_df['subject'].apply(clean_text)\n",
    "        emails_df['body'] = emails_df['body'].apply(clean_text)\n",
    "        \n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.ham_indices = self.emails_df[self.emails_df['label'] == 0].index.tolist()\n",
    "        self.phish_indices = self.emails_df[self.emails_df['label'] == 1].index.tolist()\n",
    "        if not (self.ham_indices and self.phish_indices):\n",
    "            raise ValueError(\"Dataset must contain examples of both classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def _get_random_email_idx(self, label):\n",
    "        indices = self.ham_indices if label == 0 else self.phish_indices\n",
    "        return np.random.choice(indices)\n",
    "\n",
    "    def _prepare_email_input(self, email):\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        tokenized_output = self.tokenizer(input_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        return tokenized_output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_email = self.emails_df.iloc[idx]\n",
    "        anchor_label = anchor_email['label']\n",
    "        positives, negatives = [], []\n",
    "\n",
    "        for _ in range(3):\n",
    "            pos_idx = self._get_random_email_idx(anchor_label)\n",
    "            neg_idx = self._get_random_email_idx(1 - anchor_label)\n",
    "            positives.append(self._prepare_email_input(self.emails_df.iloc[pos_idx]))\n",
    "            negatives.append(self._prepare_email_input(self.emails_df.iloc[neg_idx]))\n",
    "\n",
    "        anchor_inputs = self._prepare_email_input(anchor_email)\n",
    "        return {\n",
    "            'anchor_input_ids': anchor_inputs['input_ids'].squeeze(),\n",
    "            'anchor_attention_mask': anchor_inputs['attention_mask'].squeeze(),\n",
    "            'positive_input_ids': torch.stack([p['input_ids'].squeeze() for p in positives]),\n",
    "            'positive_attention_mask': torch.stack([p['attention_mask'].squeeze() for p in positives]),\n",
    "            'negative_input_ids': torch.stack([n['input_ids'].squeeze() for n in negatives]),\n",
    "            'negative_attention_mask': torch.stack([n['attention_mask'].squeeze() for n in negatives])\n",
    "        }\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model_config.use_cache = False\n",
    "\n",
    "   \n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        config=model_config, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"SEQ_CLS\")\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=6, margin=1.0, accumulation_steps=2):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    model = model.to(device).to(torch.float16)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                # Compute embeddings for anchor, positive, and negative samples in parallel\n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                # Reshape positive and negative embeddings to match the batch structure (batch_size, 3, embedding_dim)\n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "                # Calculate the triplet loss for each triplet in the batch\n",
    "                loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "                loss /= accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        val_metrics = evaluate_model(model, val_loader, device, margin)\n",
    "        print_metrics(epoch, total_loss, len(train_loader), val_metrics)\n",
    "\n",
    "        if val_metrics['val_loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['val_loss']\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model(model, val_loader, device, margin=1.0):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "               \n",
    "                anchor_embeddings = model(input_ids=batch['anchor_input_ids'], attention_mask=batch['anchor_attention_mask']).logits\n",
    "                positive_embeddings = model(input_ids=batch['positive_input_ids'].view(-1, batch['positive_input_ids'].size(-1)), attention_mask=batch['positive_attention_mask'].view(-1, batch['positive_attention_mask'].size(-1))).logits\n",
    "                negative_embeddings = model(input_ids=batch['negative_input_ids'].view(-1, batch['negative_input_ids'].size(-1)), attention_mask=batch['negative_attention_mask'].view(-1, batch['negative_attention_mask'].size(-1))).logits\n",
    "\n",
    "                \n",
    "                positive_embeddings = positive_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "                negative_embeddings = negative_embeddings.view(anchor_embeddings.size(0), 3, -1)\n",
    "\n",
    "               \n",
    "                val_loss = F.triplet_margin_loss(\n",
    "                    anchor_embeddings.unsqueeze(1).expand(-1, 3, -1),\n",
    "                    positive_embeddings,\n",
    "                    negative_embeddings,\n",
    "                    margin=margin\n",
    "                )\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "    return {'val_loss': avg_val_loss}\n",
    "\n",
    "\n",
    "# Main function\n",
    "# Data cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  \n",
    "        text = \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def print_metrics(epoch, total_loss, num_batches, val_metrics):\n",
    "    \n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "def main():\n",
    "    \n",
    "    device = setup_environment()\n",
    "    model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/final_data.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "\n",
    "   \n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "    train_dataset = ContrastiveEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = ContrastiveEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 6\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "    best_model_state = train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/llama_7B_contrastive_classification_model_lora_chat\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98b537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce7b07829fb49379fe3f5de3b8ffc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3ac04cdd594c2da20d586a97c6f219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/8 | Step 0/500] - Loss: 4.2860\n",
      "[Epoch 1/8 | Step 10/500] - Loss: 4.0605\n",
      "[Epoch 1/8 | Step 20/500] - Loss: 4.1956\n",
      "[Epoch 1/8 | Step 30/500] - Loss: 4.6349\n",
      "[Epoch 1/8 | Step 40/500] - Loss: 4.7998\n",
      "[Epoch 1/8 | Step 50/500] - Loss: 4.4766\n",
      "[Epoch 1/8 | Step 60/500] - Loss: 4.5783\n",
      "[Epoch 1/8 | Step 70/500] - Loss: 4.6406\n",
      "[Epoch 1/8 | Step 80/500] - Loss: 4.5046\n",
      "[Epoch 1/8 | Step 90/500] - Loss: 4.3593\n",
      "[Epoch 1/8 | Step 100/500] - Loss: 4.4299\n",
      "[Epoch 1/8 | Step 110/500] - Loss: 4.5442\n",
      "[Epoch 1/8 | Step 120/500] - Loss: 4.5273\n",
      "[Epoch 1/8 | Step 130/500] - Loss: 4.5207\n",
      "[Epoch 1/8 | Step 140/500] - Loss: 4.4689\n",
      "[Epoch 1/8 | Step 150/500] - Loss: 4.3957\n",
      "[Epoch 1/8 | Step 160/500] - Loss: 4.5157\n",
      "[Epoch 1/8 | Step 170/500] - Loss: 4.4796\n",
      "[Epoch 1/8 | Step 180/500] - Loss: 4.4771\n",
      "[Epoch 1/8 | Step 190/500] - Loss: 4.3955\n",
      "[Epoch 1/8 | Step 200/500] - Loss: 4.3488\n",
      "[Epoch 1/8 | Step 210/500] - Loss: 4.3235\n",
      "[Epoch 1/8 | Step 220/500] - Loss: 4.3200\n",
      "[Epoch 1/8 | Step 230/500] - Loss: 4.2350\n",
      "[Epoch 1/8 | Step 240/500] - Loss: 4.1433\n",
      "[Epoch 1/8 | Step 250/500] - Loss: 4.0952\n",
      "[Epoch 1/8 | Step 260/500] - Loss: 4.0934\n",
      "[Epoch 1/8 | Step 270/500] - Loss: 4.0624\n",
      "[Epoch 1/8 | Step 280/500] - Loss: 4.0133\n",
      "[Epoch 1/8 | Step 290/500] - Loss: 4.0018\n",
      "[Epoch 1/8 | Step 300/500] - Loss: 3.9575\n",
      "[Epoch 1/8 | Step 310/500] - Loss: 3.8872\n",
      "[Epoch 1/8 | Step 320/500] - Loss: 3.8380\n",
      "[Epoch 1/8 | Step 330/500] - Loss: 3.8350\n",
      "[Epoch 1/8 | Step 340/500] - Loss: 3.7779\n",
      "[Epoch 1/8 | Step 350/500] - Loss: 3.7517\n",
      "[Epoch 1/8 | Step 360/500] - Loss: 3.6812\n",
      "[Epoch 1/8 | Step 370/500] - Loss: 3.6571\n",
      "[Epoch 1/8 | Step 380/500] - Loss: 3.6266\n",
      "[Epoch 1/8 | Step 390/500] - Loss: 3.5809\n",
      "[Epoch 1/8 | Step 400/500] - Loss: 3.5320\n",
      "[Epoch 1/8 | Step 410/500] - Loss: 3.5039\n",
      "[Epoch 1/8 | Step 420/500] - Loss: 3.4664\n",
      "[Epoch 1/8 | Step 430/500] - Loss: 3.4138\n",
      "[Epoch 1/8 | Step 440/500] - Loss: 3.3769\n",
      "[Epoch 1/8 | Step 450/500] - Loss: 3.3604\n",
      "[Epoch 1/8 | Step 460/500] - Loss: 3.3185\n",
      "[Epoch 1/8 | Step 470/500] - Loss: 3.2915\n",
      "[Epoch 1/8 | Step 480/500] - Loss: 3.2532\n",
      "[Epoch 1/8 | Step 490/500] - Loss: 3.2281\n",
      "Epoch 1/8 - Avg Train Loss: 3.1984, Val Loss: 1.7842\n",
      "[Epoch 2/8 | Step 0/500] - Loss: 0.2668\n",
      "[Epoch 2/8 | Step 10/500] - Loss: 1.3528\n",
      "[Epoch 2/8 | Step 20/500] - Loss: 1.5949\n",
      "[Epoch 2/8 | Step 30/500] - Loss: 1.6038\n",
      "[Epoch 2/8 | Step 40/500] - Loss: 1.6234\n",
      "[Epoch 2/8 | Step 50/500] - Loss: 1.6055\n",
      "[Epoch 2/8 | Step 60/500] - Loss: 1.6094\n",
      "[Epoch 2/8 | Step 70/500] - Loss: 1.6752\n",
      "[Epoch 2/8 | Step 80/500] - Loss: 1.6325\n",
      "[Epoch 2/8 | Step 90/500] - Loss: 1.6571\n",
      "[Epoch 2/8 | Step 100/500] - Loss: 1.6926\n",
      "[Epoch 2/8 | Step 110/500] - Loss: 1.6640\n",
      "[Epoch 2/8 | Step 120/500] - Loss: 1.6445\n",
      "[Epoch 2/8 | Step 130/500] - Loss: 1.6681\n",
      "[Epoch 2/8 | Step 140/500] - Loss: 1.6537\n",
      "[Epoch 2/8 | Step 150/500] - Loss: 1.6586\n",
      "[Epoch 2/8 | Step 160/500] - Loss: 1.7152\n",
      "[Epoch 2/8 | Step 170/500] - Loss: 1.7252\n",
      "[Epoch 2/8 | Step 180/500] - Loss: 1.7166\n",
      "[Epoch 2/8 | Step 190/500] - Loss: 1.7155\n",
      "[Epoch 2/8 | Step 200/500] - Loss: 1.7201\n",
      "[Epoch 2/8 | Step 210/500] - Loss: 1.7575\n",
      "[Epoch 2/8 | Step 220/500] - Loss: 1.7721\n",
      "[Epoch 2/8 | Step 230/500] - Loss: 1.7608\n",
      "[Epoch 2/8 | Step 240/500] - Loss: 1.7409\n",
      "[Epoch 2/8 | Step 250/500] - Loss: 1.7267\n",
      "[Epoch 2/8 | Step 260/500] - Loss: 1.7422\n",
      "[Epoch 2/8 | Step 270/500] - Loss: 1.7341\n",
      "[Epoch 2/8 | Step 280/500] - Loss: 1.7343\n",
      "[Epoch 2/8 | Step 290/500] - Loss: 1.7328\n",
      "[Epoch 2/8 | Step 300/500] - Loss: 1.7390\n",
      "[Epoch 2/8 | Step 310/500] - Loss: 1.7521\n",
      "[Epoch 2/8 | Step 320/500] - Loss: 1.7385\n",
      "[Epoch 2/8 | Step 330/500] - Loss: 1.7309\n",
      "[Epoch 2/8 | Step 340/500] - Loss: 1.7313\n",
      "[Epoch 2/8 | Step 350/500] - Loss: 1.7174\n",
      "[Epoch 2/8 | Step 360/500] - Loss: 1.7230\n",
      "[Epoch 2/8 | Step 370/500] - Loss: 1.7215\n",
      "[Epoch 2/8 | Step 380/500] - Loss: 1.7330\n",
      "[Epoch 2/8 | Step 390/500] - Loss: 1.7401\n",
      "[Epoch 2/8 | Step 400/500] - Loss: 1.7466\n",
      "[Epoch 2/8 | Step 410/500] - Loss: 1.7526\n",
      "[Epoch 2/8 | Step 420/500] - Loss: 1.7423\n",
      "[Epoch 2/8 | Step 430/500] - Loss: 1.7511\n",
      "[Epoch 2/8 | Step 440/500] - Loss: 1.7614\n",
      "[Epoch 2/8 | Step 450/500] - Loss: 1.7641\n",
      "[Epoch 2/8 | Step 460/500] - Loss: 1.7677\n",
      "[Epoch 2/8 | Step 470/500] - Loss: 1.7704\n",
      "[Epoch 2/8 | Step 480/500] - Loss: 1.7765\n",
      "[Epoch 2/8 | Step 490/500] - Loss: 1.7776\n",
      "Epoch 2/8 - Avg Train Loss: 1.7780, Val Loss: 1.6561\n",
      "[Epoch 3/8 | Step 0/500] - Loss: 2.4576\n",
      "[Epoch 3/8 | Step 10/500] - Loss: 1.5696\n",
      "[Epoch 3/8 | Step 20/500] - Loss: 1.8315\n",
      "[Epoch 3/8 | Step 30/500] - Loss: 1.7292\n",
      "[Epoch 3/8 | Step 40/500] - Loss: 1.6568\n",
      "[Epoch 3/8 | Step 50/500] - Loss: 1.7423\n",
      "[Epoch 3/8 | Step 60/500] - Loss: 1.6716\n",
      "[Epoch 3/8 | Step 70/500] - Loss: 1.5877\n",
      "[Epoch 3/8 | Step 80/500] - Loss: 1.6815\n",
      "[Epoch 3/8 | Step 90/500] - Loss: 1.7287\n",
      "[Epoch 3/8 | Step 100/500] - Loss: 1.6462\n",
      "[Epoch 3/8 | Step 110/500] - Loss: 1.6281\n",
      "[Epoch 3/8 | Step 120/500] - Loss: 1.6075\n",
      "[Epoch 3/8 | Step 130/500] - Loss: 1.6054\n",
      "[Epoch 3/8 | Step 140/500] - Loss: 1.6242\n",
      "[Epoch 3/8 | Step 150/500] - Loss: 1.6352\n",
      "[Epoch 3/8 | Step 160/500] - Loss: 1.6669\n",
      "[Epoch 3/8 | Step 170/500] - Loss: 1.7161\n",
      "[Epoch 3/8 | Step 180/500] - Loss: 1.6876\n",
      "[Epoch 3/8 | Step 190/500] - Loss: 1.6893\n",
      "[Epoch 3/8 | Step 200/500] - Loss: 1.6954\n",
      "[Epoch 3/8 | Step 210/500] - Loss: 1.7241\n",
      "[Epoch 3/8 | Step 220/500] - Loss: 1.7488\n",
      "[Epoch 3/8 | Step 230/500] - Loss: 1.7865\n",
      "[Epoch 3/8 | Step 240/500] - Loss: 1.7626\n",
      "[Epoch 3/8 | Step 250/500] - Loss: 1.7566\n",
      "[Epoch 3/8 | Step 260/500] - Loss: 1.7437\n",
      "[Epoch 3/8 | Step 270/500] - Loss: 1.7313\n",
      "[Epoch 3/8 | Step 280/500] - Loss: 1.7417\n",
      "[Epoch 3/8 | Step 290/500] - Loss: 1.7104\n",
      "[Epoch 3/8 | Step 300/500] - Loss: 1.7416\n",
      "[Epoch 3/8 | Step 310/500] - Loss: 1.7228\n",
      "[Epoch 3/8 | Step 320/500] - Loss: 1.7098\n",
      "[Epoch 3/8 | Step 330/500] - Loss: 1.7028\n",
      "[Epoch 3/8 | Step 340/500] - Loss: 1.6911\n",
      "[Epoch 3/8 | Step 350/500] - Loss: 1.7045\n",
      "[Epoch 3/8 | Step 360/500] - Loss: 1.7037\n",
      "[Epoch 3/8 | Step 370/500] - Loss: 1.7040\n",
      "[Epoch 3/8 | Step 380/500] - Loss: 1.7046\n",
      "[Epoch 3/8 | Step 390/500] - Loss: 1.7106\n",
      "[Epoch 3/8 | Step 400/500] - Loss: 1.7148\n",
      "[Epoch 3/8 | Step 410/500] - Loss: 1.7215\n",
      "[Epoch 3/8 | Step 420/500] - Loss: 1.7368\n",
      "[Epoch 3/8 | Step 430/500] - Loss: 1.7264\n",
      "[Epoch 3/8 | Step 440/500] - Loss: 1.7194\n",
      "[Epoch 3/8 | Step 450/500] - Loss: 1.7201\n",
      "[Epoch 3/8 | Step 460/500] - Loss: 1.7109\n",
      "[Epoch 3/8 | Step 470/500] - Loss: 1.7110\n",
      "[Epoch 3/8 | Step 480/500] - Loss: 1.7138\n",
      "[Epoch 3/8 | Step 490/500] - Loss: 1.7131\n",
      "Epoch 3/8 - Avg Train Loss: 1.7184, Val Loss: 1.6587\n",
      "[Epoch 4/8 | Step 0/500] - Loss: 1.9678\n",
      "[Epoch 4/8 | Step 10/500] - Loss: 1.7174\n",
      "[Epoch 4/8 | Step 20/500] - Loss: 1.6554\n",
      "[Epoch 4/8 | Step 30/500] - Loss: 1.8668\n",
      "[Epoch 4/8 | Step 40/500] - Loss: 1.7118\n",
      "[Epoch 4/8 | Step 50/500] - Loss: 1.8177\n",
      "[Epoch 4/8 | Step 60/500] - Loss: 1.6918\n",
      "[Epoch 4/8 | Step 70/500] - Loss: 1.7315\n",
      "[Epoch 4/8 | Step 80/500] - Loss: 1.7284\n",
      "[Epoch 4/8 | Step 90/500] - Loss: 1.7424\n",
      "[Epoch 4/8 | Step 100/500] - Loss: 1.7988\n",
      "[Epoch 4/8 | Step 110/500] - Loss: 1.7627\n",
      "[Epoch 4/8 | Step 120/500] - Loss: 1.7829\n",
      "[Epoch 4/8 | Step 130/500] - Loss: 1.7738\n",
      "[Epoch 4/8 | Step 140/500] - Loss: 1.7906\n",
      "[Epoch 4/8 | Step 150/500] - Loss: 1.7858\n",
      "[Epoch 4/8 | Step 160/500] - Loss: 1.7610\n",
      "[Epoch 4/8 | Step 170/500] - Loss: 1.8051\n",
      "[Epoch 4/8 | Step 180/500] - Loss: 1.7607\n",
      "[Epoch 4/8 | Step 190/500] - Loss: 1.7251\n",
      "[Epoch 4/8 | Step 200/500] - Loss: 1.6934\n",
      "[Epoch 4/8 | Step 210/500] - Loss: 1.6894\n",
      "[Epoch 4/8 | Step 220/500] - Loss: 1.6879\n",
      "[Epoch 4/8 | Step 230/500] - Loss: 1.6796\n",
      "[Epoch 4/8 | Step 240/500] - Loss: 1.6740\n",
      "[Epoch 4/8 | Step 250/500] - Loss: 1.6592\n",
      "[Epoch 4/8 | Step 260/500] - Loss: 1.6804\n",
      "[Epoch 4/8 | Step 270/500] - Loss: 1.6749\n",
      "[Epoch 4/8 | Step 280/500] - Loss: 1.6613\n",
      "[Epoch 4/8 | Step 290/500] - Loss: 1.6674\n",
      "[Epoch 4/8 | Step 300/500] - Loss: 1.6537\n",
      "[Epoch 4/8 | Step 310/500] - Loss: 1.6537\n",
      "[Epoch 4/8 | Step 320/500] - Loss: 1.6570\n",
      "[Epoch 4/8 | Step 330/500] - Loss: 1.6433\n",
      "[Epoch 4/8 | Step 340/500] - Loss: 1.6497\n",
      "[Epoch 4/8 | Step 350/500] - Loss: 1.6597\n",
      "[Epoch 4/8 | Step 360/500] - Loss: 1.6459\n",
      "[Epoch 4/8 | Step 370/500] - Loss: 1.6513\n",
      "[Epoch 4/8 | Step 380/500] - Loss: 1.6577\n",
      "[Epoch 4/8 | Step 390/500] - Loss: 1.6480\n",
      "[Epoch 4/8 | Step 400/500] - Loss: 1.6274\n",
      "[Epoch 4/8 | Step 410/500] - Loss: 1.6406\n",
      "[Epoch 4/8 | Step 420/500] - Loss: 1.6511\n",
      "[Epoch 4/8 | Step 430/500] - Loss: 1.6596\n",
      "[Epoch 4/8 | Step 440/500] - Loss: 1.6709\n",
      "[Epoch 4/8 | Step 450/500] - Loss: 1.6756\n",
      "[Epoch 4/8 | Step 460/500] - Loss: 1.6908\n",
      "[Epoch 4/8 | Step 470/500] - Loss: 1.6924\n",
      "[Epoch 4/8 | Step 480/500] - Loss: 1.6783\n",
      "[Epoch 4/8 | Step 490/500] - Loss: 1.6829\n",
      "Epoch 4/8 - Avg Train Loss: 1.6762, Val Loss: 1.6365\n",
      "[Epoch 5/8 | Step 0/500] - Loss: 2.0575\n",
      "[Epoch 5/8 | Step 10/500] - Loss: 1.5135\n",
      "[Epoch 5/8 | Step 20/500] - Loss: 1.6404\n",
      "[Epoch 5/8 | Step 30/500] - Loss: 1.8174\n",
      "[Epoch 5/8 | Step 40/500] - Loss: 1.9422\n",
      "[Epoch 5/8 | Step 50/500] - Loss: 1.7723\n",
      "[Epoch 5/8 | Step 60/500] - Loss: 1.7242\n",
      "[Epoch 5/8 | Step 70/500] - Loss: 1.7717\n",
      "[Epoch 5/8 | Step 80/500] - Loss: 1.7470\n",
      "[Epoch 5/8 | Step 90/500] - Loss: 1.7034\n",
      "[Epoch 5/8 | Step 100/500] - Loss: 1.6345\n",
      "[Epoch 5/8 | Step 110/500] - Loss: 1.7132\n",
      "[Epoch 5/8 | Step 120/500] - Loss: 1.7201\n",
      "[Epoch 5/8 | Step 130/500] - Loss: 1.6971\n",
      "[Epoch 5/8 | Step 140/500] - Loss: 1.6594\n",
      "[Epoch 5/8 | Step 150/500] - Loss: 1.6360\n",
      "[Epoch 5/8 | Step 160/500] - Loss: 1.6225\n",
      "[Epoch 5/8 | Step 170/500] - Loss: 1.5883\n",
      "[Epoch 5/8 | Step 180/500] - Loss: 1.6026\n",
      "[Epoch 5/8 | Step 190/500] - Loss: 1.6014\n",
      "[Epoch 5/8 | Step 200/500] - Loss: 1.6051\n",
      "[Epoch 5/8 | Step 210/500] - Loss: 1.5911\n",
      "[Epoch 5/8 | Step 220/500] - Loss: 1.5942\n",
      "[Epoch 5/8 | Step 230/500] - Loss: 1.6300\n",
      "[Epoch 5/8 | Step 240/500] - Loss: 1.6344\n",
      "[Epoch 5/8 | Step 250/500] - Loss: 1.6483\n",
      "[Epoch 5/8 | Step 260/500] - Loss: 1.6393\n",
      "[Epoch 5/8 | Step 270/500] - Loss: 1.6152\n",
      "[Epoch 5/8 | Step 280/500] - Loss: 1.6074\n",
      "[Epoch 5/8 | Step 290/500] - Loss: 1.6247\n",
      "[Epoch 5/8 | Step 300/500] - Loss: 1.6255\n",
      "[Epoch 5/8 | Step 310/500] - Loss: 1.6190\n",
      "[Epoch 5/8 | Step 320/500] - Loss: 1.6149\n",
      "[Epoch 5/8 | Step 330/500] - Loss: 1.6111\n",
      "[Epoch 5/8 | Step 340/500] - Loss: 1.6069\n",
      "[Epoch 5/8 | Step 350/500] - Loss: 1.5958\n",
      "[Epoch 5/8 | Step 360/500] - Loss: 1.5853\n",
      "[Epoch 5/8 | Step 370/500] - Loss: 1.6098\n",
      "[Epoch 5/8 | Step 380/500] - Loss: 1.5928\n",
      "[Epoch 5/8 | Step 390/500] - Loss: 1.6091\n",
      "[Epoch 5/8 | Step 400/500] - Loss: 1.6025\n",
      "[Epoch 5/8 | Step 410/500] - Loss: 1.6124\n",
      "[Epoch 5/8 | Step 420/500] - Loss: 1.6241\n",
      "[Epoch 5/8 | Step 430/500] - Loss: 1.6181\n",
      "[Epoch 5/8 | Step 440/500] - Loss: 1.6201\n",
      "[Epoch 5/8 | Step 450/500] - Loss: 1.6266\n",
      "[Epoch 5/8 | Step 460/500] - Loss: 1.6355\n",
      "[Epoch 5/8 | Step 470/500] - Loss: 1.6251\n",
      "[Epoch 5/8 | Step 480/500] - Loss: 1.6210\n",
      "[Epoch 5/8 | Step 490/500] - Loss: 1.6229\n",
      "Epoch 5/8 - Avg Train Loss: 1.6307, Val Loss: 1.6358\n",
      "[Epoch 6/8 | Step 0/500] - Loss: 1.0783\n",
      "[Epoch 6/8 | Step 10/500] - Loss: 1.3279\n",
      "[Epoch 6/8 | Step 20/500] - Loss: 1.4267\n",
      "[Epoch 6/8 | Step 30/500] - Loss: 1.5957\n",
      "[Epoch 6/8 | Step 40/500] - Loss: 1.5955\n",
      "[Epoch 6/8 | Step 50/500] - Loss: 1.6043\n",
      "[Epoch 6/8 | Step 60/500] - Loss: 1.5055\n",
      "[Epoch 6/8 | Step 70/500] - Loss: 1.4884\n",
      "[Epoch 6/8 | Step 80/500] - Loss: 1.5650\n",
      "[Epoch 6/8 | Step 90/500] - Loss: 1.5575\n",
      "[Epoch 6/8 | Step 100/500] - Loss: 1.5428\n",
      "[Epoch 6/8 | Step 110/500] - Loss: 1.5475\n",
      "[Epoch 6/8 | Step 120/500] - Loss: 1.5805\n",
      "[Epoch 6/8 | Step 130/500] - Loss: 1.6033\n",
      "[Epoch 6/8 | Step 140/500] - Loss: 1.5741\n",
      "[Epoch 6/8 | Step 150/500] - Loss: 1.6233\n",
      "[Epoch 6/8 | Step 160/500] - Loss: 1.6047\n",
      "[Epoch 6/8 | Step 170/500] - Loss: 1.6071\n",
      "[Epoch 6/8 | Step 180/500] - Loss: 1.5963\n",
      "[Epoch 6/8 | Step 190/500] - Loss: 1.5857\n",
      "[Epoch 6/8 | Step 200/500] - Loss: 1.6294\n",
      "[Epoch 6/8 | Step 210/500] - Loss: 1.6491\n",
      "[Epoch 6/8 | Step 220/500] - Loss: 1.6283\n",
      "[Epoch 6/8 | Step 230/500] - Loss: 1.6166\n",
      "[Epoch 6/8 | Step 240/500] - Loss: 1.6076\n",
      "[Epoch 6/8 | Step 250/500] - Loss: 1.6080\n",
      "[Epoch 6/8 | Step 260/500] - Loss: 1.6260\n",
      "[Epoch 6/8 | Step 270/500] - Loss: 1.6316\n",
      "[Epoch 6/8 | Step 280/500] - Loss: 1.6339\n",
      "[Epoch 6/8 | Step 290/500] - Loss: 1.6337\n",
      "[Epoch 6/8 | Step 300/500] - Loss: 1.6279\n",
      "[Epoch 6/8 | Step 310/500] - Loss: 1.6299\n",
      "[Epoch 6/8 | Step 320/500] - Loss: 1.6363\n",
      "[Epoch 6/8 | Step 330/500] - Loss: 1.6278\n",
      "[Epoch 6/8 | Step 340/500] - Loss: 1.6185\n",
      "[Epoch 6/8 | Step 350/500] - Loss: 1.6183\n",
      "[Epoch 6/8 | Step 360/500] - Loss: 1.6197\n",
      "[Epoch 6/8 | Step 370/500] - Loss: 1.6266\n",
      "[Epoch 6/8 | Step 380/500] - Loss: 1.6145\n",
      "[Epoch 6/8 | Step 390/500] - Loss: 1.6213\n",
      "[Epoch 6/8 | Step 400/500] - Loss: 1.6226\n",
      "[Epoch 6/8 | Step 410/500] - Loss: 1.6082\n",
      "[Epoch 6/8 | Step 420/500] - Loss: 1.6339\n",
      "[Epoch 6/8 | Step 430/500] - Loss: 1.6181\n",
      "[Epoch 6/8 | Step 440/500] - Loss: 1.6199\n",
      "[Epoch 6/8 | Step 450/500] - Loss: 1.6120\n",
      "[Epoch 6/8 | Step 460/500] - Loss: 1.6118\n",
      "[Epoch 6/8 | Step 470/500] - Loss: 1.6064\n",
      "[Epoch 6/8 | Step 480/500] - Loss: 1.6126\n",
      "[Epoch 6/8 | Step 490/500] - Loss: 1.6033\n",
      "Epoch 6/8 - Avg Train Loss: 1.6098, Val Loss: 1.6304\n",
      "[Epoch 7/8 | Step 0/500] - Loss: 2.3020\n",
      "[Epoch 7/8 | Step 10/500] - Loss: 1.4223\n",
      "[Epoch 7/8 | Step 20/500] - Loss: 1.2660\n",
      "[Epoch 7/8 | Step 30/500] - Loss: 1.3354\n",
      "[Epoch 7/8 | Step 40/500] - Loss: 1.2249\n",
      "[Epoch 7/8 | Step 50/500] - Loss: 1.2301\n",
      "[Epoch 7/8 | Step 60/500] - Loss: 1.2866\n",
      "[Epoch 7/8 | Step 70/500] - Loss: 1.2992\n",
      "[Epoch 7/8 | Step 80/500] - Loss: 1.3327\n",
      "[Epoch 7/8 | Step 90/500] - Loss: 1.3403\n",
      "[Epoch 7/8 | Step 100/500] - Loss: 1.3801\n",
      "[Epoch 7/8 | Step 110/500] - Loss: 1.4180\n",
      "[Epoch 7/8 | Step 120/500] - Loss: 1.4759\n",
      "[Epoch 7/8 | Step 130/500] - Loss: 1.4620\n",
      "[Epoch 7/8 | Step 140/500] - Loss: 1.4776\n",
      "[Epoch 7/8 | Step 150/500] - Loss: 1.5159\n",
      "[Epoch 7/8 | Step 160/500] - Loss: 1.5142\n",
      "[Epoch 7/8 | Step 170/500] - Loss: 1.5701\n",
      "[Epoch 7/8 | Step 180/500] - Loss: 1.5758\n",
      "[Epoch 7/8 | Step 190/500] - Loss: 1.5829\n",
      "[Epoch 7/8 | Step 200/500] - Loss: 1.5984\n",
      "[Epoch 7/8 | Step 210/500] - Loss: 1.5885\n",
      "[Epoch 7/8 | Step 220/500] - Loss: 1.5861\n",
      "[Epoch 7/8 | Step 230/500] - Loss: 1.5928\n",
      "[Epoch 7/8 | Step 240/500] - Loss: 1.5904\n",
      "[Epoch 7/8 | Step 250/500] - Loss: 1.5881\n",
      "[Epoch 7/8 | Step 260/500] - Loss: 1.5475\n",
      "[Epoch 7/8 | Step 270/500] - Loss: 1.5570\n",
      "[Epoch 7/8 | Step 280/500] - Loss: 1.5390\n",
      "[Epoch 7/8 | Step 290/500] - Loss: 1.5314\n",
      "[Epoch 7/8 | Step 300/500] - Loss: 1.5217\n",
      "[Epoch 7/8 | Step 310/500] - Loss: 1.5273\n",
      "[Epoch 7/8 | Step 320/500] - Loss: 1.5391\n",
      "[Epoch 7/8 | Step 330/500] - Loss: 1.5372\n",
      "[Epoch 7/8 | Step 340/500] - Loss: 1.5487\n",
      "[Epoch 7/8 | Step 350/500] - Loss: 1.5415\n",
      "[Epoch 7/8 | Step 360/500] - Loss: 1.5611\n",
      "[Epoch 7/8 | Step 370/500] - Loss: 1.5492\n",
      "[Epoch 7/8 | Step 380/500] - Loss: 1.5539\n",
      "[Epoch 7/8 | Step 390/500] - Loss: 1.5520\n",
      "[Epoch 7/8 | Step 400/500] - Loss: 1.5511\n",
      "[Epoch 7/8 | Step 410/500] - Loss: 1.5686\n",
      "[Epoch 7/8 | Step 420/500] - Loss: 1.5726\n",
      "[Epoch 7/8 | Step 430/500] - Loss: 1.5760\n",
      "[Epoch 7/8 | Step 440/500] - Loss: 1.5781\n",
      "[Epoch 7/8 | Step 450/500] - Loss: 1.5855\n",
      "[Epoch 7/8 | Step 460/500] - Loss: 1.6009\n",
      "[Epoch 7/8 | Step 470/500] - Loss: 1.6035\n",
      "[Epoch 7/8 | Step 480/500] - Loss: 1.5942\n",
      "[Epoch 7/8 | Step 490/500] - Loss: 1.5870\n",
      "Epoch 7/8 - Avg Train Loss: 1.5900, Val Loss: 1.6395\n",
      "[Epoch 8/8 | Step 0/500] - Loss: 0.4879\n",
      "[Epoch 8/8 | Step 10/500] - Loss: 1.4726\n",
      "[Epoch 8/8 | Step 20/500] - Loss: 1.4574\n",
      "[Epoch 8/8 | Step 30/500] - Loss: 1.4665\n",
      "[Epoch 8/8 | Step 40/500] - Loss: 1.4023\n",
      "[Epoch 8/8 | Step 50/500] - Loss: 1.4050\n",
      "[Epoch 8/8 | Step 60/500] - Loss: 1.4539\n",
      "[Epoch 8/8 | Step 70/500] - Loss: 1.3754\n",
      "[Epoch 8/8 | Step 80/500] - Loss: 1.5245\n",
      "[Epoch 8/8 | Step 90/500] - Loss: 1.4983\n",
      "[Epoch 8/8 | Step 100/500] - Loss: 1.4867\n",
      "[Epoch 8/8 | Step 110/500] - Loss: 1.4826\n",
      "[Epoch 8/8 | Step 120/500] - Loss: 1.5390\n",
      "[Epoch 8/8 | Step 130/500] - Loss: 1.5551\n",
      "[Epoch 8/8 | Step 140/500] - Loss: 1.5360\n",
      "[Epoch 8/8 | Step 150/500] - Loss: 1.5485\n",
      "[Epoch 8/8 | Step 160/500] - Loss: 1.5234\n",
      "[Epoch 8/8 | Step 170/500] - Loss: 1.5308\n",
      "[Epoch 8/8 | Step 180/500] - Loss: 1.5016\n",
      "[Epoch 8/8 | Step 190/500] - Loss: 1.4919\n",
      "[Epoch 8/8 | Step 200/500] - Loss: 1.4726\n",
      "[Epoch 8/8 | Step 210/500] - Loss: 1.4894\n",
      "[Epoch 8/8 | Step 220/500] - Loss: 1.4999\n",
      "[Epoch 8/8 | Step 230/500] - Loss: 1.4954\n",
      "[Epoch 8/8 | Step 240/500] - Loss: 1.4881\n",
      "[Epoch 8/8 | Step 250/500] - Loss: 1.4841\n",
      "[Epoch 8/8 | Step 260/500] - Loss: 1.4773\n",
      "[Epoch 8/8 | Step 270/500] - Loss: 1.4879\n",
      "[Epoch 8/8 | Step 280/500] - Loss: 1.5009\n",
      "[Epoch 8/8 | Step 290/500] - Loss: 1.4971\n",
      "[Epoch 8/8 | Step 300/500] - Loss: 1.5088\n",
      "[Epoch 8/8 | Step 310/500] - Loss: 1.5396\n",
      "[Epoch 8/8 | Step 320/500] - Loss: 1.5358\n",
      "[Epoch 8/8 | Step 330/500] - Loss: 1.5357\n",
      "[Epoch 8/8 | Step 340/500] - Loss: 1.5266\n",
      "[Epoch 8/8 | Step 350/500] - Loss: 1.5424\n",
      "[Epoch 8/8 | Step 360/500] - Loss: 1.5392\n",
      "[Epoch 8/8 | Step 370/500] - Loss: 1.5400\n",
      "[Epoch 8/8 | Step 380/500] - Loss: 1.5469\n",
      "[Epoch 8/8 | Step 390/500] - Loss: 1.5420\n",
      "[Epoch 8/8 | Step 400/500] - Loss: 1.5307\n",
      "[Epoch 8/8 | Step 410/500] - Loss: 1.5317\n",
      "[Epoch 8/8 | Step 420/500] - Loss: 1.5414\n",
      "[Epoch 8/8 | Step 430/500] - Loss: 1.5435\n",
      "[Epoch 8/8 | Step 440/500] - Loss: 1.5539\n",
      "[Epoch 8/8 | Step 450/500] - Loss: 1.5535\n",
      "[Epoch 8/8 | Step 460/500] - Loss: 1.5532\n",
      "[Epoch 8/8 | Step 470/500] - Loss: 1.5640\n",
      "[Epoch 8/8 | Step 480/500] - Loss: 1.5650\n",
      "[Epoch 8/8 | Step 490/500] - Loss: 1.5561\n",
      "Epoch 8/8 - Avg Train Loss: 1.5654, Val Loss: 1.6286\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import traceback\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Set up the GPU environment and return the appropriate device.\"\"\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "  \n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    return device\n",
    "\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model_config.use_cache = False\n",
    "\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        config=model_config, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"SEQ_CLS\")\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "class PreferenceEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Dataset to create pairs of message, preferred response, and rejected response for DPO training.\n",
    "        \"\"\"\n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pairs = self._create_preference_pairs()\n",
    "\n",
    "    def _create_preference_pairs(self):\n",
    "        \"\"\"\n",
    "        Create pairs using emails from the dataset based on their labels.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for _, selected_email in self.emails_df.iterrows():\n",
    "            selected_label = selected_email['label']\n",
    "            ham_emails = self.emails_df[self.emails_df['label'] == 0]\n",
    "            phish_emails = self.emails_df[self.emails_df['label'] == 1]\n",
    "\n",
    "            if selected_label == 1:  # Phishing email\n",
    "                # Preferred: Another phishing email\n",
    "                preferred_email = phish_emails[phish_emails.index != selected_email.name].sample(n=1).iloc[0]\n",
    "                # Rejected: A ham email\n",
    "                rejected_email = ham_emails.sample(n=1).iloc[0]\n",
    "                pairs.append({\n",
    "                    'message': selected_email,\n",
    "                    'preferred': preferred_email,\n",
    "                    'rejected': rejected_email\n",
    "                })\n",
    "            elif selected_label == 0:  # Ham email\n",
    "                # Preferred: Another ham email\n",
    "                preferred_email = ham_emails[ham_emails.index != selected_email.name].sample(n=1).iloc[0]\n",
    "                # Rejected: A phishing email\n",
    "                rejected_email = phish_emails.sample(n=1).iloc[0]\n",
    "                pairs.append({\n",
    "                    'message': selected_email,\n",
    "                    'preferred': preferred_email,\n",
    "                    'rejected': rejected_email\n",
    "                })\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def _prepare_email_input(self, message, response):\n",
    "        \"\"\"\n",
    "        Prepare the input text with formatted message and response for tokenization.\n",
    "        \"\"\"\n",
    "        formatted_input = f\"<s>[INST] {message} [/INST] {response}</s>\"\n",
    "        return self.tokenizer(\n",
    "            formatted_input,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        pair = self.pairs[idx]\n",
    "\n",
    "\n",
    "        message_text = f\"Sender: {pair['message']['sender']} [SEP] Subject: {pair['message']['subject']} [SEP] {pair['message']['body']}\"\n",
    "     \n",
    "        preferred_response = f\"Sender: {pair['preferred']['sender']} [SEP] Subject: {pair['preferred']['subject']} [SEP] {pair['preferred']['body']}\"\n",
    "        rejected_response = f\"Sender: {pair['rejected']['sender']} [SEP] Subject: {pair['rejected']['subject']} [SEP] {pair['rejected']['body']}\"\n",
    "        message_inputs = self._prepare_email_input(message_text, \"\")\n",
    "        preferred_inputs = self._prepare_email_input(message_text, preferred_response)\n",
    "        rejected_inputs = self._prepare_email_input(message_text, rejected_response)\n",
    "\n",
    "        return {\n",
    "            'message_input_ids': message_inputs['input_ids'].squeeze(),\n",
    "            'message_attention_mask': message_inputs['attention_mask'].squeeze(),\n",
    "            'preferred_input_ids': preferred_inputs['input_ids'].squeeze(),\n",
    "            'preferred_attention_mask': preferred_inputs['attention_mask'].squeeze(),\n",
    "            'rejected_input_ids': rejected_inputs['input_ids'].squeeze(),\n",
    "            'rejected_attention_mask': rejected_inputs['attention_mask'].squeeze(),\n",
    "        }\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "   \n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def compute_dpo_loss(policy_chosen_logits, policy_rejected_logits, \n",
    "                    reference_chosen_logits, reference_rejected_logits, \n",
    "                    beta=0.2):\n",
    "   \n",
    "    epsilon = 1e-8\n",
    "    \n",
    "   \n",
    "    policy_chosen_probs = F.softmax(policy_chosen_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    policy_rejected_probs = F.softmax(policy_rejected_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    ref_chosen_probs = F.softmax(reference_chosen_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    ref_rejected_probs = F.softmax(reference_rejected_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    \n",
    "  \n",
    "    chosen_rewards = (torch.log(policy_chosen_probs + epsilon) - \n",
    "                     torch.log(ref_chosen_probs + epsilon))\n",
    "    rejected_rewards = (torch.log(policy_rejected_probs + epsilon) - \n",
    "                       torch.log(ref_rejected_probs + epsilon))\n",
    "    \n",
    "    \n",
    "    max_reward = 50.0\n",
    "    chosen_rewards = torch.clamp(chosen_rewards, -max_reward, max_reward)\n",
    "    rejected_rewards = torch.clamp(rejected_rewards, -max_reward, max_reward)\n",
    "    \n",
    "    \n",
    "    logits_diff = (chosen_rewards - rejected_rewards) / beta\n",
    "    \n",
    "    valid_mask = ~torch.isnan(logits_diff)\n",
    "    if valid_mask.any():\n",
    "        loss = -F.logsigmoid(logits_diff[valid_mask]).mean()\n",
    "    else:\n",
    "        loss = torch.tensor(0.0, device=logits_diff.device)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_model_dpo(policy_model, reference_model, train_loader, val_loader, \n",
    "                   optimizer, scheduler, device, num_epochs=5, beta=0.2, gradient_accumulation_steps=2):\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    policy_model = policy_model.to(device).float()\n",
    "    reference_model = reference_model.to(device).float()\n",
    "    reference_model.eval()  # Ensure reference model does not get updated during training\n",
    "    \n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        policy_model.train()\n",
    "        total_loss = 0\n",
    "        valid_steps = 0\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            try:\n",
    "                with torch.amp.autocast('cuda', dtype=torch.float32):\n",
    "                    policy_chosen_outputs = policy_model(\n",
    "                        input_ids=batch['preferred_input_ids'],\n",
    "                        attention_mask=batch['preferred_attention_mask']\n",
    "                    )\n",
    "                    policy_rejected_outputs = policy_model(\n",
    "                        input_ids=batch['rejected_input_ids'],\n",
    "                        attention_mask=batch['rejected_attention_mask']\n",
    "                    )\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        ref_chosen_outputs = reference_model(\n",
    "                            input_ids=batch['preferred_input_ids'],\n",
    "                            attention_mask=batch['preferred_attention_mask']\n",
    "                        )\n",
    "                        ref_rejected_outputs = reference_model(\n",
    "                            input_ids=batch['rejected_input_ids'],\n",
    "                            attention_mask=batch['rejected_attention_mask']\n",
    "                        )\n",
    "                    \n",
    "                    loss = compute_dpo_loss(\n",
    "                        policy_chosen_outputs.logits,\n",
    "                        policy_rejected_outputs.logits,\n",
    "                        ref_chosen_outputs.logits,\n",
    "                        ref_rejected_outputs.logits,\n",
    "                        beta=beta\n",
    "                    )\n",
    "                    \n",
    "                    if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                        scaler.scale(loss).backward()\n",
    "                        \n",
    "                        # Gradient accumulation logic\n",
    "                        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                            scaler.unscale_(optimizer)\n",
    "                            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                            scheduler.step()\n",
    "                            optimizer.zero_grad()\n",
    "                        \n",
    "                        total_loss += loss.item()\n",
    "                        valid_steps += 1\n",
    "                    \n",
    "                    if step % 10 == 0:\n",
    "                        avg_loss = total_loss / max(valid_steps, 1)\n",
    "                        print(f\"[Epoch {epoch+1}/{num_epochs} | Step {step}/{len(train_loader)}] - Loss: {avg_loss:.4f}\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {step}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        if valid_steps > 0:\n",
    "            avg_train_loss = total_loss / valid_steps\n",
    "            val_loss = evaluate_model_dpo(policy_model, reference_model, val_loader, device, beta)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Avg Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = {k: v.cpu() for k, v in policy_model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
    "    \n",
    "    return best_model_state\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model_dpo(policy_model, reference_model, val_loader, device, beta):\n",
    "    policy_model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.amp.autocast('cuda', dtype=torch.float32):\n",
    "                policy_chosen_outputs = policy_model(\n",
    "                    input_ids=batch['preferred_input_ids'],\n",
    "                    attention_mask=batch['preferred_attention_mask']\n",
    "                )\n",
    "                policy_rejected_outputs = policy_model(\n",
    "                    input_ids=batch['rejected_input_ids'],\n",
    "                    attention_mask=batch['rejected_attention_mask']\n",
    "                )\n",
    "                \n",
    "                ref_chosen_outputs = reference_model(\n",
    "                    input_ids=batch['preferred_input_ids'],\n",
    "                    attention_mask=batch['preferred_attention_mask']\n",
    "                )\n",
    "                ref_rejected_outputs = reference_model(\n",
    "                    input_ids=batch['rejected_input_ids'],\n",
    "                    attention_mask=batch['rejected_attention_mask']\n",
    "                )\n",
    "                \n",
    "                loss = compute_dpo_loss(\n",
    "                    policy_chosen_outputs.logits,\n",
    "                    policy_rejected_outputs.logits,\n",
    "                    ref_chosen_outputs.logits,\n",
    "                    ref_rejected_outputs.logits,\n",
    "                    beta=beta\n",
    "                )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "   \n",
    "    device = setup_environment()\n",
    "    model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/final_data.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    policy_model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    reference_model, _ = setup_model_and_tokenizer(model_name, device)\n",
    "    \n",
    "\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "  \n",
    "    train_dataset = PreferenceEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = PreferenceEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # Setup optimization\n",
    "    optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "    num_epochs = 8\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    best_model_state = train_model_dpo(\n",
    "        policy_model,\n",
    "        reference_model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        num_epochs=num_epochs,\n",
    "        beta=0.2\n",
    "    )\n",
    "\n",
    "   \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/llama_7b_dpo_classification_model_chat\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    policy_model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"beta\": 0.2\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ae1c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n",
      "GPU Memory: 23.68 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d3fe3f38a745ce8d5094c217a28ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0: Loss = 0.4272\n",
      "Epoch 1, Step 10: Loss = 1.6470\n",
      "Epoch 1, Step 20: Loss = 0.7630\n",
      "Epoch 1, Step 30: Loss = 2.1678\n",
      "Epoch 1, Step 40: Loss = 1.5892\n",
      "Epoch 1, Step 50: Loss = 2.3011\n",
      "Epoch 1, Step 60: Loss = 0.5395\n",
      "Epoch 1, Step 70: Loss = 1.2031\n",
      "Epoch 1, Step 80: Loss = 1.8276\n",
      "Epoch 1, Step 90: Loss = 1.1809\n",
      "Epoch 1, Step 100: Loss = 1.4345\n",
      "Epoch 1, Step 110: Loss = 0.8915\n",
      "Epoch 1, Step 120: Loss = 1.1646\n",
      "Epoch 1, Step 130: Loss = 1.5051\n",
      "Epoch 1, Step 140: Loss = 1.9505\n",
      "Epoch 1, Step 150: Loss = 1.2951\n",
      "Epoch 1, Step 160: Loss = 2.5265\n",
      "Epoch 1, Step 170: Loss = 0.2750\n",
      "Epoch 1, Step 180: Loss = 0.3918\n",
      "Epoch 1, Step 190: Loss = 1.0108\n",
      "Epoch 1, Step 200: Loss = 0.4703\n",
      "Epoch 1, Step 210: Loss = 2.1217\n",
      "Epoch 1, Step 220: Loss = 1.1946\n",
      "Epoch 1, Step 230: Loss = 0.7917\n",
      "Epoch 1, Step 240: Loss = 1.4148\n",
      "Epoch 1, Step 250: Loss = 1.3443\n",
      "Epoch 1, Step 260: Loss = 1.8523\n",
      "Epoch 1, Step 270: Loss = 1.0013\n",
      "Epoch 1, Step 280: Loss = 0.5420\n",
      "Epoch 1, Step 290: Loss = 1.0984\n",
      "Epoch 1, Step 300: Loss = 1.0665\n",
      "Epoch 1, Step 310: Loss = 1.1811\n",
      "Epoch 1, Step 320: Loss = 1.1403\n",
      "Epoch 1, Step 330: Loss = 1.2913\n",
      "Epoch 1, Step 340: Loss = 0.6945\n",
      "Epoch 1, Step 350: Loss = 1.0976\n",
      "Epoch 1, Step 360: Loss = 0.9205\n",
      "Epoch 1, Step 370: Loss = 0.2872\n",
      "Epoch 1, Step 380: Loss = 1.0667\n",
      "Epoch 1, Step 390: Loss = 0.4122\n",
      "Epoch 1, Step 400: Loss = 1.1434\n",
      "Epoch 1, Step 410: Loss = 0.2722\n",
      "Epoch 1, Step 420: Loss = 0.8958\n",
      "Epoch 1, Step 430: Loss = 0.8415\n",
      "Epoch 1, Step 440: Loss = 0.1967\n",
      "Epoch 1, Step 450: Loss = 1.1153\n",
      "Epoch 1, Step 460: Loss = 0.7209\n",
      "Epoch 1, Step 470: Loss = 1.2120\n",
      "Epoch 1, Step 480: Loss = 0.2836\n",
      "Epoch 1, Step 490: Loss = 0.4516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.6092\n",
      "precision: 0.5673\n",
      "recall: 0.9210\n",
      "f1: 0.7021\n",
      "loss: 1.1232\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7880\n",
      "precision: 0.7491\n",
      "recall: 0.8660\n",
      "f1: 0.8033\n",
      "loss: 0.5239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 0: Loss = 0.7248\n",
      "Epoch 2, Step 10: Loss = 0.6782\n",
      "Epoch 2, Step 20: Loss = 0.0501\n",
      "Epoch 2, Step 30: Loss = 0.1776\n",
      "Epoch 2, Step 40: Loss = 0.8486\n",
      "Epoch 2, Step 50: Loss = 0.1535\n",
      "Epoch 2, Step 60: Loss = 0.9027\n",
      "Epoch 2, Step 70: Loss = 0.1491\n",
      "Epoch 2, Step 80: Loss = 0.5188\n",
      "Epoch 2, Step 90: Loss = 0.2357\n",
      "Epoch 2, Step 100: Loss = 1.1729\n",
      "Epoch 2, Step 110: Loss = 0.6698\n",
      "Epoch 2, Step 120: Loss = 0.1304\n",
      "Epoch 2, Step 130: Loss = 0.4738\n",
      "Epoch 2, Step 140: Loss = 0.7800\n",
      "Epoch 2, Step 150: Loss = 0.7193\n",
      "Epoch 2, Step 160: Loss = 0.3190\n",
      "Epoch 2, Step 170: Loss = 0.5726\n",
      "Epoch 2, Step 180: Loss = 0.4613\n",
      "Epoch 2, Step 190: Loss = 0.0764\n",
      "Epoch 2, Step 200: Loss = 0.1840\n",
      "Epoch 2, Step 210: Loss = 0.0874\n",
      "Epoch 2, Step 220: Loss = 0.2503\n",
      "Epoch 2, Step 230: Loss = 0.5001\n",
      "Epoch 2, Step 240: Loss = 0.2976\n",
      "Epoch 2, Step 250: Loss = 0.4904\n",
      "Epoch 2, Step 260: Loss = 0.5987\n",
      "Epoch 2, Step 270: Loss = 0.2482\n",
      "Epoch 2, Step 280: Loss = 0.2561\n",
      "Epoch 2, Step 290: Loss = 0.1637\n",
      "Epoch 2, Step 300: Loss = 0.4354\n",
      "Epoch 2, Step 310: Loss = 0.2283\n",
      "Epoch 2, Step 320: Loss = 0.7345\n",
      "Epoch 2, Step 330: Loss = 0.3446\n",
      "Epoch 2, Step 340: Loss = 1.7090\n",
      "Epoch 2, Step 350: Loss = 0.2430\n",
      "Epoch 2, Step 360: Loss = 0.1123\n",
      "Epoch 2, Step 370: Loss = 0.2931\n",
      "Epoch 2, Step 380: Loss = 0.4288\n",
      "Epoch 2, Step 390: Loss = 0.2753\n",
      "Epoch 2, Step 400: Loss = 0.4096\n",
      "Epoch 2, Step 410: Loss = 0.0971\n",
      "Epoch 2, Step 420: Loss = 0.0830\n",
      "Epoch 2, Step 430: Loss = 0.0385\n",
      "Epoch 2, Step 440: Loss = 0.0499\n",
      "Epoch 2, Step 450: Loss = 0.3502\n",
      "Epoch 2, Step 460: Loss = 0.5967\n",
      "Epoch 2, Step 470: Loss = 0.7631\n",
      "Epoch 2, Step 480: Loss = 0.7892\n",
      "Epoch 2, Step 490: Loss = 0.0599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8293\n",
      "precision: 0.7984\n",
      "recall: 0.8810\n",
      "f1: 0.8377\n",
      "loss: 0.4229\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8590\n",
      "precision: 0.8406\n",
      "recall: 0.8860\n",
      "f1: 0.8627\n",
      "loss: 0.3375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 0: Loss = 0.1100\n",
      "Epoch 3, Step 10: Loss = 0.0630\n",
      "Epoch 3, Step 20: Loss = 0.1822\n",
      "Epoch 3, Step 30: Loss = 0.2325\n",
      "Epoch 3, Step 40: Loss = 0.3266\n",
      "Epoch 3, Step 50: Loss = 0.3994\n",
      "Epoch 3, Step 60: Loss = 0.0473\n",
      "Epoch 3, Step 70: Loss = 0.0595\n",
      "Epoch 3, Step 80: Loss = 0.4212\n",
      "Epoch 3, Step 90: Loss = 0.0516\n",
      "Epoch 3, Step 100: Loss = 0.5670\n",
      "Epoch 3, Step 110: Loss = 0.7838\n",
      "Epoch 3, Step 120: Loss = 0.3184\n",
      "Epoch 3, Step 130: Loss = 0.3589\n",
      "Epoch 3, Step 140: Loss = 0.1838\n",
      "Epoch 3, Step 150: Loss = 0.0455\n",
      "Epoch 3, Step 160: Loss = 0.3019\n",
      "Epoch 3, Step 170: Loss = 0.1569\n",
      "Epoch 3, Step 180: Loss = 0.2494\n",
      "Epoch 3, Step 190: Loss = 0.4196\n",
      "Epoch 3, Step 200: Loss = 0.4416\n",
      "Epoch 3, Step 210: Loss = 0.0932\n",
      "Epoch 3, Step 220: Loss = 0.2014\n",
      "Epoch 3, Step 230: Loss = 0.2580\n",
      "Epoch 3, Step 240: Loss = 0.1099\n",
      "Epoch 3, Step 250: Loss = 0.3512\n",
      "Epoch 3, Step 260: Loss = 0.0346\n",
      "Epoch 3, Step 270: Loss = 0.6410\n",
      "Epoch 3, Step 280: Loss = 0.3313\n",
      "Epoch 3, Step 290: Loss = 1.2524\n",
      "Epoch 3, Step 300: Loss = 0.6130\n",
      "Epoch 3, Step 310: Loss = 0.3055\n",
      "Epoch 3, Step 320: Loss = 0.5128\n",
      "Epoch 3, Step 330: Loss = 0.3703\n",
      "Epoch 3, Step 340: Loss = 0.4210\n",
      "Epoch 3, Step 350: Loss = 0.1788\n",
      "Epoch 3, Step 360: Loss = 0.5428\n",
      "Epoch 3, Step 370: Loss = 0.0875\n",
      "Epoch 3, Step 380: Loss = 0.8668\n",
      "Epoch 3, Step 390: Loss = 0.2133\n",
      "Epoch 3, Step 400: Loss = 0.1178\n",
      "Epoch 3, Step 410: Loss = 0.1723\n",
      "Epoch 3, Step 420: Loss = 0.0570\n",
      "Epoch 3, Step 430: Loss = 0.0605\n",
      "Epoch 3, Step 440: Loss = 0.0661\n",
      "Epoch 3, Step 450: Loss = 0.0311\n",
      "Epoch 3, Step 460: Loss = 0.6938\n",
      "Epoch 3, Step 470: Loss = 1.2644\n",
      "Epoch 3, Step 480: Loss = 1.0991\n",
      "Epoch 3, Step 490: Loss = 0.2158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8755\n",
      "precision: 0.8573\n",
      "recall: 0.9010\n",
      "f1: 0.8786\n",
      "loss: 0.3140\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.8890\n",
      "precision: 0.8806\n",
      "recall: 0.9000\n",
      "f1: 0.8902\n",
      "loss: 0.2861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 0: Loss = 1.0407\n",
      "Epoch 4, Step 10: Loss = 0.3867\n",
      "Epoch 4, Step 20: Loss = 0.2628\n",
      "Epoch 4, Step 30: Loss = 0.1276\n",
      "Epoch 4, Step 40: Loss = 0.0725\n",
      "Epoch 4, Step 50: Loss = 0.3154\n",
      "Epoch 4, Step 60: Loss = 0.2460\n",
      "Epoch 4, Step 70: Loss = 0.2625\n",
      "Epoch 4, Step 80: Loss = 0.1105\n",
      "Epoch 4, Step 90: Loss = 0.5443\n",
      "Epoch 4, Step 100: Loss = 0.3665\n",
      "Epoch 4, Step 110: Loss = 0.2582\n",
      "Epoch 4, Step 120: Loss = 0.2831\n",
      "Epoch 4, Step 130: Loss = 0.3791\n",
      "Epoch 4, Step 140: Loss = 0.2010\n",
      "Epoch 4, Step 150: Loss = 0.1029\n",
      "Epoch 4, Step 160: Loss = 0.2065\n",
      "Epoch 4, Step 170: Loss = 0.4754\n",
      "Epoch 4, Step 180: Loss = 0.0299\n",
      "Epoch 4, Step 190: Loss = 0.3958\n",
      "Epoch 4, Step 200: Loss = 0.1270\n",
      "Epoch 4, Step 210: Loss = 0.1072\n",
      "Epoch 4, Step 220: Loss = 0.1097\n",
      "Epoch 4, Step 230: Loss = 0.5458\n",
      "Epoch 4, Step 240: Loss = 0.1787\n",
      "Epoch 4, Step 250: Loss = 0.4305\n",
      "Epoch 4, Step 260: Loss = 0.2355\n",
      "Epoch 4, Step 270: Loss = 0.2236\n",
      "Epoch 4, Step 280: Loss = 0.1132\n",
      "Epoch 4, Step 290: Loss = 0.2852\n",
      "Epoch 4, Step 300: Loss = 0.2337\n",
      "Epoch 4, Step 310: Loss = 0.0752\n",
      "Epoch 4, Step 320: Loss = 0.2668\n",
      "Epoch 4, Step 330: Loss = 0.1046\n",
      "Epoch 4, Step 340: Loss = 0.1384\n",
      "Epoch 4, Step 350: Loss = 0.1242\n",
      "Epoch 4, Step 360: Loss = 0.0298\n",
      "Epoch 4, Step 370: Loss = 0.0446\n",
      "Epoch 4, Step 380: Loss = 0.1396\n",
      "Epoch 4, Step 390: Loss = 0.1385\n",
      "Epoch 4, Step 400: Loss = 0.3449\n",
      "Epoch 4, Step 410: Loss = 0.0252\n",
      "Epoch 4, Step 420: Loss = 0.8593\n",
      "Epoch 4, Step 430: Loss = 0.5694\n",
      "Epoch 4, Step 440: Loss = 0.1438\n",
      "Epoch 4, Step 450: Loss = 0.1585\n",
      "Epoch 4, Step 460: Loss = 0.1662\n",
      "Epoch 4, Step 470: Loss = 0.0763\n",
      "Epoch 4, Step 480: Loss = 0.1080\n",
      "Epoch 4, Step 490: Loss = 0.1165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8910\n",
      "precision: 0.8749\n",
      "recall: 0.9125\n",
      "f1: 0.8933\n",
      "loss: 0.2749\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9040\n",
      "precision: 0.8945\n",
      "recall: 0.9160\n",
      "f1: 0.9051\n",
      "loss: 0.2648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Step 0: Loss = 0.0657\n",
      "Epoch 5, Step 10: Loss = 0.2040\n",
      "Epoch 5, Step 20: Loss = 0.2207\n",
      "Epoch 5, Step 30: Loss = 0.0299\n",
      "Epoch 5, Step 40: Loss = 0.2215\n",
      "Epoch 5, Step 50: Loss = 0.1889\n",
      "Epoch 5, Step 60: Loss = 0.0978\n",
      "Epoch 5, Step 70: Loss = 0.0882\n",
      "Epoch 5, Step 80: Loss = 0.1888\n",
      "Epoch 5, Step 90: Loss = 0.9026\n",
      "Epoch 5, Step 100: Loss = 0.8007\n",
      "Epoch 5, Step 110: Loss = 0.2342\n",
      "Epoch 5, Step 120: Loss = 0.1604\n",
      "Epoch 5, Step 130: Loss = 0.1269\n",
      "Epoch 5, Step 140: Loss = 0.0861\n",
      "Epoch 5, Step 150: Loss = 0.6603\n",
      "Epoch 5, Step 160: Loss = 0.0764\n",
      "Epoch 5, Step 170: Loss = 0.2565\n",
      "Epoch 5, Step 180: Loss = 0.4057\n",
      "Epoch 5, Step 190: Loss = 0.3256\n",
      "Epoch 5, Step 200: Loss = 0.0426\n",
      "Epoch 5, Step 210: Loss = 0.7952\n",
      "Epoch 5, Step 220: Loss = 0.0568\n",
      "Epoch 5, Step 230: Loss = 0.2645\n",
      "Epoch 5, Step 240: Loss = 0.4267\n",
      "Epoch 5, Step 250: Loss = 0.0793\n",
      "Epoch 5, Step 260: Loss = 0.3919\n",
      "Epoch 5, Step 270: Loss = 0.1660\n",
      "Epoch 5, Step 280: Loss = 0.2850\n",
      "Epoch 5, Step 290: Loss = 0.1087\n",
      "Epoch 5, Step 300: Loss = 0.2368\n",
      "Epoch 5, Step 310: Loss = 0.4072\n",
      "Epoch 5, Step 320: Loss = 0.0659\n",
      "Epoch 5, Step 330: Loss = 0.0111\n",
      "Epoch 5, Step 340: Loss = 0.5371\n",
      "Epoch 5, Step 350: Loss = 0.2357\n",
      "Epoch 5, Step 360: Loss = 0.2233\n",
      "Epoch 5, Step 370: Loss = 0.3997\n",
      "Epoch 5, Step 380: Loss = 0.4804\n",
      "Epoch 5, Step 390: Loss = 0.3024\n",
      "Epoch 5, Step 400: Loss = 0.0558\n",
      "Epoch 5, Step 410: Loss = 0.4615\n",
      "Epoch 5, Step 420: Loss = 0.7265\n",
      "Epoch 5, Step 430: Loss = 0.1225\n",
      "Epoch 5, Step 440: Loss = 0.0901\n",
      "Epoch 5, Step 450: Loss = 0.0721\n",
      "Epoch 5, Step 460: Loss = 0.0874\n",
      "Epoch 5, Step 470: Loss = 0.1279\n",
      "Epoch 5, Step 480: Loss = 0.2063\n",
      "Epoch 5, Step 490: Loss = 0.1052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.8970\n",
      "precision: 0.8825\n",
      "recall: 0.9160\n",
      "f1: 0.8989\n",
      "loss: 0.2616\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9060\n",
      "precision: 0.8965\n",
      "recall: 0.9180\n",
      "f1: 0.9071\n",
      "loss: 0.2568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Step 0: Loss = 0.2733\n",
      "Epoch 6, Step 10: Loss = 0.1311\n",
      "Epoch 6, Step 20: Loss = 0.0605\n",
      "Epoch 6, Step 30: Loss = 0.4768\n",
      "Epoch 6, Step 40: Loss = 0.5181\n",
      "Epoch 6, Step 50: Loss = 0.3234\n",
      "Epoch 6, Step 60: Loss = 0.2090\n",
      "Epoch 6, Step 70: Loss = 0.4782\n",
      "Epoch 6, Step 80: Loss = 0.2583\n",
      "Epoch 6, Step 90: Loss = 0.0413\n",
      "Epoch 6, Step 100: Loss = 0.4889\n",
      "Epoch 6, Step 110: Loss = 0.0198\n",
      "Epoch 6, Step 120: Loss = 0.1177\n",
      "Epoch 6, Step 130: Loss = 0.1281\n",
      "Epoch 6, Step 140: Loss = 0.3601\n",
      "Epoch 6, Step 150: Loss = 0.3155\n",
      "Epoch 6, Step 160: Loss = 0.5489\n",
      "Epoch 6, Step 170: Loss = 0.0185\n",
      "Epoch 6, Step 180: Loss = 0.8911\n",
      "Epoch 6, Step 190: Loss = 0.3537\n",
      "Epoch 6, Step 200: Loss = 0.0821\n",
      "Epoch 6, Step 210: Loss = 0.9456\n",
      "Epoch 6, Step 220: Loss = 0.2114\n",
      "Epoch 6, Step 230: Loss = 0.0300\n",
      "Epoch 6, Step 240: Loss = 0.0390\n",
      "Epoch 6, Step 250: Loss = 0.0582\n",
      "Epoch 6, Step 260: Loss = 0.1660\n",
      "Epoch 6, Step 270: Loss = 0.4701\n",
      "Epoch 6, Step 280: Loss = 0.6595\n",
      "Epoch 6, Step 290: Loss = 0.7898\n",
      "Epoch 6, Step 300: Loss = 0.6727\n",
      "Epoch 6, Step 310: Loss = 1.1386\n",
      "Epoch 6, Step 320: Loss = 0.1346\n",
      "Epoch 6, Step 330: Loss = 0.5214\n",
      "Epoch 6, Step 340: Loss = 0.7004\n",
      "Epoch 6, Step 350: Loss = 0.6217\n",
      "Epoch 6, Step 360: Loss = 0.3306\n",
      "Epoch 6, Step 370: Loss = 0.1235\n",
      "Epoch 6, Step 380: Loss = 0.0867\n",
      "Epoch 6, Step 390: Loss = 0.5186\n",
      "Epoch 6, Step 400: Loss = 0.2017\n",
      "Epoch 6, Step 410: Loss = 0.6299\n",
      "Epoch 6, Step 420: Loss = 0.0589\n",
      "Epoch 6, Step 430: Loss = 0.1869\n",
      "Epoch 6, Step 440: Loss = 0.0837\n",
      "Epoch 6, Step 450: Loss = 0.2814\n",
      "Epoch 6, Step 460: Loss = 0.5540\n",
      "Epoch 6, Step 470: Loss = 0.6028\n",
      "Epoch 6, Step 480: Loss = 0.6000\n",
      "Epoch 6, Step 490: Loss = 0.0946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/users/skuikel/anaconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Training Metrics:\n",
      "accuracy: 0.9018\n",
      "precision: 0.8846\n",
      "recall: 0.9240\n",
      "f1: 0.9039\n",
      "loss: 0.2545\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.9070\n",
      "precision: 0.8982\n",
      "recall: 0.9180\n",
      "f1: 0.9080\n",
      "loss: 0.2532\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig, BitsAndBytesConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        self.emails_df = emails_df.copy()\n",
    "        self.emails_df['sender'] = self.emails_df['sender'].apply(clean_text)\n",
    "        self.emails_df['subject'] = self.emails_df['subject'].apply(clean_text)\n",
    "        self.emails_df['body'] = self.emails_df['body'].apply(clean_text)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emails_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        email = self.emails_df.iloc[idx]\n",
    "        input_text = f\"Sender: {email['sender']} [SEP] Subject: {email['subject']} [SEP] {email['body']}\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(email['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model_config.use_cache = False\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, output_dir):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "    \n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    return metrics\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=6):\n",
    "    best_val_metrics = {'f1': 0}\n",
    "    best_model_state = None\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['label']\n",
    "                )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % 2 == 0:  # Gradient accumulation steps = 2\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(batch['label'].cpu().numpy())\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\")\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "  \n",
    "        train_metrics = compute_metrics(train_preds, train_labels)\n",
    "        train_metrics['loss'] = total_loss / len(train_loader)\n",
    "      \n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "      \n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(\"Training Metrics:\")\n",
    "        for metric, value in train_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        for metric, value in val_metrics.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_val_metrics['f1']:\n",
    "            best_val_metrics = val_metrics\n",
    "            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return best_model_state, best_val_metrics\n",
    "\n",
    "def main():\n",
    "    \n",
    "    device = setup_environment()\n",
    "    model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/final_data.csv\")\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "    \n",
    "    model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    \n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "    \n",
    "    train_dataset = EmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = EmailDataset(val_df, tokenizer, max_length=512)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 6\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    best_model_state, best_metrics = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/lama7b_binary_classification_model_chat\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    plot_confusion_matrix(best_metrics['confusion_matrix'], output_dir)\n",
    "    \n",
    "  \n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"best_metrics\": {k: float(v) if k != 'confusion_matrix' else v.tolist() \n",
    "                        for k, v in best_metrics.items()}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e5b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
