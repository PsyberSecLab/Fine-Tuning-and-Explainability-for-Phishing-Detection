{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac34cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d8990d2bc840c68ad8f357e655c5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758b387e656e4cf689102e041720788d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/.local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/8 | Step 0/500] - Loss: 6.0408\n",
      "[Epoch 1/8 | Step 10/500] - Loss: 3.5454\n",
      "[Epoch 1/8 | Step 20/500] - Loss: 3.1262\n",
      "[Epoch 1/8 | Step 30/500] - Loss: 3.4185\n",
      "[Epoch 1/8 | Step 40/500] - Loss: 3.4728\n",
      "[Epoch 1/8 | Step 50/500] - Loss: 3.5754\n",
      "[Epoch 1/8 | Step 60/500] - Loss: 3.7399\n",
      "[Epoch 1/8 | Step 70/500] - Loss: 3.7017\n",
      "[Epoch 1/8 | Step 80/500] - Loss: 3.6941\n",
      "[Epoch 1/8 | Step 90/500] - Loss: 3.6940\n",
      "[Epoch 1/8 | Step 100/500] - Loss: 3.7085\n",
      "[Epoch 1/8 | Step 110/500] - Loss: 3.6996\n",
      "[Epoch 1/8 | Step 120/500] - Loss: 3.6417\n",
      "[Epoch 1/8 | Step 130/500] - Loss: 3.5847\n",
      "[Epoch 1/8 | Step 140/500] - Loss: 3.6472\n",
      "[Epoch 1/8 | Step 150/500] - Loss: 3.6390\n",
      "[Epoch 1/8 | Step 160/500] - Loss: 3.6050\n",
      "[Epoch 1/8 | Step 170/500] - Loss: 3.5473\n",
      "[Epoch 1/8 | Step 180/500] - Loss: 3.4963\n",
      "[Epoch 1/8 | Step 190/500] - Loss: 3.4660\n",
      "[Epoch 1/8 | Step 200/500] - Loss: 3.4016\n",
      "[Epoch 1/8 | Step 210/500] - Loss: 3.4156\n",
      "[Epoch 1/8 | Step 220/500] - Loss: 3.4077\n",
      "[Epoch 1/8 | Step 230/500] - Loss: 3.3576\n",
      "[Epoch 1/8 | Step 240/500] - Loss: 3.3538\n",
      "[Epoch 1/8 | Step 250/500] - Loss: 3.3199\n",
      "[Epoch 1/8 | Step 260/500] - Loss: 3.2896\n",
      "[Epoch 1/8 | Step 270/500] - Loss: 3.2562\n",
      "[Epoch 1/8 | Step 280/500] - Loss: 3.2525\n",
      "[Epoch 1/8 | Step 290/500] - Loss: 3.2384\n",
      "[Epoch 1/8 | Step 300/500] - Loss: 3.2162\n",
      "[Epoch 1/8 | Step 310/500] - Loss: 3.2287\n",
      "[Epoch 1/8 | Step 320/500] - Loss: 3.2152\n",
      "[Epoch 1/8 | Step 330/500] - Loss: 3.1987\n",
      "[Epoch 1/8 | Step 340/500] - Loss: 3.1933\n",
      "[Epoch 1/8 | Step 350/500] - Loss: 3.1871\n",
      "[Epoch 1/8 | Step 360/500] - Loss: 3.1857\n",
      "[Epoch 1/8 | Step 370/500] - Loss: 3.1883\n",
      "[Epoch 1/8 | Step 380/500] - Loss: 3.1632\n",
      "[Epoch 1/8 | Step 390/500] - Loss: 3.1578\n",
      "[Epoch 1/8 | Step 400/500] - Loss: 3.1226\n",
      "[Epoch 1/8 | Step 410/500] - Loss: 3.1003\n",
      "[Epoch 1/8 | Step 420/500] - Loss: 3.1002\n",
      "[Epoch 1/8 | Step 430/500] - Loss: 3.0960\n",
      "[Epoch 1/8 | Step 440/500] - Loss: 3.1033\n",
      "[Epoch 1/8 | Step 450/500] - Loss: 3.0735\n",
      "[Epoch 1/8 | Step 460/500] - Loss: 3.0478\n",
      "[Epoch 1/8 | Step 470/500] - Loss: 3.0144\n",
      "[Epoch 1/8 | Step 480/500] - Loss: 2.9979\n",
      "[Epoch 1/8 | Step 490/500] - Loss: 2.9891\n",
      "Epoch 1/8 - Avg Train Loss: 2.9672, Val Loss: 2.4492\n",
      "[Epoch 2/8 | Step 0/500] - Loss: 3.1815\n",
      "[Epoch 2/8 | Step 10/500] - Loss: 2.1931\n",
      "[Epoch 2/8 | Step 20/500] - Loss: 2.5745\n",
      "[Epoch 2/8 | Step 30/500] - Loss: 2.5951\n",
      "[Epoch 2/8 | Step 40/500] - Loss: 2.5506\n",
      "[Epoch 2/8 | Step 50/500] - Loss: 2.4601\n",
      "[Epoch 2/8 | Step 60/500] - Loss: 2.3812\n",
      "[Epoch 2/8 | Step 70/500] - Loss: 2.4457\n",
      "[Epoch 2/8 | Step 80/500] - Loss: 2.3674\n",
      "[Epoch 2/8 | Step 90/500] - Loss: 2.3396\n",
      "[Epoch 2/8 | Step 100/500] - Loss: 2.3011\n",
      "[Epoch 2/8 | Step 110/500] - Loss: 2.3306\n",
      "[Epoch 2/8 | Step 120/500] - Loss: 2.4017\n",
      "[Epoch 2/8 | Step 130/500] - Loss: 2.3968\n",
      "[Epoch 2/8 | Step 140/500] - Loss: 2.3818\n",
      "[Epoch 2/8 | Step 150/500] - Loss: 2.3556\n",
      "[Epoch 2/8 | Step 160/500] - Loss: 2.3300\n",
      "[Epoch 2/8 | Step 170/500] - Loss: 2.3247\n",
      "[Epoch 2/8 | Step 180/500] - Loss: 2.3289\n",
      "[Epoch 2/8 | Step 190/500] - Loss: 2.3188\n",
      "[Epoch 2/8 | Step 200/500] - Loss: 2.3065\n",
      "[Epoch 2/8 | Step 210/500] - Loss: 2.3115\n",
      "[Epoch 2/8 | Step 220/500] - Loss: 2.3230\n",
      "[Epoch 2/8 | Step 230/500] - Loss: 2.3367\n",
      "[Epoch 2/8 | Step 240/500] - Loss: 2.3169\n",
      "[Epoch 2/8 | Step 250/500] - Loss: 2.2885\n",
      "[Epoch 2/8 | Step 260/500] - Loss: 2.2863\n",
      "[Epoch 2/8 | Step 270/500] - Loss: 2.2866\n",
      "[Epoch 2/8 | Step 280/500] - Loss: 2.2817\n",
      "[Epoch 2/8 | Step 290/500] - Loss: 2.2616\n",
      "[Epoch 2/8 | Step 300/500] - Loss: 2.2754\n",
      "[Epoch 2/8 | Step 310/500] - Loss: 2.2583\n",
      "[Epoch 2/8 | Step 320/500] - Loss: 2.2402\n",
      "[Epoch 2/8 | Step 330/500] - Loss: 2.2568\n",
      "[Epoch 2/8 | Step 340/500] - Loss: 2.2491\n",
      "[Epoch 2/8 | Step 350/500] - Loss: 2.2234\n",
      "[Epoch 2/8 | Step 360/500] - Loss: 2.2206\n",
      "[Epoch 2/8 | Step 370/500] - Loss: 2.2177\n",
      "[Epoch 2/8 | Step 380/500] - Loss: 2.2184\n",
      "[Epoch 2/8 | Step 390/500] - Loss: 2.2161\n",
      "[Epoch 2/8 | Step 400/500] - Loss: 2.2156\n",
      "[Epoch 2/8 | Step 410/500] - Loss: 2.2087\n",
      "[Epoch 2/8 | Step 420/500] - Loss: 2.2192\n",
      "[Epoch 2/8 | Step 430/500] - Loss: 2.2335\n",
      "[Epoch 2/8 | Step 440/500] - Loss: 2.2260\n",
      "[Epoch 2/8 | Step 450/500] - Loss: 2.2048\n",
      "[Epoch 2/8 | Step 460/500] - Loss: 2.2100\n",
      "[Epoch 2/8 | Step 470/500] - Loss: 2.2094\n",
      "[Epoch 2/8 | Step 480/500] - Loss: 2.2078\n",
      "[Epoch 2/8 | Step 490/500] - Loss: 2.2147\n",
      "Epoch 2/8 - Avg Train Loss: 2.2186, Val Loss: 2.2257\n",
      "[Epoch 3/8 | Step 0/500] - Loss: 1.9888\n",
      "[Epoch 3/8 | Step 10/500] - Loss: 1.7840\n",
      "[Epoch 3/8 | Step 20/500] - Loss: 2.1552\n",
      "[Epoch 3/8 | Step 30/500] - Loss: 2.3731\n",
      "[Epoch 3/8 | Step 40/500] - Loss: 2.4683\n",
      "[Epoch 3/8 | Step 50/500] - Loss: 2.4432\n",
      "[Epoch 3/8 | Step 60/500] - Loss: 2.3661\n",
      "[Epoch 3/8 | Step 70/500] - Loss: 2.3583\n",
      "[Epoch 3/8 | Step 80/500] - Loss: 2.2517\n",
      "[Epoch 3/8 | Step 90/500] - Loss: 2.3784\n",
      "[Epoch 3/8 | Step 100/500] - Loss: 2.3685\n",
      "[Epoch 3/8 | Step 110/500] - Loss: 2.3358\n",
      "[Epoch 3/8 | Step 120/500] - Loss: 2.3602\n",
      "[Epoch 3/8 | Step 130/500] - Loss: 2.3449\n",
      "[Epoch 3/8 | Step 140/500] - Loss: 2.3369\n",
      "[Epoch 3/8 | Step 150/500] - Loss: 2.3034\n",
      "[Epoch 3/8 | Step 160/500] - Loss: 2.2600\n",
      "[Epoch 3/8 | Step 170/500] - Loss: 2.2438\n",
      "[Epoch 3/8 | Step 180/500] - Loss: 2.2289\n",
      "[Epoch 3/8 | Step 190/500] - Loss: 2.2110\n",
      "[Epoch 3/8 | Step 200/500] - Loss: 2.2041\n",
      "[Epoch 3/8 | Step 210/500] - Loss: 2.2027\n",
      "[Epoch 3/8 | Step 220/500] - Loss: 2.2093\n",
      "[Epoch 3/8 | Step 230/500] - Loss: 2.2488\n",
      "[Epoch 3/8 | Step 240/500] - Loss: 2.2562\n",
      "[Epoch 3/8 | Step 250/500] - Loss: 2.2216\n",
      "[Epoch 3/8 | Step 260/500] - Loss: 2.2123\n",
      "[Epoch 3/8 | Step 270/500] - Loss: 2.2087\n",
      "[Epoch 3/8 | Step 280/500] - Loss: 2.2125\n",
      "[Epoch 3/8 | Step 290/500] - Loss: 2.2149\n",
      "[Epoch 3/8 | Step 300/500] - Loss: 2.2156\n",
      "[Epoch 3/8 | Step 310/500] - Loss: 2.2085\n",
      "[Epoch 3/8 | Step 320/500] - Loss: 2.2170\n",
      "[Epoch 3/8 | Step 330/500] - Loss: 2.2245\n",
      "[Epoch 3/8 | Step 340/500] - Loss: 2.2338\n",
      "[Epoch 3/8 | Step 350/500] - Loss: 2.2254\n",
      "[Epoch 3/8 | Step 360/500] - Loss: 2.2163\n",
      "[Epoch 3/8 | Step 370/500] - Loss: 2.2129\n",
      "[Epoch 3/8 | Step 380/500] - Loss: 2.1965\n",
      "[Epoch 3/8 | Step 390/500] - Loss: 2.1963\n",
      "[Epoch 3/8 | Step 400/500] - Loss: 2.1906\n",
      "[Epoch 3/8 | Step 410/500] - Loss: 2.1811\n",
      "[Epoch 3/8 | Step 420/500] - Loss: 2.1817\n",
      "[Epoch 3/8 | Step 430/500] - Loss: 2.1893\n",
      "[Epoch 3/8 | Step 440/500] - Loss: 2.1970\n",
      "[Epoch 3/8 | Step 450/500] - Loss: 2.1858\n",
      "[Epoch 3/8 | Step 460/500] - Loss: 2.1806\n",
      "[Epoch 3/8 | Step 470/500] - Loss: 2.1868\n",
      "[Epoch 3/8 | Step 480/500] - Loss: 2.1841\n",
      "[Epoch 3/8 | Step 490/500] - Loss: 2.1746\n",
      "Epoch 3/8 - Avg Train Loss: 2.1748, Val Loss: 2.2312\n",
      "[Epoch 4/8 | Step 0/500] - Loss: 2.6514\n",
      "[Epoch 4/8 | Step 10/500] - Loss: 2.1464\n",
      "[Epoch 4/8 | Step 20/500] - Loss: 1.6770\n",
      "[Epoch 4/8 | Step 30/500] - Loss: 1.6682\n",
      "[Epoch 4/8 | Step 40/500] - Loss: 1.7997\n",
      "[Epoch 4/8 | Step 50/500] - Loss: 1.8561\n",
      "[Epoch 4/8 | Step 60/500] - Loss: 1.9262\n",
      "[Epoch 4/8 | Step 70/500] - Loss: 2.0315\n",
      "[Epoch 4/8 | Step 80/500] - Loss: 1.9985\n",
      "[Epoch 4/8 | Step 90/500] - Loss: 2.0487\n",
      "[Epoch 4/8 | Step 100/500] - Loss: 2.0915\n",
      "[Epoch 4/8 | Step 110/500] - Loss: 2.1374\n",
      "[Epoch 4/8 | Step 120/500] - Loss: 2.1390\n",
      "[Epoch 4/8 | Step 130/500] - Loss: 2.1673\n",
      "[Epoch 4/8 | Step 140/500] - Loss: 2.2197\n",
      "[Epoch 4/8 | Step 150/500] - Loss: 2.2373\n",
      "[Epoch 4/8 | Step 160/500] - Loss: 2.2303\n",
      "[Epoch 4/8 | Step 170/500] - Loss: 2.2311\n",
      "[Epoch 4/8 | Step 180/500] - Loss: 2.1991\n",
      "[Epoch 4/8 | Step 190/500] - Loss: 2.1980\n",
      "[Epoch 4/8 | Step 200/500] - Loss: 2.2303\n",
      "[Epoch 4/8 | Step 210/500] - Loss: 2.2289\n",
      "[Epoch 4/8 | Step 220/500] - Loss: 2.2007\n",
      "[Epoch 4/8 | Step 230/500] - Loss: 2.2073\n",
      "[Epoch 4/8 | Step 240/500] - Loss: 2.2064\n",
      "[Epoch 4/8 | Step 250/500] - Loss: 2.1893\n",
      "[Epoch 4/8 | Step 260/500] - Loss: 2.1837\n",
      "[Epoch 4/8 | Step 270/500] - Loss: 2.1805\n",
      "[Epoch 4/8 | Step 280/500] - Loss: 2.1739\n",
      "[Epoch 4/8 | Step 290/500] - Loss: 2.1568\n",
      "[Epoch 4/8 | Step 300/500] - Loss: 2.1510\n",
      "[Epoch 4/8 | Step 310/500] - Loss: 2.1406\n",
      "[Epoch 4/8 | Step 320/500] - Loss: 2.1484\n",
      "[Epoch 4/8 | Step 330/500] - Loss: 2.1548\n",
      "[Epoch 4/8 | Step 340/500] - Loss: 2.1545\n",
      "[Epoch 4/8 | Step 350/500] - Loss: 2.1688\n",
      "[Epoch 4/8 | Step 360/500] - Loss: 2.1551\n",
      "[Epoch 4/8 | Step 370/500] - Loss: 2.1620\n",
      "[Epoch 4/8 | Step 380/500] - Loss: 2.1950\n",
      "[Epoch 4/8 | Step 390/500] - Loss: 2.1985\n",
      "[Epoch 4/8 | Step 400/500] - Loss: 2.2030\n",
      "[Epoch 4/8 | Step 410/500] - Loss: 2.1995\n",
      "[Epoch 4/8 | Step 420/500] - Loss: 2.1892\n",
      "[Epoch 4/8 | Step 430/500] - Loss: 2.1876\n",
      "[Epoch 4/8 | Step 440/500] - Loss: 2.1995\n",
      "[Epoch 4/8 | Step 450/500] - Loss: 2.2018\n",
      "[Epoch 4/8 | Step 460/500] - Loss: 2.2051\n",
      "[Epoch 4/8 | Step 470/500] - Loss: 2.2001\n",
      "[Epoch 4/8 | Step 480/500] - Loss: 2.1959\n",
      "[Epoch 4/8 | Step 490/500] - Loss: 2.1992\n",
      "Epoch 4/8 - Avg Train Loss: 2.1922, Val Loss: 2.2283\n",
      "[Epoch 5/8 | Step 0/500] - Loss: 1.9495\n",
      "[Epoch 5/8 | Step 10/500] - Loss: 2.9258\n",
      "[Epoch 5/8 | Step 20/500] - Loss: 2.6319\n",
      "[Epoch 5/8 | Step 30/500] - Loss: 2.5194\n",
      "[Epoch 5/8 | Step 40/500] - Loss: 2.4350\n",
      "[Epoch 5/8 | Step 50/500] - Loss: 2.3514\n",
      "[Epoch 5/8 | Step 60/500] - Loss: 2.2350\n",
      "[Epoch 5/8 | Step 70/500] - Loss: 2.2414\n",
      "[Epoch 5/8 | Step 80/500] - Loss: 2.2904\n",
      "[Epoch 5/8 | Step 90/500] - Loss: 2.2452\n",
      "[Epoch 5/8 | Step 100/500] - Loss: 2.2318\n",
      "[Epoch 5/8 | Step 110/500] - Loss: 2.1992\n",
      "[Epoch 5/8 | Step 120/500] - Loss: 2.1893\n",
      "[Epoch 5/8 | Step 130/500] - Loss: 2.2092\n",
      "[Epoch 5/8 | Step 140/500] - Loss: 2.2689\n",
      "[Epoch 5/8 | Step 150/500] - Loss: 2.2903\n",
      "[Epoch 5/8 | Step 160/500] - Loss: 2.2940\n",
      "[Epoch 5/8 | Step 170/500] - Loss: 2.2713\n",
      "[Epoch 5/8 | Step 180/500] - Loss: 2.2525\n",
      "[Epoch 5/8 | Step 190/500] - Loss: 2.2449\n",
      "[Epoch 5/8 | Step 200/500] - Loss: 2.2513\n",
      "[Epoch 5/8 | Step 210/500] - Loss: 2.2409\n",
      "[Epoch 5/8 | Step 220/500] - Loss: 2.2197\n",
      "[Epoch 5/8 | Step 230/500] - Loss: 2.2292\n",
      "[Epoch 5/8 | Step 240/500] - Loss: 2.2197\n",
      "[Epoch 5/8 | Step 250/500] - Loss: 2.2218\n",
      "[Epoch 5/8 | Step 260/500] - Loss: 2.2492\n",
      "[Epoch 5/8 | Step 270/500] - Loss: 2.2587\n",
      "[Epoch 5/8 | Step 280/500] - Loss: 2.2414\n",
      "[Epoch 5/8 | Step 290/500] - Loss: 2.2236\n",
      "[Epoch 5/8 | Step 300/500] - Loss: 2.2074\n",
      "[Epoch 5/8 | Step 310/500] - Loss: 2.1936\n",
      "[Epoch 5/8 | Step 320/500] - Loss: 2.1817\n",
      "[Epoch 5/8 | Step 330/500] - Loss: 2.1668\n",
      "[Epoch 5/8 | Step 340/500] - Loss: 2.1720\n",
      "[Epoch 5/8 | Step 350/500] - Loss: 2.1802\n",
      "[Epoch 5/8 | Step 360/500] - Loss: 2.1793\n",
      "[Epoch 5/8 | Step 370/500] - Loss: 2.1707\n",
      "[Epoch 5/8 | Step 380/500] - Loss: 2.1621\n",
      "[Epoch 5/8 | Step 390/500] - Loss: 2.1760\n",
      "[Epoch 5/8 | Step 400/500] - Loss: 2.1752\n",
      "[Epoch 5/8 | Step 410/500] - Loss: 2.1780\n",
      "[Epoch 5/8 | Step 420/500] - Loss: 2.1887\n",
      "[Epoch 5/8 | Step 430/500] - Loss: 2.1816\n",
      "[Epoch 5/8 | Step 440/500] - Loss: 2.1759\n",
      "[Epoch 5/8 | Step 450/500] - Loss: 2.1709\n",
      "[Epoch 5/8 | Step 460/500] - Loss: 2.1627\n",
      "[Epoch 5/8 | Step 470/500] - Loss: 2.1653\n",
      "[Epoch 5/8 | Step 480/500] - Loss: 2.1647\n",
      "[Epoch 5/8 | Step 490/500] - Loss: 2.1576\n",
      "Epoch 5/8 - Avg Train Loss: 2.1610, Val Loss: 2.2222\n",
      "[Epoch 6/8 | Step 0/500] - Loss: 1.0530\n",
      "[Epoch 6/8 | Step 10/500] - Loss: 1.5070\n",
      "[Epoch 6/8 | Step 20/500] - Loss: 1.8249\n",
      "[Epoch 6/8 | Step 30/500] - Loss: 1.9799\n",
      "[Epoch 6/8 | Step 40/500] - Loss: 2.0342\n",
      "[Epoch 6/8 | Step 50/500] - Loss: 2.0774\n",
      "[Epoch 6/8 | Step 60/500] - Loss: 2.2142\n",
      "[Epoch 6/8 | Step 70/500] - Loss: 2.2651\n",
      "[Epoch 6/8 | Step 80/500] - Loss: 2.2713\n",
      "[Epoch 6/8 | Step 90/500] - Loss: 2.2712\n",
      "[Epoch 6/8 | Step 100/500] - Loss: 2.2655\n",
      "[Epoch 6/8 | Step 110/500] - Loss: 2.2708\n",
      "[Epoch 6/8 | Step 120/500] - Loss: 2.3012\n",
      "[Epoch 6/8 | Step 130/500] - Loss: 2.2604\n",
      "[Epoch 6/8 | Step 140/500] - Loss: 2.2582\n",
      "[Epoch 6/8 | Step 150/500] - Loss: 2.2179\n",
      "[Epoch 6/8 | Step 160/500] - Loss: 2.2110\n",
      "[Epoch 6/8 | Step 170/500] - Loss: 2.1906\n",
      "[Epoch 6/8 | Step 180/500] - Loss: 2.1786\n",
      "[Epoch 6/8 | Step 190/500] - Loss: 2.2091\n",
      "[Epoch 6/8 | Step 200/500] - Loss: 2.1732\n",
      "[Epoch 6/8 | Step 210/500] - Loss: 2.1956\n",
      "[Epoch 6/8 | Step 220/500] - Loss: 2.2133\n",
      "[Epoch 6/8 | Step 230/500] - Loss: 2.2436\n",
      "[Epoch 6/8 | Step 240/500] - Loss: 2.2176\n",
      "[Epoch 6/8 | Step 250/500] - Loss: 2.1960\n",
      "[Epoch 6/8 | Step 260/500] - Loss: 2.2005\n",
      "[Epoch 6/8 | Step 270/500] - Loss: 2.2247\n",
      "[Epoch 6/8 | Step 280/500] - Loss: 2.2237\n",
      "[Epoch 6/8 | Step 290/500] - Loss: 2.2092\n",
      "[Epoch 6/8 | Step 300/500] - Loss: 2.1875\n",
      "[Epoch 6/8 | Step 310/500] - Loss: 2.1791\n",
      "[Epoch 6/8 | Step 320/500] - Loss: 2.1653\n",
      "[Epoch 6/8 | Step 330/500] - Loss: 2.1973\n",
      "[Epoch 6/8 | Step 340/500] - Loss: 2.2047\n",
      "[Epoch 6/8 | Step 350/500] - Loss: 2.2032\n",
      "[Epoch 6/8 | Step 360/500] - Loss: 2.1903\n",
      "[Epoch 6/8 | Step 370/500] - Loss: 2.1900\n",
      "[Epoch 6/8 | Step 380/500] - Loss: 2.1916\n",
      "[Epoch 6/8 | Step 390/500] - Loss: 2.1804\n",
      "[Epoch 6/8 | Step 400/500] - Loss: 2.1689\n",
      "[Epoch 6/8 | Step 410/500] - Loss: 2.1739\n",
      "[Epoch 6/8 | Step 420/500] - Loss: 2.1845\n",
      "[Epoch 6/8 | Step 430/500] - Loss: 2.1887\n",
      "[Epoch 6/8 | Step 440/500] - Loss: 2.1717\n",
      "[Epoch 6/8 | Step 450/500] - Loss: 2.1723\n",
      "[Epoch 6/8 | Step 460/500] - Loss: 2.1663\n",
      "[Epoch 6/8 | Step 470/500] - Loss: 2.1648\n",
      "[Epoch 6/8 | Step 480/500] - Loss: 2.1575\n",
      "[Epoch 6/8 | Step 490/500] - Loss: 2.1593\n",
      "Epoch 6/8 - Avg Train Loss: 2.1634, Val Loss: 2.2358\n",
      "[Epoch 7/8 | Step 0/500] - Loss: 1.0869\n",
      "[Epoch 7/8 | Step 10/500] - Loss: 3.0613\n",
      "[Epoch 7/8 | Step 20/500] - Loss: 2.7773\n",
      "[Epoch 7/8 | Step 30/500] - Loss: 2.3819\n",
      "[Epoch 7/8 | Step 40/500] - Loss: 2.2010\n",
      "[Epoch 7/8 | Step 50/500] - Loss: 2.1993\n",
      "[Epoch 7/8 | Step 60/500] - Loss: 2.1963\n",
      "[Epoch 7/8 | Step 70/500] - Loss: 2.1885\n",
      "[Epoch 7/8 | Step 80/500] - Loss: 2.2177\n",
      "[Epoch 7/8 | Step 90/500] - Loss: 2.2500\n",
      "[Epoch 7/8 | Step 100/500] - Loss: 2.2291\n",
      "[Epoch 7/8 | Step 110/500] - Loss: 2.2226\n",
      "[Epoch 7/8 | Step 120/500] - Loss: 2.2320\n",
      "[Epoch 7/8 | Step 130/500] - Loss: 2.2225\n",
      "[Epoch 7/8 | Step 140/500] - Loss: 2.2315\n",
      "[Epoch 7/8 | Step 150/500] - Loss: 2.2602\n",
      "[Epoch 7/8 | Step 160/500] - Loss: 2.2514\n",
      "[Epoch 7/8 | Step 170/500] - Loss: 2.1973\n",
      "[Epoch 7/8 | Step 180/500] - Loss: 2.1993\n",
      "[Epoch 7/8 | Step 190/500] - Loss: 2.1809\n",
      "[Epoch 7/8 | Step 200/500] - Loss: 2.1777\n",
      "[Epoch 7/8 | Step 210/500] - Loss: 2.1449\n",
      "[Epoch 7/8 | Step 220/500] - Loss: 2.1432\n",
      "[Epoch 7/8 | Step 230/500] - Loss: 2.1318\n",
      "[Epoch 7/8 | Step 240/500] - Loss: 2.1672\n",
      "[Epoch 7/8 | Step 250/500] - Loss: 2.2143\n",
      "[Epoch 7/8 | Step 260/500] - Loss: 2.2444\n",
      "[Epoch 7/8 | Step 270/500] - Loss: 2.2294\n",
      "[Epoch 7/8 | Step 280/500] - Loss: 2.2215\n",
      "[Epoch 7/8 | Step 290/500] - Loss: 2.1994\n",
      "[Epoch 7/8 | Step 300/500] - Loss: 2.1924\n",
      "[Epoch 7/8 | Step 310/500] - Loss: 2.1710\n",
      "[Epoch 7/8 | Step 320/500] - Loss: 2.1579\n",
      "[Epoch 7/8 | Step 330/500] - Loss: 2.1687\n",
      "[Epoch 7/8 | Step 340/500] - Loss: 2.1651\n",
      "[Epoch 7/8 | Step 350/500] - Loss: 2.1613\n",
      "[Epoch 7/8 | Step 360/500] - Loss: 2.1701\n",
      "[Epoch 7/8 | Step 370/500] - Loss: 2.1613\n",
      "[Epoch 7/8 | Step 380/500] - Loss: 2.1810\n",
      "[Epoch 7/8 | Step 390/500] - Loss: 2.1864\n",
      "[Epoch 7/8 | Step 400/500] - Loss: 2.1855\n",
      "[Epoch 7/8 | Step 410/500] - Loss: 2.1912\n",
      "[Epoch 7/8 | Step 420/500] - Loss: 2.1874\n",
      "[Epoch 7/8 | Step 430/500] - Loss: 2.1901\n",
      "[Epoch 7/8 | Step 440/500] - Loss: 2.1828\n",
      "[Epoch 7/8 | Step 450/500] - Loss: 2.1850\n",
      "[Epoch 7/8 | Step 460/500] - Loss: 2.1905\n",
      "[Epoch 7/8 | Step 470/500] - Loss: 2.1854\n",
      "[Epoch 7/8 | Step 480/500] - Loss: 2.1802\n",
      "[Epoch 7/8 | Step 490/500] - Loss: 2.1677\n",
      "Epoch 7/8 - Avg Train Loss: 2.1640, Val Loss: 2.2397\n",
      "[Epoch 8/8 | Step 0/500] - Loss: 4.3247\n",
      "[Epoch 8/8 | Step 10/500] - Loss: 1.8558\n",
      "[Epoch 8/8 | Step 20/500] - Loss: 2.0122\n",
      "[Epoch 8/8 | Step 30/500] - Loss: 2.1110\n",
      "[Epoch 8/8 | Step 40/500] - Loss: 2.3047\n",
      "[Epoch 8/8 | Step 50/500] - Loss: 2.2757\n",
      "[Epoch 8/8 | Step 60/500] - Loss: 2.2615\n",
      "[Epoch 8/8 | Step 70/500] - Loss: 2.1584\n",
      "[Epoch 8/8 | Step 80/500] - Loss: 2.2310\n",
      "[Epoch 8/8 | Step 90/500] - Loss: 2.3099\n",
      "[Epoch 8/8 | Step 100/500] - Loss: 2.2840\n",
      "[Epoch 8/8 | Step 110/500] - Loss: 2.2903\n",
      "[Epoch 8/8 | Step 120/500] - Loss: 2.3060\n",
      "[Epoch 8/8 | Step 130/500] - Loss: 2.2653\n",
      "[Epoch 8/8 | Step 140/500] - Loss: 2.2548\n",
      "[Epoch 8/8 | Step 150/500] - Loss: 2.2716\n",
      "[Epoch 8/8 | Step 160/500] - Loss: 2.2814\n",
      "[Epoch 8/8 | Step 170/500] - Loss: 2.2579\n",
      "[Epoch 8/8 | Step 180/500] - Loss: 2.2435\n",
      "[Epoch 8/8 | Step 190/500] - Loss: 2.2879\n",
      "[Epoch 8/8 | Step 200/500] - Loss: 2.2781\n",
      "[Epoch 8/8 | Step 210/500] - Loss: 2.2684\n",
      "[Epoch 8/8 | Step 220/500] - Loss: 2.2644\n",
      "[Epoch 8/8 | Step 230/500] - Loss: 2.2308\n",
      "[Epoch 8/8 | Step 240/500] - Loss: 2.2091\n",
      "[Epoch 8/8 | Step 250/500] - Loss: 2.1991\n",
      "[Epoch 8/8 | Step 260/500] - Loss: 2.1941\n",
      "[Epoch 8/8 | Step 270/500] - Loss: 2.1815\n",
      "[Epoch 8/8 | Step 280/500] - Loss: 2.1912\n",
      "[Epoch 8/8 | Step 290/500] - Loss: 2.1722\n",
      "[Epoch 8/8 | Step 300/500] - Loss: 2.1641\n",
      "[Epoch 8/8 | Step 310/500] - Loss: 2.1585\n",
      "[Epoch 8/8 | Step 320/500] - Loss: 2.1710\n",
      "[Epoch 8/8 | Step 330/500] - Loss: 2.1716\n",
      "[Epoch 8/8 | Step 340/500] - Loss: 2.1816\n",
      "[Epoch 8/8 | Step 350/500] - Loss: 2.1757\n",
      "[Epoch 8/8 | Step 360/500] - Loss: 2.1658\n",
      "[Epoch 8/8 | Step 370/500] - Loss: 2.1600\n",
      "[Epoch 8/8 | Step 380/500] - Loss: 2.1586\n",
      "[Epoch 8/8 | Step 390/500] - Loss: 2.1539\n",
      "[Epoch 8/8 | Step 400/500] - Loss: 2.1720\n",
      "[Epoch 8/8 | Step 410/500] - Loss: 2.1523\n",
      "[Epoch 8/8 | Step 420/500] - Loss: 2.1548\n",
      "[Epoch 8/8 | Step 430/500] - Loss: 2.1522\n",
      "[Epoch 8/8 | Step 440/500] - Loss: 2.1427\n",
      "[Epoch 8/8 | Step 450/500] - Loss: 2.1261\n",
      "[Epoch 8/8 | Step 460/500] - Loss: 2.1416\n",
      "[Epoch 8/8 | Step 470/500] - Loss: 2.1479\n",
      "[Epoch 8/8 | Step 480/500] - Loss: 2.1433\n",
      "[Epoch 8/8 | Step 490/500] - Loss: 2.1506\n",
      "Epoch 8/8 - Avg Train Loss: 2.1411, Val Loss: 2.2408\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Set up the GPU environment and return the appropriate device.\"\"\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "  \n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    return device\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model_config.use_cache = False\n",
    "\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        config=model_config, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"SEQ_CLS\")\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "class PreferenceEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Dataset to create pairs of message, preferred response, and rejected response for DPO training.\n",
    "        \"\"\"\n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pairs = self._create_preference_pairs()\n",
    "\n",
    "    def _create_preference_pairs(self):\n",
    "        \"\"\"\n",
    "        Create pairs using emails from the dataset based on their labels.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for _, selected_email in self.emails_df.iterrows():\n",
    "            selected_label = selected_email['label']\n",
    "            ham_emails = self.emails_df[self.emails_df['label'] == 0]\n",
    "            phish_emails = self.emails_df[self.emails_df['label'] == 1]\n",
    "\n",
    "            if selected_label == 1:  # Phishing email\n",
    "                # Preferred: Another phishing email\n",
    "                preferred_email = phish_emails[phish_emails.index != selected_email.name].sample(n=1).iloc[0]\n",
    "                # Rejected: A ham email\n",
    "                rejected_email = ham_emails.sample(n=1).iloc[0]\n",
    "                pairs.append({\n",
    "                    'message': selected_email,\n",
    "                    'preferred': preferred_email,\n",
    "                    'rejected': rejected_email\n",
    "                })\n",
    "            elif selected_label == 0:  # Ham email\n",
    "                # Preferred: Another ham email\n",
    "                preferred_email = ham_emails[ham_emails.index != selected_email.name].sample(n=1).iloc[0]\n",
    "                # Rejected: A phishing email\n",
    "                rejected_email = phish_emails.sample(n=1).iloc[0]\n",
    "                pairs.append({\n",
    "                    'message': selected_email,\n",
    "                    'preferred': preferred_email,\n",
    "                    'rejected': rejected_email\n",
    "                })\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def _prepare_email_input(self, message, response):\n",
    "        \"\"\"\n",
    "        Prepare the input text with formatted message and response for tokenization.\n",
    "        \"\"\"\n",
    "        formatted_input = f\"<s>[INST] {message} [/INST] {response}</s>\"\n",
    "        return self.tokenizer(\n",
    "            formatted_input,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        pair = self.pairs[idx]\n",
    "\n",
    "\n",
    "        message_text = f\"Sender: {pair['message']['sender']} [SEP] Subject: {pair['message']['subject']} [SEP] {pair['message']['body']}\"\n",
    "     \n",
    "        preferred_response = f\"Sender: {pair['preferred']['sender']} [SEP] Subject: {pair['preferred']['subject']} [SEP] {pair['preferred']['body']}\"\n",
    "        rejected_response = f\"Sender: {pair['rejected']['sender']} [SEP] Subject: {pair['rejected']['subject']} [SEP] {pair['rejected']['body']}\"\n",
    "        message_inputs = self._prepare_email_input(message_text, \"\")\n",
    "        preferred_inputs = self._prepare_email_input(message_text, preferred_response)\n",
    "        rejected_inputs = self._prepare_email_input(message_text, rejected_response)\n",
    "\n",
    "        return {\n",
    "            'message_input_ids': message_inputs['input_ids'].squeeze(),\n",
    "            'message_attention_mask': message_inputs['attention_mask'].squeeze(),\n",
    "            'preferred_input_ids': preferred_inputs['input_ids'].squeeze(),\n",
    "            'preferred_attention_mask': preferred_inputs['attention_mask'].squeeze(),\n",
    "            'rejected_input_ids': rejected_inputs['input_ids'].squeeze(),\n",
    "            'rejected_attention_mask': rejected_inputs['attention_mask'].squeeze(),\n",
    "        }\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "   \n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def compute_dpo_loss(policy_chosen_logits, policy_rejected_logits, \n",
    "                    reference_chosen_logits, reference_rejected_logits, \n",
    "                    beta=0.2):\n",
    "   \n",
    "    epsilon = 1e-8\n",
    "    \n",
    "   \n",
    "    policy_chosen_probs = F.softmax(policy_chosen_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    policy_rejected_probs = F.softmax(policy_rejected_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    ref_chosen_probs = F.softmax(reference_chosen_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    ref_rejected_probs = F.softmax(reference_rejected_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    \n",
    "  \n",
    "    chosen_rewards = (torch.log(policy_chosen_probs + epsilon) - \n",
    "                     torch.log(ref_chosen_probs + epsilon))\n",
    "    rejected_rewards = (torch.log(policy_rejected_probs + epsilon) - \n",
    "                       torch.log(ref_rejected_probs + epsilon))\n",
    "    \n",
    "    \n",
    "    max_reward = 50.0\n",
    "    chosen_rewards = torch.clamp(chosen_rewards, -max_reward, max_reward)\n",
    "    rejected_rewards = torch.clamp(rejected_rewards, -max_reward, max_reward)\n",
    "    \n",
    "    \n",
    "    logits_diff = (chosen_rewards - rejected_rewards) / beta\n",
    "    \n",
    "    valid_mask = ~torch.isnan(logits_diff)\n",
    "    if valid_mask.any():\n",
    "        loss = -F.logsigmoid(logits_diff[valid_mask]).mean()\n",
    "    else:\n",
    "        loss = torch.tensor(0.0, device=logits_diff.device)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_model_dpo(policy_model, reference_model, train_loader, val_loader, \n",
    "                   optimizer, scheduler, device, num_epochs=5, beta=0.2, gradient_accumulation_steps=2):\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    policy_model = policy_model.to(device).float()\n",
    "    reference_model = reference_model.to(device).float()\n",
    "    reference_model.eval()  # Ensure reference model does not get updated during training\n",
    "    \n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        policy_model.train()\n",
    "        total_loss = 0\n",
    "        valid_steps = 0\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            try:\n",
    "                with torch.amp.autocast('cuda', dtype=torch.float32):\n",
    "                    policy_chosen_outputs = policy_model(\n",
    "                        input_ids=batch['preferred_input_ids'],\n",
    "                        attention_mask=batch['preferred_attention_mask']\n",
    "                    )\n",
    "                    policy_rejected_outputs = policy_model(\n",
    "                        input_ids=batch['rejected_input_ids'],\n",
    "                        attention_mask=batch['rejected_attention_mask']\n",
    "                    )\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        ref_chosen_outputs = reference_model(\n",
    "                            input_ids=batch['preferred_input_ids'],\n",
    "                            attention_mask=batch['preferred_attention_mask']\n",
    "                        )\n",
    "                        ref_rejected_outputs = reference_model(\n",
    "                            input_ids=batch['rejected_input_ids'],\n",
    "                            attention_mask=batch['rejected_attention_mask']\n",
    "                        )\n",
    "                    \n",
    "                    loss = compute_dpo_loss(\n",
    "                        policy_chosen_outputs.logits,\n",
    "                        policy_rejected_outputs.logits,\n",
    "                        ref_chosen_outputs.logits,\n",
    "                        ref_rejected_outputs.logits,\n",
    "                        beta=beta\n",
    "                    )\n",
    "                    \n",
    "                    if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                        scaler.scale(loss).backward()\n",
    "                        \n",
    "                        # Gradient accumulation logic\n",
    "                        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                            scaler.unscale_(optimizer)\n",
    "                            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                            scheduler.step()\n",
    "                            optimizer.zero_grad()\n",
    "                        \n",
    "                        total_loss += loss.item()\n",
    "                        valid_steps += 1\n",
    "                    \n",
    "                    if step % 10 == 0:\n",
    "                        avg_loss = total_loss / max(valid_steps, 1)\n",
    "                        print(f\"[Epoch {epoch+1}/{num_epochs} | Step {step}/{len(train_loader)}] - Loss: {avg_loss:.4f}\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {step}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        if valid_steps > 0:\n",
    "            avg_train_loss = total_loss / valid_steps\n",
    "            val_loss = evaluate_model_dpo(policy_model, reference_model, val_loader, device, beta)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Avg Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = {k: v.cpu() for k, v in policy_model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
    "    \n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model_dpo(policy_model, reference_model, val_loader, device, beta):\n",
    "   \n",
    "    policy_model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.amp.autocast('cuda', dtype=torch.float32):\n",
    "                policy_chosen_outputs = policy_model(\n",
    "                    input_ids=batch['preferred_input_ids'],\n",
    "                    attention_mask=batch['preferred_attention_mask']\n",
    "                )\n",
    "                policy_rejected_outputs = policy_model(\n",
    "                    input_ids=batch['rejected_input_ids'],\n",
    "                    attention_mask=batch['rejected_attention_mask']\n",
    "                )\n",
    "                \n",
    "                ref_chosen_outputs = reference_model(\n",
    "                    input_ids=batch['preferred_input_ids'],\n",
    "                    attention_mask=batch['preferred_attention_mask']\n",
    "                )\n",
    "                ref_rejected_outputs = reference_model(\n",
    "                    input_ids=batch['rejected_input_ids'],\n",
    "                    attention_mask=batch['rejected_attention_mask']\n",
    "                )\n",
    "                \n",
    "                loss = compute_dpo_loss(\n",
    "                    policy_chosen_outputs.logits,\n",
    "                    policy_rejected_outputs.logits,\n",
    "                    ref_chosen_outputs.logits,\n",
    "                    ref_rejected_outputs.logits,\n",
    "                    beta=beta\n",
    "                )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "def main():\n",
    "\n",
    "  \n",
    "    device = setup_environment()\n",
    "    model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/final_data.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    policy_model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    reference_model, _ = setup_model_and_tokenizer(model_name, device)\n",
    "    \n",
    "\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "  \n",
    "    train_dataset = PreferenceEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = PreferenceEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # Setup optimization\n",
    "    optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "    num_epochs = 8\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    best_model_state = train_model_dpo(\n",
    "        policy_model,\n",
    "        reference_model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        num_epochs=num_epochs,\n",
    "        beta=0.2\n",
    "    )\n",
    "\n",
    "   \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/llama_8b_dpo_classification_model\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    policy_model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": s tr(device),\n",
    "        \"beta\": 0.2\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4de264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5be2de10d94152a16452533df1b451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1847de2a020429a8071e8fac669bc7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/users/skuikel/.local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/8 | Step 0/500] - Loss: 1.0263\n",
      "[Epoch 1/8 | Step 10/500] - Loss: 2.2608\n",
      "[Epoch 1/8 | Step 20/500] - Loss: 2.6308\n",
      "[Epoch 1/8 | Step 30/500] - Loss: 3.0243\n",
      "[Epoch 1/8 | Step 40/500] - Loss: 3.1960\n",
      "[Epoch 1/8 | Step 50/500] - Loss: 3.4414\n",
      "[Epoch 1/8 | Step 60/500] - Loss: 3.3087\n",
      "[Epoch 1/8 | Step 70/500] - Loss: 3.2294\n",
      "[Epoch 1/8 | Step 80/500] - Loss: 3.2480\n",
      "[Epoch 1/8 | Step 90/500] - Loss: 3.3849\n",
      "[Epoch 1/8 | Step 100/500] - Loss: 3.3588\n",
      "[Epoch 1/8 | Step 110/500] - Loss: 3.5123\n",
      "[Epoch 1/8 | Step 120/500] - Loss: 3.5350\n",
      "[Epoch 1/8 | Step 130/500] - Loss: 3.5167\n",
      "[Epoch 1/8 | Step 140/500] - Loss: 3.4866\n",
      "[Epoch 1/8 | Step 150/500] - Loss: 3.4178\n",
      "[Epoch 1/8 | Step 160/500] - Loss: 3.3666\n",
      "[Epoch 1/8 | Step 170/500] - Loss: 3.4325\n",
      "[Epoch 1/8 | Step 180/500] - Loss: 3.4171\n",
      "[Epoch 1/8 | Step 190/500] - Loss: 3.4409\n",
      "[Epoch 1/8 | Step 200/500] - Loss: 3.3828\n",
      "[Epoch 1/8 | Step 210/500] - Loss: 3.3968\n",
      "[Epoch 1/8 | Step 220/500] - Loss: 3.4264\n",
      "[Epoch 1/8 | Step 230/500] - Loss: 3.3990\n",
      "[Epoch 1/8 | Step 240/500] - Loss: 3.3764\n",
      "[Epoch 1/8 | Step 250/500] - Loss: 3.3381\n",
      "[Epoch 1/8 | Step 260/500] - Loss: 3.2952\n",
      "[Epoch 1/8 | Step 270/500] - Loss: 3.3111\n",
      "[Epoch 1/8 | Step 280/500] - Loss: 3.2815\n",
      "[Epoch 1/8 | Step 290/500] - Loss: 3.2664\n",
      "[Epoch 1/8 | Step 300/500] - Loss: 3.2340\n",
      "[Epoch 1/8 | Step 310/500] - Loss: 3.2373\n",
      "[Epoch 1/8 | Step 320/500] - Loss: 3.2227\n",
      "[Epoch 1/8 | Step 330/500] - Loss: 3.1813\n",
      "[Epoch 1/8 | Step 340/500] - Loss: 3.1653\n",
      "[Epoch 1/8 | Step 350/500] - Loss: 3.1381\n",
      "[Epoch 1/8 | Step 360/500] - Loss: 3.1347\n",
      "[Epoch 1/8 | Step 370/500] - Loss: 3.1342\n",
      "[Epoch 1/8 | Step 380/500] - Loss: 3.1189\n",
      "[Epoch 1/8 | Step 390/500] - Loss: 3.0764\n",
      "[Epoch 1/8 | Step 400/500] - Loss: 3.0519\n",
      "[Epoch 1/8 | Step 410/500] - Loss: 3.0456\n",
      "[Epoch 1/8 | Step 420/500] - Loss: 3.0254\n",
      "[Epoch 1/8 | Step 430/500] - Loss: 3.0160\n",
      "[Epoch 1/8 | Step 440/500] - Loss: 3.0172\n",
      "[Epoch 1/8 | Step 450/500] - Loss: 3.0069\n",
      "[Epoch 1/8 | Step 460/500] - Loss: 2.9872\n",
      "[Epoch 1/8 | Step 470/500] - Loss: 2.9606\n",
      "[Epoch 1/8 | Step 480/500] - Loss: 2.9628\n",
      "[Epoch 1/8 | Step 490/500] - Loss: 2.9493\n",
      "Epoch 1/8 - Avg Train Loss: 2.9347, Val Loss: 2.4442\n",
      "[Epoch 2/8 | Step 0/500] - Loss: 3.8091\n",
      "[Epoch 2/8 | Step 10/500] - Loss: 1.9032\n",
      "[Epoch 2/8 | Step 20/500] - Loss: 1.9208\n",
      "[Epoch 2/8 | Step 30/500] - Loss: 2.1927\n",
      "[Epoch 2/8 | Step 40/500] - Loss: 2.1305\n",
      "[Epoch 2/8 | Step 50/500] - Loss: 2.2342\n",
      "[Epoch 2/8 | Step 60/500] - Loss: 2.1914\n",
      "[Epoch 2/8 | Step 70/500] - Loss: 2.1486\n",
      "[Epoch 2/8 | Step 80/500] - Loss: 2.1746\n",
      "[Epoch 2/8 | Step 90/500] - Loss: 2.1308\n",
      "[Epoch 2/8 | Step 100/500] - Loss: 2.1215\n",
      "[Epoch 2/8 | Step 110/500] - Loss: 2.0780\n",
      "[Epoch 2/8 | Step 120/500] - Loss: 2.1295\n",
      "[Epoch 2/8 | Step 130/500] - Loss: 2.1330\n",
      "[Epoch 2/8 | Step 140/500] - Loss: 2.0949\n",
      "[Epoch 2/8 | Step 150/500] - Loss: 2.0903\n",
      "[Epoch 2/8 | Step 160/500] - Loss: 2.0707\n",
      "[Epoch 2/8 | Step 170/500] - Loss: 2.0689\n",
      "[Epoch 2/8 | Step 180/500] - Loss: 2.0971\n",
      "[Epoch 2/8 | Step 190/500] - Loss: 2.0621\n",
      "[Epoch 2/8 | Step 200/500] - Loss: 2.1124\n",
      "[Epoch 2/8 | Step 210/500] - Loss: 2.1339\n",
      "[Epoch 2/8 | Step 220/500] - Loss: 2.1082\n",
      "[Epoch 2/8 | Step 230/500] - Loss: 2.0901\n",
      "[Epoch 2/8 | Step 240/500] - Loss: 2.1226\n",
      "[Epoch 2/8 | Step 250/500] - Loss: 2.1212\n",
      "[Epoch 2/8 | Step 260/500] - Loss: 2.1068\n",
      "[Epoch 2/8 | Step 270/500] - Loss: 2.0989\n",
      "[Epoch 2/8 | Step 280/500] - Loss: 2.1075\n",
      "[Epoch 2/8 | Step 290/500] - Loss: 2.1000\n",
      "[Epoch 2/8 | Step 300/500] - Loss: 2.1012\n",
      "[Epoch 2/8 | Step 310/500] - Loss: 2.0837\n",
      "[Epoch 2/8 | Step 320/500] - Loss: 2.0654\n",
      "[Epoch 2/8 | Step 330/500] - Loss: 2.0559\n",
      "[Epoch 2/8 | Step 340/500] - Loss: 2.0528\n",
      "[Epoch 2/8 | Step 350/500] - Loss: 2.0385\n",
      "[Epoch 2/8 | Step 360/500] - Loss: 2.0346\n",
      "[Epoch 2/8 | Step 370/500] - Loss: 2.0332\n",
      "[Epoch 2/8 | Step 380/500] - Loss: 2.0452\n",
      "[Epoch 2/8 | Step 390/500] - Loss: 2.0392\n",
      "[Epoch 2/8 | Step 400/500] - Loss: 2.0490\n",
      "[Epoch 2/8 | Step 410/500] - Loss: 2.0691\n",
      "[Epoch 2/8 | Step 420/500] - Loss: 2.0632\n",
      "[Epoch 2/8 | Step 430/500] - Loss: 2.0545\n",
      "[Epoch 2/8 | Step 440/500] - Loss: 2.0485\n",
      "[Epoch 2/8 | Step 450/500] - Loss: 2.0450\n",
      "[Epoch 2/8 | Step 460/500] - Loss: 2.0303\n",
      "[Epoch 2/8 | Step 470/500] - Loss: 2.0351\n",
      "[Epoch 2/8 | Step 480/500] - Loss: 2.0427\n",
      "[Epoch 2/8 | Step 490/500] - Loss: 2.0273\n",
      "Epoch 2/8 - Avg Train Loss: 2.0239, Val Loss: 2.2330\n",
      "[Epoch 3/8 | Step 0/500] - Loss: 1.5426\n",
      "[Epoch 3/8 | Step 10/500] - Loss: 1.4175\n",
      "[Epoch 3/8 | Step 20/500] - Loss: 1.5449\n",
      "[Epoch 3/8 | Step 30/500] - Loss: 2.0407\n",
      "[Epoch 3/8 | Step 40/500] - Loss: 2.1779\n",
      "[Epoch 3/8 | Step 50/500] - Loss: 2.1348\n",
      "[Epoch 3/8 | Step 60/500] - Loss: 2.0807\n",
      "[Epoch 3/8 | Step 70/500] - Loss: 2.1126\n",
      "[Epoch 3/8 | Step 80/500] - Loss: 2.0947\n",
      "[Epoch 3/8 | Step 90/500] - Loss: 2.1729\n",
      "[Epoch 3/8 | Step 100/500] - Loss: 2.1509\n",
      "[Epoch 3/8 | Step 110/500] - Loss: 2.1035\n",
      "[Epoch 3/8 | Step 120/500] - Loss: 2.0800\n",
      "[Epoch 3/8 | Step 130/500] - Loss: 2.0565\n",
      "[Epoch 3/8 | Step 140/500] - Loss: 2.0408\n",
      "[Epoch 3/8 | Step 150/500] - Loss: 2.0800\n",
      "[Epoch 3/8 | Step 160/500] - Loss: 2.0690\n",
      "[Epoch 3/8 | Step 170/500] - Loss: 2.0720\n",
      "[Epoch 3/8 | Step 180/500] - Loss: 2.0663\n",
      "[Epoch 3/8 | Step 190/500] - Loss: 2.0629\n",
      "[Epoch 3/8 | Step 200/500] - Loss: 2.0270\n",
      "[Epoch 3/8 | Step 210/500] - Loss: 2.0415\n",
      "[Epoch 3/8 | Step 220/500] - Loss: 2.0348\n",
      "[Epoch 3/8 | Step 230/500] - Loss: 2.0393\n",
      "[Epoch 3/8 | Step 240/500] - Loss: 2.0173\n",
      "[Epoch 3/8 | Step 250/500] - Loss: 2.0248\n",
      "[Epoch 3/8 | Step 260/500] - Loss: 2.0173\n",
      "[Epoch 3/8 | Step 270/500] - Loss: 2.0199\n",
      "[Epoch 3/8 | Step 280/500] - Loss: 1.9926\n",
      "[Epoch 3/8 | Step 290/500] - Loss: 1.9864\n",
      "[Epoch 3/8 | Step 300/500] - Loss: 1.9890\n",
      "[Epoch 3/8 | Step 310/500] - Loss: 1.9824\n",
      "[Epoch 3/8 | Step 320/500] - Loss: 1.9756\n",
      "[Epoch 3/8 | Step 330/500] - Loss: 1.9657\n",
      "[Epoch 3/8 | Step 340/500] - Loss: 1.9674\n",
      "[Epoch 3/8 | Step 350/500] - Loss: 1.9484\n",
      "[Epoch 3/8 | Step 360/500] - Loss: 1.9336\n",
      "[Epoch 3/8 | Step 370/500] - Loss: 1.9186\n",
      "[Epoch 3/8 | Step 380/500] - Loss: 1.9245\n",
      "[Epoch 3/8 | Step 390/500] - Loss: 1.9354\n",
      "[Epoch 3/8 | Step 400/500] - Loss: 1.9458\n",
      "[Epoch 3/8 | Step 410/500] - Loss: 1.9359\n",
      "[Epoch 3/8 | Step 420/500] - Loss: 1.9333\n",
      "[Epoch 3/8 | Step 430/500] - Loss: 1.9312\n",
      "[Epoch 3/8 | Step 440/500] - Loss: 1.9162\n",
      "[Epoch 3/8 | Step 450/500] - Loss: 1.8939\n",
      "[Epoch 3/8 | Step 460/500] - Loss: 1.8798\n",
      "[Epoch 3/8 | Step 470/500] - Loss: 1.8701\n",
      "[Epoch 3/8 | Step 480/500] - Loss: 1.8701\n",
      "[Epoch 3/8 | Step 490/500] - Loss: 1.8673\n",
      "Epoch 3/8 - Avg Train Loss: 1.8548, Val Loss: 1.7894\n",
      "[Epoch 4/8 | Step 0/500] - Loss: 1.5024\n",
      "[Epoch 4/8 | Step 10/500] - Loss: 0.9953\n",
      "[Epoch 4/8 | Step 20/500] - Loss: 1.3181\n",
      "[Epoch 4/8 | Step 30/500] - Loss: 1.3211\n",
      "[Epoch 4/8 | Step 40/500] - Loss: 1.4025\n",
      "[Epoch 4/8 | Step 50/500] - Loss: 1.5951\n",
      "[Epoch 4/8 | Step 60/500] - Loss: 1.5609\n",
      "[Epoch 4/8 | Step 70/500] - Loss: 1.6281\n",
      "[Epoch 4/8 | Step 80/500] - Loss: 1.6424\n",
      "[Epoch 4/8 | Step 90/500] - Loss: 1.6342\n",
      "[Epoch 4/8 | Step 100/500] - Loss: 1.6428\n",
      "[Epoch 4/8 | Step 110/500] - Loss: 1.5877\n",
      "[Epoch 4/8 | Step 120/500] - Loss: 1.6134\n",
      "[Epoch 4/8 | Step 130/500] - Loss: 1.5691\n",
      "[Epoch 4/8 | Step 140/500] - Loss: 1.5271\n",
      "[Epoch 4/8 | Step 150/500] - Loss: 1.5572\n",
      "[Epoch 4/8 | Step 160/500] - Loss: 1.5047\n",
      "[Epoch 4/8 | Step 170/500] - Loss: 1.4801\n",
      "[Epoch 4/8 | Step 180/500] - Loss: 1.4765\n",
      "[Epoch 4/8 | Step 190/500] - Loss: 1.4534\n",
      "[Epoch 4/8 | Step 200/500] - Loss: 1.4596\n",
      "[Epoch 4/8 | Step 210/500] - Loss: 1.4205\n",
      "[Epoch 4/8 | Step 220/500] - Loss: 1.3960\n",
      "[Epoch 4/8 | Step 230/500] - Loss: 1.4113\n",
      "[Epoch 4/8 | Step 240/500] - Loss: 1.4302\n",
      "[Epoch 4/8 | Step 250/500] - Loss: 1.4413\n",
      "[Epoch 4/8 | Step 260/500] - Loss: 1.4431\n",
      "[Epoch 4/8 | Step 270/500] - Loss: 1.4454\n",
      "[Epoch 4/8 | Step 280/500] - Loss: 1.4184\n",
      "[Epoch 4/8 | Step 290/500] - Loss: 1.3833\n",
      "[Epoch 4/8 | Step 300/500] - Loss: 1.3846\n",
      "[Epoch 4/8 | Step 310/500] - Loss: 1.3783\n",
      "[Epoch 4/8 | Step 320/500] - Loss: 1.3682\n",
      "[Epoch 4/8 | Step 330/500] - Loss: 1.3711\n",
      "[Epoch 4/8 | Step 340/500] - Loss: 1.3520\n",
      "[Epoch 4/8 | Step 350/500] - Loss: 1.3563\n",
      "[Epoch 4/8 | Step 360/500] - Loss: 1.3656\n",
      "[Epoch 4/8 | Step 370/500] - Loss: 1.3620\n",
      "[Epoch 4/8 | Step 380/500] - Loss: 1.3638\n",
      "[Epoch 4/8 | Step 390/500] - Loss: 1.3592\n",
      "[Epoch 4/8 | Step 400/500] - Loss: 1.3515\n",
      "[Epoch 4/8 | Step 410/500] - Loss: 1.3595\n",
      "[Epoch 4/8 | Step 420/500] - Loss: 1.3478\n",
      "[Epoch 4/8 | Step 430/500] - Loss: 1.3495\n",
      "[Epoch 4/8 | Step 440/500] - Loss: 1.3442\n",
      "[Epoch 4/8 | Step 450/500] - Loss: 1.3381\n",
      "[Epoch 4/8 | Step 460/500] - Loss: 1.3415\n",
      "[Epoch 4/8 | Step 470/500] - Loss: 1.3504\n",
      "[Epoch 4/8 | Step 480/500] - Loss: 1.3503\n",
      "[Epoch 4/8 | Step 490/500] - Loss: 1.3445\n",
      "Epoch 4/8 - Avg Train Loss: 1.3314, Val Loss: 1.3827\n",
      "[Epoch 5/8 | Step 0/500] - Loss: 0.7553\n",
      "[Epoch 5/8 | Step 10/500] - Loss: 1.7695\n",
      "[Epoch 5/8 | Step 20/500] - Loss: 1.6060\n",
      "[Epoch 5/8 | Step 30/500] - Loss: 1.3388\n",
      "[Epoch 5/8 | Step 40/500] - Loss: 1.3694\n",
      "[Epoch 5/8 | Step 50/500] - Loss: 1.2802\n",
      "[Epoch 5/8 | Step 60/500] - Loss: 1.2248\n",
      "[Epoch 5/8 | Step 70/500] - Loss: 1.2437\n",
      "[Epoch 5/8 | Step 80/500] - Loss: 1.3571\n",
      "[Epoch 5/8 | Step 90/500] - Loss: 1.3765\n",
      "[Epoch 5/8 | Step 100/500] - Loss: 1.3894\n",
      "[Epoch 5/8 | Step 110/500] - Loss: 1.3412\n",
      "[Epoch 5/8 | Step 120/500] - Loss: 1.3540\n",
      "[Epoch 5/8 | Step 130/500] - Loss: 1.3814\n",
      "[Epoch 5/8 | Step 140/500] - Loss: 1.3532\n",
      "[Epoch 5/8 | Step 150/500] - Loss: 1.3220\n",
      "[Epoch 5/8 | Step 160/500] - Loss: 1.3008\n",
      "[Epoch 5/8 | Step 170/500] - Loss: 1.3061\n",
      "[Epoch 5/8 | Step 180/500] - Loss: 1.2692\n",
      "[Epoch 5/8 | Step 190/500] - Loss: 1.2869\n",
      "[Epoch 5/8 | Step 200/500] - Loss: 1.2647\n",
      "[Epoch 5/8 | Step 210/500] - Loss: 1.2378\n",
      "[Epoch 5/8 | Step 220/500] - Loss: 1.2330\n",
      "[Epoch 5/8 | Step 230/500] - Loss: 1.2384\n",
      "[Epoch 5/8 | Step 240/500] - Loss: 1.2423\n",
      "[Epoch 5/8 | Step 250/500] - Loss: 1.2328\n",
      "[Epoch 5/8 | Step 260/500] - Loss: 1.2311\n",
      "[Epoch 5/8 | Step 270/500] - Loss: 1.2253\n",
      "[Epoch 5/8 | Step 280/500] - Loss: 1.2028\n",
      "[Epoch 5/8 | Step 290/500] - Loss: 1.1795\n",
      "[Epoch 5/8 | Step 300/500] - Loss: 1.1489\n",
      "[Epoch 5/8 | Step 310/500] - Loss: 1.1348\n",
      "[Epoch 5/8 | Step 320/500] - Loss: 1.1110\n",
      "[Epoch 5/8 | Step 330/500] - Loss: 1.1159\n",
      "[Epoch 5/8 | Step 340/500] - Loss: 1.1076\n",
      "[Epoch 5/8 | Step 350/500] - Loss: 1.1052\n",
      "[Epoch 5/8 | Step 360/500] - Loss: 1.1054\n",
      "[Epoch 5/8 | Step 370/500] - Loss: 1.1104\n",
      "[Epoch 5/8 | Step 380/500] - Loss: 1.1042\n",
      "[Epoch 5/8 | Step 390/500] - Loss: 1.1110\n",
      "[Epoch 5/8 | Step 400/500] - Loss: 1.1124\n",
      "[Epoch 5/8 | Step 410/500] - Loss: 1.1092\n",
      "[Epoch 5/8 | Step 420/500] - Loss: 1.1081\n",
      "[Epoch 5/8 | Step 430/500] - Loss: 1.1012\n",
      "[Epoch 5/8 | Step 440/500] - Loss: 1.0989\n",
      "[Epoch 5/8 | Step 450/500] - Loss: 1.1044\n",
      "[Epoch 5/8 | Step 460/500] - Loss: 1.0967\n",
      "[Epoch 5/8 | Step 470/500] - Loss: 1.1001\n",
      "[Epoch 5/8 | Step 480/500] - Loss: 1.0988\n",
      "[Epoch 5/8 | Step 490/500] - Loss: 1.0949\n",
      "Epoch 5/8 - Avg Train Loss: 1.0946, Val Loss: 1.2803\n",
      "[Epoch 6/8 | Step 0/500] - Loss: 0.0865\n",
      "[Epoch 6/8 | Step 10/500] - Loss: 1.3939\n",
      "[Epoch 6/8 | Step 20/500] - Loss: 1.3254\n",
      "[Epoch 6/8 | Step 30/500] - Loss: 1.1487\n",
      "[Epoch 6/8 | Step 40/500] - Loss: 1.1317\n",
      "[Epoch 6/8 | Step 50/500] - Loss: 0.9787\n",
      "[Epoch 6/8 | Step 60/500] - Loss: 1.0578\n",
      "[Epoch 6/8 | Step 70/500] - Loss: 1.0791\n",
      "[Epoch 6/8 | Step 80/500] - Loss: 1.1423\n",
      "[Epoch 6/8 | Step 90/500] - Loss: 1.1948\n",
      "[Epoch 6/8 | Step 100/500] - Loss: 1.2393\n",
      "[Epoch 6/8 | Step 110/500] - Loss: 1.2086\n",
      "[Epoch 6/8 | Step 120/500] - Loss: 1.2482\n",
      "[Epoch 6/8 | Step 130/500] - Loss: 1.2022\n",
      "[Epoch 6/8 | Step 140/500] - Loss: 1.1750\n",
      "[Epoch 6/8 | Step 150/500] - Loss: 1.1643\n",
      "[Epoch 6/8 | Step 160/500] - Loss: 1.1321\n",
      "[Epoch 6/8 | Step 170/500] - Loss: 1.0972\n",
      "[Epoch 6/8 | Step 180/500] - Loss: 1.0617\n",
      "[Epoch 6/8 | Step 190/500] - Loss: 1.0473\n",
      "[Epoch 6/8 | Step 200/500] - Loss: 1.0131\n",
      "[Epoch 6/8 | Step 210/500] - Loss: 1.0163\n",
      "[Epoch 6/8 | Step 220/500] - Loss: 1.0221\n",
      "[Epoch 6/8 | Step 230/500] - Loss: 1.0130\n",
      "[Epoch 6/8 | Step 240/500] - Loss: 1.0067\n",
      "[Epoch 6/8 | Step 250/500] - Loss: 0.9867\n",
      "[Epoch 6/8 | Step 260/500] - Loss: 0.9757\n",
      "[Epoch 6/8 | Step 270/500] - Loss: 0.9994\n",
      "[Epoch 6/8 | Step 280/500] - Loss: 1.0062\n",
      "[Epoch 6/8 | Step 290/500] - Loss: 1.0100\n",
      "[Epoch 6/8 | Step 300/500] - Loss: 1.0070\n",
      "[Epoch 6/8 | Step 310/500] - Loss: 1.0122\n",
      "[Epoch 6/8 | Step 320/500] - Loss: 1.0168\n",
      "[Epoch 6/8 | Step 330/500] - Loss: 1.0178\n",
      "[Epoch 6/8 | Step 340/500] - Loss: 1.0290\n",
      "[Epoch 6/8 | Step 350/500] - Loss: 1.0322\n",
      "[Epoch 6/8 | Step 360/500] - Loss: 1.0261\n",
      "[Epoch 6/8 | Step 370/500] - Loss: 1.0372\n",
      "[Epoch 6/8 | Step 380/500] - Loss: 1.0194\n",
      "[Epoch 6/8 | Step 390/500] - Loss: 1.0179\n",
      "[Epoch 6/8 | Step 400/500] - Loss: 1.0210\n",
      "[Epoch 6/8 | Step 410/500] - Loss: 1.0175\n",
      "[Epoch 6/8 | Step 420/500] - Loss: 1.0211\n",
      "[Epoch 6/8 | Step 430/500] - Loss: 1.0279\n",
      "[Epoch 6/8 | Step 440/500] - Loss: 1.0196\n",
      "[Epoch 6/8 | Step 450/500] - Loss: 1.0158\n",
      "[Epoch 6/8 | Step 460/500] - Loss: 1.0121\n",
      "[Epoch 6/8 | Step 470/500] - Loss: 1.0123\n",
      "[Epoch 6/8 | Step 480/500] - Loss: 1.0054\n",
      "[Epoch 6/8 | Step 490/500] - Loss: 0.9976\n",
      "Epoch 6/8 - Avg Train Loss: 0.9973, Val Loss: 1.2154\n",
      "[Epoch 7/8 | Step 0/500] - Loss: 1.7418\n",
      "[Epoch 7/8 | Step 10/500] - Loss: 1.1822\n",
      "[Epoch 7/8 | Step 20/500] - Loss: 1.0478\n",
      "[Epoch 7/8 | Step 30/500] - Loss: 0.9002\n",
      "[Epoch 7/8 | Step 40/500] - Loss: 0.8150\n",
      "[Epoch 7/8 | Step 50/500] - Loss: 0.8925\n",
      "[Epoch 7/8 | Step 60/500] - Loss: 0.8774\n",
      "[Epoch 7/8 | Step 70/500] - Loss: 0.8953\n",
      "[Epoch 7/8 | Step 80/500] - Loss: 0.9005\n",
      "[Epoch 7/8 | Step 90/500] - Loss: 0.8712\n",
      "[Epoch 7/8 | Step 100/500] - Loss: 0.8459\n",
      "[Epoch 7/8 | Step 110/500] - Loss: 0.8915\n",
      "[Epoch 7/8 | Step 120/500] - Loss: 0.9284\n",
      "[Epoch 7/8 | Step 130/500] - Loss: 0.9520\n",
      "[Epoch 7/8 | Step 140/500] - Loss: 0.9703\n",
      "[Epoch 7/8 | Step 150/500] - Loss: 0.9273\n",
      "[Epoch 7/8 | Step 160/500] - Loss: 0.9272\n",
      "[Epoch 7/8 | Step 170/500] - Loss: 0.9093\n",
      "[Epoch 7/8 | Step 180/500] - Loss: 0.8879\n",
      "[Epoch 7/8 | Step 190/500] - Loss: 0.8661\n",
      "[Epoch 7/8 | Step 200/500] - Loss: 0.8812\n",
      "[Epoch 7/8 | Step 210/500] - Loss: 0.8723\n",
      "[Epoch 7/8 | Step 220/500] - Loss: 0.8870\n",
      "[Epoch 7/8 | Step 230/500] - Loss: 0.8969\n",
      "[Epoch 7/8 | Step 240/500] - Loss: 0.8972\n",
      "[Epoch 7/8 | Step 250/500] - Loss: 0.9118\n",
      "[Epoch 7/8 | Step 260/500] - Loss: 0.9037\n",
      "[Epoch 7/8 | Step 270/500] - Loss: 0.8871\n",
      "[Epoch 7/8 | Step 280/500] - Loss: 0.8724\n",
      "[Epoch 7/8 | Step 290/500] - Loss: 0.8887\n",
      "[Epoch 7/8 | Step 300/500] - Loss: 0.9132\n",
      "[Epoch 7/8 | Step 310/500] - Loss: 0.9199\n",
      "[Epoch 7/8 | Step 320/500] - Loss: 0.9189\n",
      "[Epoch 7/8 | Step 330/500] - Loss: 0.9267\n",
      "[Epoch 7/8 | Step 340/500] - Loss: 0.9258\n",
      "[Epoch 7/8 | Step 350/500] - Loss: 0.9309\n",
      "[Epoch 7/8 | Step 360/500] - Loss: 0.9275\n",
      "[Epoch 7/8 | Step 370/500] - Loss: 0.9373\n",
      "[Epoch 7/8 | Step 380/500] - Loss: 0.9432\n",
      "[Epoch 7/8 | Step 390/500] - Loss: 0.9545\n",
      "[Epoch 7/8 | Step 400/500] - Loss: 0.9500\n",
      "[Epoch 7/8 | Step 410/500] - Loss: 0.9596\n",
      "[Epoch 7/8 | Step 420/500] - Loss: 0.9557\n",
      "[Epoch 7/8 | Step 430/500] - Loss: 0.9482\n",
      "[Epoch 7/8 | Step 440/500] - Loss: 0.9498\n",
      "[Epoch 7/8 | Step 450/500] - Loss: 0.9469\n",
      "[Epoch 7/8 | Step 460/500] - Loss: 0.9405\n",
      "[Epoch 7/8 | Step 470/500] - Loss: 0.9293\n",
      "[Epoch 7/8 | Step 480/500] - Loss: 0.9334\n",
      "[Epoch 7/8 | Step 490/500] - Loss: 0.9351\n",
      "Epoch 7/8 - Avg Train Loss: 0.9332, Val Loss: 1.1768\n",
      "[Epoch 8/8 | Step 0/500] - Loss: 0.4702\n",
      "[Epoch 8/8 | Step 10/500] - Loss: 0.5485\n",
      "[Epoch 8/8 | Step 20/500] - Loss: 0.7064\n",
      "[Epoch 8/8 | Step 30/500] - Loss: 0.9485\n",
      "[Epoch 8/8 | Step 40/500] - Loss: 1.1412\n",
      "[Epoch 8/8 | Step 50/500] - Loss: 1.0970\n",
      "[Epoch 8/8 | Step 60/500] - Loss: 1.1475\n",
      "[Epoch 8/8 | Step 70/500] - Loss: 1.0256\n",
      "[Epoch 8/8 | Step 80/500] - Loss: 1.0066\n",
      "[Epoch 8/8 | Step 90/500] - Loss: 0.9691\n",
      "[Epoch 8/8 | Step 100/500] - Loss: 0.9928\n",
      "[Epoch 8/8 | Step 110/500] - Loss: 0.9638\n",
      "[Epoch 8/8 | Step 120/500] - Loss: 0.9306\n",
      "[Epoch 8/8 | Step 130/500] - Loss: 0.9134\n",
      "[Epoch 8/8 | Step 140/500] - Loss: 0.9561\n",
      "[Epoch 8/8 | Step 150/500] - Loss: 0.9700\n",
      "[Epoch 8/8 | Step 160/500] - Loss: 0.9369\n",
      "[Epoch 8/8 | Step 170/500] - Loss: 1.0002\n",
      "[Epoch 8/8 | Step 180/500] - Loss: 0.9876\n",
      "[Epoch 8/8 | Step 190/500] - Loss: 0.9670\n",
      "[Epoch 8/8 | Step 200/500] - Loss: 0.9799\n",
      "[Epoch 8/8 | Step 210/500] - Loss: 0.9611\n",
      "[Epoch 8/8 | Step 220/500] - Loss: 0.9584\n",
      "[Epoch 8/8 | Step 230/500] - Loss: 0.9690\n",
      "[Epoch 8/8 | Step 240/500] - Loss: 0.9475\n",
      "[Epoch 8/8 | Step 250/500] - Loss: 0.9485\n",
      "[Epoch 8/8 | Step 260/500] - Loss: 0.9289\n",
      "[Epoch 8/8 | Step 270/500] - Loss: 0.9275\n",
      "[Epoch 8/8 | Step 280/500] - Loss: 0.9042\n",
      "[Epoch 8/8 | Step 290/500] - Loss: 0.8930\n",
      "[Epoch 8/8 | Step 300/500] - Loss: 0.8798\n",
      "[Epoch 8/8 | Step 310/500] - Loss: 0.9048\n",
      "[Epoch 8/8 | Step 320/500] - Loss: 0.9018\n",
      "[Epoch 8/8 | Step 330/500] - Loss: 0.8934\n",
      "[Epoch 8/8 | Step 340/500] - Loss: 0.8797\n",
      "[Epoch 8/8 | Step 350/500] - Loss: 0.9015\n",
      "[Epoch 8/8 | Step 360/500] - Loss: 0.8951\n",
      "[Epoch 8/8 | Step 370/500] - Loss: 0.8924\n",
      "[Epoch 8/8 | Step 380/500] - Loss: 0.8922\n",
      "[Epoch 8/8 | Step 390/500] - Loss: 0.8983\n",
      "[Epoch 8/8 | Step 400/500] - Loss: 0.9138\n",
      "[Epoch 8/8 | Step 410/500] - Loss: 0.8979\n",
      "[Epoch 8/8 | Step 420/500] - Loss: 0.9159\n",
      "[Epoch 8/8 | Step 430/500] - Loss: 0.9101\n",
      "[Epoch 8/8 | Step 440/500] - Loss: 0.8996\n",
      "[Epoch 8/8 | Step 450/500] - Loss: 0.9026\n",
      "[Epoch 8/8 | Step 460/500] - Loss: 0.9089\n",
      "[Epoch 8/8 | Step 470/500] - Loss: 0.9075\n",
      "[Epoch 8/8 | Step 480/500] - Loss: 0.9120\n",
      "[Epoch 8/8 | Step 490/500] - Loss: 0.9020\n",
      "Epoch 8/8 - Avg Train Loss: 0.9001, Val Loss: 1.1638\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import bitsandbytes as bnb\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Set up the GPU environment and return the appropriate device.\"\"\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "  \n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    return device\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "    model_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model_config.use_cache = False\n",
    "\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        config=model_config, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"SEQ_CLS\")\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "class PreferenceEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Dataset to create pairs of message, preferred response, and rejected response for DPO training.\n",
    "        \"\"\"\n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pairs = self._create_preference_pairs()\n",
    "\n",
    "    def _create_preference_pairs(self):\n",
    "        \"\"\"\n",
    "        Create pairs using emails from the dataset based on their labels.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for _, selected_email in self.emails_df.iterrows():\n",
    "            selected_label = selected_email['label']\n",
    "            ham_emails = self.emails_df[self.emails_df['label'] == 0]\n",
    "            phish_emails = self.emails_df[self.emails_df['label'] == 1]\n",
    "\n",
    "            if selected_label == 1:  # Phishing email\n",
    "                preferred_email = phish_emails[phish_emails.index != selected_email.name].sample(n=1).iloc[0]\n",
    "                rejected_email = ham_emails.sample(n=1).iloc[0]\n",
    "                pairs.append({\n",
    "                    'message': selected_email,\n",
    "                    'preferred': preferred_email,\n",
    "                    'rejected': rejected_email\n",
    "                })\n",
    "            elif selected_label == 0:  # Ham email\n",
    "                preferred_email = ham_emails[ham_emails.index != selected_email.name].sample(n=1).iloc[0]\n",
    "                rejected_email = phish_emails.sample(n=1).iloc[0]\n",
    "                pairs.append({\n",
    "                    'message': selected_email,\n",
    "                    'preferred': preferred_email,\n",
    "                    'rejected': rejected_email\n",
    "                })\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def _prepare_email_input(self, message, response):\n",
    "        \"\"\"\n",
    "        Prepare the input text with formatted message and response for tokenization.\n",
    "        \"\"\"\n",
    "        formatted_input = f\"<s>[INST] {message} [/INST] {response}</s>\"\n",
    "        return self.tokenizer(\n",
    "            formatted_input,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        \n",
    "        if pair['message']['label'] == 1:\n",
    "            message_text = (\n",
    "                \"This email is flagged as a phishing email. \"\n",
    "                \"Carefully examine the sender's address, subject line, and content of the email. \"\n",
    "                f\"Sender: {pair['message']['sender']} [SEP] \"\n",
    "                f\"Subject: {pair['message']['subject']} [SEP] \"\n",
    "                f\"Body: {pair['message']['body']}\"\n",
    "            )\n",
    "        else:\n",
    "            message_text = (\n",
    "                \"This email is flagged as a legitimate email. \"\n",
    "                \"Look for consistent and clear sender details, subject relevance, and authentic body content. \"\n",
    "                f\"Sender: {pair['message']['sender']} [SEP] \"\n",
    "                f\"Subject: {pair['message']['subject']} [SEP] \"\n",
    "                f\"Body: {pair['message']['body']}\"\n",
    "            )\n",
    "        \n",
    "        \n",
    "        preferred_response = (\n",
    "            \"This is a similar email example to the one above. \"\n",
    "            f\"Sender: {pair['preferred']['sender']} [SEP] \"\n",
    "            f\"Subject: {pair['preferred']['subject']} [SEP] \"\n",
    "            f\"Body: {pair['preferred']['body']}\"\n",
    "        )\n",
    "        rejected_response = (\n",
    "            \"This email is different in intent. Notice the sender's address, subject, and content mismatch. \"\n",
    "            f\"Sender: {pair['rejected']['sender']} [SEP] \"\n",
    "            f\"Subject: {pair['rejected']['subject']} [SEP] \"\n",
    "            f\"Body: {pair['rejected']['body']}\"\n",
    "        )\n",
    "        \n",
    "        message_inputs = self._prepare_email_input(message_text, \"\")\n",
    "        preferred_inputs = self._prepare_email_input(message_text, preferred_response)\n",
    "        rejected_inputs = self._prepare_email_input(message_text, rejected_response)\n",
    "\n",
    "        return {\n",
    "            'message_input_ids': message_inputs['input_ids'].squeeze(),\n",
    "            'message_attention_mask': message_inputs['attention_mask'].squeeze(),\n",
    "            'preferred_input_ids': preferred_inputs['input_ids'].squeeze(),\n",
    "            'preferred_attention_mask': preferred_inputs['attention_mask'].squeeze(),\n",
    "            'rejected_input_ids': rejected_inputs['input_ids'].squeeze(),\n",
    "            'rejected_attention_mask': rejected_inputs['attention_mask'].squeeze(),\n",
    "        }\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "   \n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    #text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    #text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def compute_dpo_loss(policy_chosen_logits, policy_rejected_logits, \n",
    "                    reference_chosen_logits, reference_rejected_logits, \n",
    "                    beta=0.2):\n",
    "   \n",
    "    epsilon = 1e-8\n",
    "    \n",
    "   \n",
    "    policy_chosen_probs = F.softmax(policy_chosen_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    policy_rejected_probs = F.softmax(policy_rejected_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    ref_chosen_probs = F.softmax(reference_chosen_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    ref_rejected_probs = F.softmax(reference_rejected_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    \n",
    "  \n",
    "    chosen_rewards = (torch.log(policy_chosen_probs + epsilon) - \n",
    "                     torch.log(ref_chosen_probs + epsilon))\n",
    "    rejected_rewards = (torch.log(policy_rejected_probs + epsilon) - \n",
    "                       torch.log(ref_rejected_probs + epsilon))\n",
    "    \n",
    "    \n",
    "    max_reward = 50.0\n",
    "    chosen_rewards = torch.clamp(chosen_rewards, -max_reward, max_reward)\n",
    "    rejected_rewards = torch.clamp(rejected_rewards, -max_reward, max_reward)\n",
    "    \n",
    "    \n",
    "    logits_diff = (chosen_rewards - rejected_rewards) / beta\n",
    "    \n",
    "    valid_mask = ~torch.isnan(logits_diff)\n",
    "    if valid_mask.any():\n",
    "        loss = -F.logsigmoid(logits_diff[valid_mask]).mean()\n",
    "    else:\n",
    "        loss = torch.tensor(0.0, device=logits_diff.device)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_model_dpo(policy_model, reference_model, train_loader, val_loader, \n",
    "                   optimizer, scheduler, device, num_epochs=5, beta=0.2, gradient_accumulation_steps=2):\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    policy_model = policy_model.to(device).float()\n",
    "    reference_model = reference_model.to(device).float()\n",
    "    reference_model.eval()  # Ensure reference model does not get updated during training\n",
    "    \n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        policy_model.train()\n",
    "        total_loss = 0\n",
    "        valid_steps = 0\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            try:\n",
    "                with torch.amp.autocast('cuda', dtype=torch.float32):\n",
    "                    policy_chosen_outputs = policy_model(\n",
    "                        input_ids=batch['preferred_input_ids'],\n",
    "                        attention_mask=batch['preferred_attention_mask']\n",
    "                    )\n",
    "                    policy_rejected_outputs = policy_model(\n",
    "                        input_ids=batch['rejected_input_ids'],\n",
    "                        attention_mask=batch['rejected_attention_mask']\n",
    "                    )\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        ref_chosen_outputs = reference_model(\n",
    "                            input_ids=batch['preferred_input_ids'],\n",
    "                            attention_mask=batch['preferred_attention_mask']\n",
    "                        )\n",
    "                        ref_rejected_outputs = reference_model(\n",
    "                            input_ids=batch['rejected_input_ids'],\n",
    "                            attention_mask=batch['rejected_attention_mask']\n",
    "                        )\n",
    "                    \n",
    "                    loss = compute_dpo_loss(\n",
    "                        policy_chosen_outputs.logits,\n",
    "                        policy_rejected_outputs.logits,\n",
    "                        ref_chosen_outputs.logits,\n",
    "                        ref_rejected_outputs.logits,\n",
    "                        beta=beta\n",
    "                    )\n",
    "                    \n",
    "                    if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                        scaler.scale(loss).backward()\n",
    "                        \n",
    "                        # Gradient accumulation logic\n",
    "                        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                            scaler.unscale_(optimizer)\n",
    "                            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                            scheduler.step()\n",
    "                            optimizer.zero_grad()\n",
    "                        \n",
    "                        total_loss += loss.item()\n",
    "                        valid_steps += 1\n",
    "                    \n",
    "                    if step % 10 == 0:\n",
    "                        avg_loss = total_loss / max(valid_steps, 1)\n",
    "                        print(f\"[Epoch {epoch+1}/{num_epochs} | Step {step}/{len(train_loader)}] - Loss: {avg_loss:.4f}\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {step}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        if valid_steps > 0:\n",
    "            avg_train_loss = total_loss / valid_steps\n",
    "            val_loss = evaluate_model_dpo(policy_model, reference_model, val_loader, device, beta)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Avg Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = {k: v.cpu() for k, v in policy_model.state_dict().items() if isinstance(v, torch.Tensor)}\n",
    "    \n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model_dpo(policy_model, reference_model, val_loader, device, beta):\n",
    "   \n",
    "    policy_model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.amp.autocast('cuda', dtype=torch.float32):\n",
    "                policy_chosen_outputs = policy_model(\n",
    "                    input_ids=batch['preferred_input_ids'],\n",
    "                    attention_mask=batch['preferred_attention_mask']\n",
    "                )\n",
    "                policy_rejected_outputs = policy_model(\n",
    "                    input_ids=batch['rejected_input_ids'],\n",
    "                    attention_mask=batch['rejected_attention_mask']\n",
    "                )\n",
    "                \n",
    "                ref_chosen_outputs = reference_model(\n",
    "                    input_ids=batch['preferred_input_ids'],\n",
    "                    attention_mask=batch['preferred_attention_mask']\n",
    "                )\n",
    "                ref_rejected_outputs = reference_model(\n",
    "                    input_ids=batch['rejected_input_ids'],\n",
    "                    attention_mask=batch['rejected_attention_mask']\n",
    "                )\n",
    "                \n",
    "                loss = compute_dpo_loss(\n",
    "                    policy_chosen_outputs.logits,\n",
    "                    policy_rejected_outputs.logits,\n",
    "                    ref_chosen_outputs.logits,\n",
    "                    ref_rejected_outputs.logits,\n",
    "                    beta=beta\n",
    "                )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "def main():\n",
    "\n",
    "  \n",
    "    device = setup_environment()\n",
    "    model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/final_data.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    policy_model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    reference_model, _ = setup_model_and_tokenizer(model_name, device)\n",
    "    \n",
    "\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "  \n",
    "    train_dataset = PreferenceEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = PreferenceEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # Setup optimization\n",
    "    optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "    num_epochs = 8\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    best_model_state = train_model_dpo(\n",
    "        policy_model,\n",
    "        reference_model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        num_epochs=num_epochs,\n",
    "        beta=0.2\n",
    "    )\n",
    "\n",
    "   \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/llama_8b_dpo123_classification_model\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    policy_model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"beta\": 0.2\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31bd33f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/8 | Step 0/500] - Loss: 0.8203\n",
      "[Epoch 1/8 | Step 10/500] - Loss: 0.7087\n",
      "[Epoch 1/8 | Step 20/500] - Loss: 0.7226\n",
      "[Epoch 1/8 | Step 30/500] - Loss: 0.7019\n",
      "[Epoch 1/8 | Step 40/500] - Loss: 0.7047\n",
      "[Epoch 1/8 | Step 50/500] - Loss: 0.6942\n",
      "[Epoch 1/8 | Step 60/500] - Loss: 0.6977\n",
      "[Epoch 1/8 | Step 70/500] - Loss: 0.6917\n",
      "[Epoch 1/8 | Step 80/500] - Loss: 0.6937\n",
      "[Epoch 1/8 | Step 90/500] - Loss: 0.6947\n",
      "[Epoch 1/8 | Step 100/500] - Loss: 0.6892\n",
      "[Epoch 1/8 | Step 110/500] - Loss: 0.6869\n",
      "[Epoch 1/8 | Step 120/500] - Loss: 0.6846\n",
      "[Epoch 1/8 | Step 130/500] - Loss: 0.6834\n",
      "[Epoch 1/8 | Step 140/500] - Loss: 0.6790\n",
      "[Epoch 1/8 | Step 150/500] - Loss: 0.6726\n",
      "[Epoch 1/8 | Step 160/500] - Loss: 0.6572\n",
      "[Epoch 1/8 | Step 170/500] - Loss: 0.6463\n",
      "[Epoch 1/8 | Step 180/500] - Loss: 0.6308\n",
      "[Epoch 1/8 | Step 190/500] - Loss: 0.6082\n",
      "[Epoch 1/8 | Step 200/500] - Loss: 0.5822\n",
      "[Epoch 1/8 | Step 210/500] - Loss: 0.5622\n",
      "[Epoch 1/8 | Step 220/500] - Loss: 0.5386\n",
      "[Epoch 1/8 | Step 230/500] - Loss: 0.5172\n",
      "[Epoch 1/8 | Step 240/500] - Loss: 0.4995\n",
      "[Epoch 1/8 | Step 250/500] - Loss: 0.4834\n",
      "[Epoch 1/8 | Step 260/500] - Loss: 0.4685\n",
      "[Epoch 1/8 | Step 270/500] - Loss: 0.4538\n",
      "[Epoch 1/8 | Step 280/500] - Loss: 0.4387\n",
      "[Epoch 1/8 | Step 290/500] - Loss: 0.4241\n",
      "[Epoch 1/8 | Step 300/500] - Loss: 0.4131\n",
      "[Epoch 1/8 | Step 310/500] - Loss: 0.4026\n",
      "[Epoch 1/8 | Step 320/500] - Loss: 0.3906\n",
      "[Epoch 1/8 | Step 330/500] - Loss: 0.3810\n",
      "[Epoch 1/8 | Step 340/500] - Loss: 0.3740\n",
      "[Epoch 1/8 | Step 350/500] - Loss: 0.3644\n",
      "[Epoch 1/8 | Step 360/500] - Loss: 0.3574\n",
      "[Epoch 1/8 | Step 370/500] - Loss: 0.3499\n",
      "[Epoch 1/8 | Step 380/500] - Loss: 0.3414\n",
      "[Epoch 1/8 | Step 390/500] - Loss: 0.3361\n",
      "[Epoch 1/8 | Step 400/500] - Loss: 0.3286\n",
      "[Epoch 1/8 | Step 410/500] - Loss: 0.3217\n",
      "[Epoch 1/8 | Step 420/500] - Loss: 0.3166\n",
      "[Epoch 1/8 | Step 430/500] - Loss: 0.3113\n",
      "[Epoch 1/8 | Step 440/500] - Loss: 0.3064\n",
      "[Epoch 1/8 | Step 450/500] - Loss: 0.3014\n",
      "[Epoch 1/8 | Step 460/500] - Loss: 0.2973\n",
      "[Epoch 1/8 | Step 470/500] - Loss: 0.2932\n",
      "[Epoch 1/8 | Step 480/500] - Loss: 0.2885\n",
      "[Epoch 1/8 | Step 490/500] - Loss: 0.2850\n",
      "Epoch 1/8 - Avg Train Loss: 0.2818, Val Loss: 0.0687\n",
      "[Epoch 2/8 | Step 0/500] - Loss: 0.1699\n",
      "[Epoch 2/8 | Step 10/500] - Loss: 0.1234\n",
      "[Epoch 2/8 | Step 20/500] - Loss: 0.1085\n",
      "[Epoch 2/8 | Step 30/500] - Loss: 0.1063\n",
      "[Epoch 2/8 | Step 40/500] - Loss: 0.1094\n",
      "[Epoch 2/8 | Step 50/500] - Loss: 0.0978\n",
      "[Epoch 2/8 | Step 60/500] - Loss: 0.0897\n",
      "[Epoch 2/8 | Step 70/500] - Loss: 0.0898\n",
      "[Epoch 2/8 | Step 80/500] - Loss: 0.0819\n",
      "[Epoch 2/8 | Step 90/500] - Loss: 0.0877\n",
      "[Epoch 2/8 | Step 100/500] - Loss: 0.0897\n",
      "[Epoch 2/8 | Step 110/500] - Loss: 0.0898\n",
      "[Epoch 2/8 | Step 120/500] - Loss: 0.0900\n",
      "[Epoch 2/8 | Step 130/500] - Loss: 0.0887\n",
      "[Epoch 2/8 | Step 140/500] - Loss: 0.0896\n",
      "[Epoch 2/8 | Step 150/500] - Loss: 0.0883\n",
      "[Epoch 2/8 | Step 160/500] - Loss: 0.0895\n",
      "[Epoch 2/8 | Step 170/500] - Loss: 0.0901\n",
      "[Epoch 2/8 | Step 180/500] - Loss: 0.0880\n",
      "[Epoch 2/8 | Step 190/500] - Loss: 0.0856\n",
      "[Epoch 2/8 | Step 200/500] - Loss: 0.0857\n",
      "[Epoch 2/8 | Step 210/500] - Loss: 0.0846\n",
      "[Epoch 2/8 | Step 220/500] - Loss: 0.0851\n",
      "[Epoch 2/8 | Step 230/500] - Loss: 0.0862\n",
      "[Epoch 2/8 | Step 240/500] - Loss: 0.0856\n",
      "[Epoch 2/8 | Step 250/500] - Loss: 0.0854\n",
      "[Epoch 2/8 | Step 260/500] - Loss: 0.0848\n",
      "[Epoch 2/8 | Step 270/500] - Loss: 0.0834\n",
      "[Epoch 2/8 | Step 280/500] - Loss: 0.0834\n",
      "[Epoch 2/8 | Step 290/500] - Loss: 0.0824\n",
      "[Epoch 2/8 | Step 300/500] - Loss: 0.0834\n",
      "[Epoch 2/8 | Step 310/500] - Loss: 0.0832\n",
      "[Epoch 2/8 | Step 320/500] - Loss: 0.0814\n",
      "[Epoch 2/8 | Step 330/500] - Loss: 0.0807\n",
      "[Epoch 2/8 | Step 340/500] - Loss: 0.0792\n",
      "[Epoch 2/8 | Step 350/500] - Loss: 0.0775\n",
      "[Epoch 2/8 | Step 360/500] - Loss: 0.0800\n",
      "[Epoch 2/8 | Step 370/500] - Loss: 0.0804\n",
      "[Epoch 2/8 | Step 380/500] - Loss: 0.0810\n",
      "[Epoch 2/8 | Step 390/500] - Loss: 0.0808\n",
      "[Epoch 2/8 | Step 400/500] - Loss: 0.0809\n",
      "[Epoch 2/8 | Step 410/500] - Loss: 0.0820\n",
      "[Epoch 2/8 | Step 420/500] - Loss: 0.0827\n",
      "[Epoch 2/8 | Step 430/500] - Loss: 0.0821\n",
      "[Epoch 2/8 | Step 440/500] - Loss: 0.0819\n",
      "[Epoch 2/8 | Step 450/500] - Loss: 0.0825\n",
      "[Epoch 2/8 | Step 460/500] - Loss: 0.0824\n",
      "[Epoch 2/8 | Step 470/500] - Loss: 0.0811\n",
      "[Epoch 2/8 | Step 480/500] - Loss: 0.0823\n",
      "[Epoch 2/8 | Step 490/500] - Loss: 0.0833\n",
      "Epoch 2/8 - Avg Train Loss: 0.0831, Val Loss: 0.0690\n",
      "[Epoch 3/8 | Step 0/500] - Loss: 0.3398\n",
      "[Epoch 3/8 | Step 10/500] - Loss: 0.1487\n",
      "[Epoch 3/8 | Step 20/500] - Loss: 0.1137\n",
      "[Epoch 3/8 | Step 30/500] - Loss: 0.1135\n",
      "[Epoch 3/8 | Step 40/500] - Loss: 0.1050\n",
      "[Epoch 3/8 | Step 50/500] - Loss: 0.1030\n",
      "[Epoch 3/8 | Step 60/500] - Loss: 0.1014\n",
      "[Epoch 3/8 | Step 70/500] - Loss: 0.0950\n",
      "[Epoch 3/8 | Step 80/500] - Loss: 0.0924\n",
      "[Epoch 3/8 | Step 90/500] - Loss: 0.0888\n",
      "[Epoch 3/8 | Step 100/500] - Loss: 0.0953\n",
      "[Epoch 3/8 | Step 110/500] - Loss: 0.0924\n",
      "[Epoch 3/8 | Step 120/500] - Loss: 0.0907\n",
      "[Epoch 3/8 | Step 130/500] - Loss: 0.0895\n",
      "[Epoch 3/8 | Step 140/500] - Loss: 0.0864\n",
      "[Epoch 3/8 | Step 150/500] - Loss: 0.0844\n",
      "[Epoch 3/8 | Step 160/500] - Loss: 0.0852\n",
      "[Epoch 3/8 | Step 170/500] - Loss: 0.0835\n",
      "[Epoch 3/8 | Step 180/500] - Loss: 0.0852\n",
      "[Epoch 3/8 | Step 190/500] - Loss: 0.0842\n",
      "[Epoch 3/8 | Step 200/500] - Loss: 0.0855\n",
      "[Epoch 3/8 | Step 210/500] - Loss: 0.0832\n",
      "[Epoch 3/8 | Step 220/500] - Loss: 0.0836\n",
      "[Epoch 3/8 | Step 230/500] - Loss: 0.0837\n",
      "[Epoch 3/8 | Step 240/500] - Loss: 0.0842\n",
      "[Epoch 3/8 | Step 250/500] - Loss: 0.0826\n",
      "[Epoch 3/8 | Step 260/500] - Loss: 0.0819\n",
      "[Epoch 3/8 | Step 270/500] - Loss: 0.0811\n",
      "[Epoch 3/8 | Step 280/500] - Loss: 0.0806\n",
      "[Epoch 3/8 | Step 290/500] - Loss: 0.0801\n",
      "[Epoch 3/8 | Step 300/500] - Loss: 0.0797\n",
      "[Epoch 3/8 | Step 310/500] - Loss: 0.0802\n",
      "[Epoch 3/8 | Step 320/500] - Loss: 0.0795\n",
      "[Epoch 3/8 | Step 330/500] - Loss: 0.0783\n",
      "[Epoch 3/8 | Step 340/500] - Loss: 0.0775\n",
      "[Epoch 3/8 | Step 350/500] - Loss: 0.0770\n",
      "[Epoch 3/8 | Step 360/500] - Loss: 0.0777\n",
      "[Epoch 3/8 | Step 370/500] - Loss: 0.0771\n",
      "[Epoch 3/8 | Step 380/500] - Loss: 0.0788\n",
      "[Epoch 3/8 | Step 390/500] - Loss: 0.0779\n",
      "[Epoch 3/8 | Step 400/500] - Loss: 0.0771\n",
      "[Epoch 3/8 | Step 410/500] - Loss: 0.0763\n",
      "[Epoch 3/8 | Step 420/500] - Loss: 0.0782\n",
      "[Epoch 3/8 | Step 430/500] - Loss: 0.0778\n",
      "[Epoch 3/8 | Step 440/500] - Loss: 0.0777\n",
      "[Epoch 3/8 | Step 450/500] - Loss: 0.0770\n",
      "[Epoch 3/8 | Step 460/500] - Loss: 0.0785\n",
      "[Epoch 3/8 | Step 470/500] - Loss: 0.0783\n",
      "[Epoch 3/8 | Step 480/500] - Loss: 0.0780\n",
      "[Epoch 3/8 | Step 490/500] - Loss: 0.0778\n",
      "Epoch 3/8 - Avg Train Loss: 0.0779, Val Loss: 0.0688\n",
      "[Epoch 4/8 | Step 0/500] - Loss: 0.0013\n",
      "[Epoch 4/8 | Step 10/500] - Loss: 0.0420\n",
      "[Epoch 4/8 | Step 20/500] - Loss: 0.0697\n",
      "[Epoch 4/8 | Step 30/500] - Loss: 0.0635\n",
      "[Epoch 4/8 | Step 40/500] - Loss: 0.0575\n",
      "[Epoch 4/8 | Step 50/500] - Loss: 0.0545\n",
      "[Epoch 4/8 | Step 60/500] - Loss: 0.0648\n",
      "[Epoch 4/8 | Step 70/500] - Loss: 0.0657\n",
      "[Epoch 4/8 | Step 80/500] - Loss: 0.0599\n",
      "[Epoch 4/8 | Step 90/500] - Loss: 0.0596\n",
      "[Epoch 4/8 | Step 100/500] - Loss: 0.0604\n",
      "[Epoch 4/8 | Step 110/500] - Loss: 0.0624\n",
      "[Epoch 4/8 | Step 120/500] - Loss: 0.0644\n",
      "[Epoch 4/8 | Step 130/500] - Loss: 0.0651\n",
      "[Epoch 4/8 | Step 140/500] - Loss: 0.0698\n",
      "[Epoch 4/8 | Step 150/500] - Loss: 0.0677\n",
      "[Epoch 4/8 | Step 160/500] - Loss: 0.0663\n",
      "[Epoch 4/8 | Step 170/500] - Loss: 0.0651\n",
      "[Epoch 4/8 | Step 180/500] - Loss: 0.0675\n",
      "[Epoch 4/8 | Step 190/500] - Loss: 0.0684\n",
      "[Epoch 4/8 | Step 200/500] - Loss: 0.0689\n",
      "[Epoch 4/8 | Step 210/500] - Loss: 0.0695\n",
      "[Epoch 4/8 | Step 220/500] - Loss: 0.0711\n",
      "[Epoch 4/8 | Step 230/500] - Loss: 0.0727\n",
      "[Epoch 4/8 | Step 240/500] - Loss: 0.0744\n",
      "[Epoch 4/8 | Step 250/500] - Loss: 0.0751\n",
      "[Epoch 4/8 | Step 260/500] - Loss: 0.0747\n",
      "[Epoch 4/8 | Step 270/500] - Loss: 0.0742\n",
      "[Epoch 4/8 | Step 280/500] - Loss: 0.0765\n",
      "[Epoch 4/8 | Step 290/500] - Loss: 0.0748\n",
      "[Epoch 4/8 | Step 300/500] - Loss: 0.0750\n",
      "[Epoch 4/8 | Step 310/500] - Loss: 0.0770\n",
      "[Epoch 4/8 | Step 320/500] - Loss: 0.0769\n",
      "[Epoch 4/8 | Step 330/500] - Loss: 0.0777\n",
      "[Epoch 4/8 | Step 340/500] - Loss: 0.0780\n",
      "[Epoch 4/8 | Step 350/500] - Loss: 0.0777\n",
      "[Epoch 4/8 | Step 360/500] - Loss: 0.0763\n",
      "[Epoch 4/8 | Step 370/500] - Loss: 0.0756\n",
      "[Epoch 4/8 | Step 380/500] - Loss: 0.0762\n",
      "[Epoch 4/8 | Step 390/500] - Loss: 0.0771\n",
      "[Epoch 4/8 | Step 400/500] - Loss: 0.0763\n",
      "[Epoch 4/8 | Step 410/500] - Loss: 0.0750\n",
      "[Epoch 4/8 | Step 420/500] - Loss: 0.0753\n",
      "[Epoch 4/8 | Step 430/500] - Loss: 0.0769\n",
      "[Epoch 4/8 | Step 440/500] - Loss: 0.0765\n",
      "[Epoch 4/8 | Step 450/500] - Loss: 0.0761\n",
      "[Epoch 4/8 | Step 460/500] - Loss: 0.0760\n",
      "[Epoch 4/8 | Step 470/500] - Loss: 0.0759\n",
      "[Epoch 4/8 | Step 480/500] - Loss: 0.0771\n",
      "[Epoch 4/8 | Step 490/500] - Loss: 0.0769\n",
      "Epoch 4/8 - Avg Train Loss: 0.0773, Val Loss: 0.0687\n",
      "[Epoch 5/8 | Step 0/500] - Loss: 0.0637\n",
      "[Epoch 5/8 | Step 10/500] - Loss: 0.0763\n",
      "[Epoch 5/8 | Step 20/500] - Loss: 0.0963\n",
      "[Epoch 5/8 | Step 30/500] - Loss: 0.0854\n",
      "[Epoch 5/8 | Step 40/500] - Loss: 0.0694\n",
      "[Epoch 5/8 | Step 50/500] - Loss: 0.0712\n",
      "[Epoch 5/8 | Step 60/500] - Loss: 0.0763\n",
      "[Epoch 5/8 | Step 70/500] - Loss: 0.0678\n",
      "[Epoch 5/8 | Step 80/500] - Loss: 0.0705\n",
      "[Epoch 5/8 | Step 90/500] - Loss: 0.0712\n",
      "[Epoch 5/8 | Step 100/500] - Loss: 0.0746\n",
      "[Epoch 5/8 | Step 110/500] - Loss: 0.0768\n",
      "[Epoch 5/8 | Step 120/500] - Loss: 0.0745\n",
      "[Epoch 5/8 | Step 130/500] - Loss: 0.0753\n",
      "[Epoch 5/8 | Step 140/500] - Loss: 0.0744\n",
      "[Epoch 5/8 | Step 150/500] - Loss: 0.0747\n",
      "[Epoch 5/8 | Step 160/500] - Loss: 0.0725\n",
      "[Epoch 5/8 | Step 170/500] - Loss: 0.0710\n",
      "[Epoch 5/8 | Step 180/500] - Loss: 0.0691\n",
      "[Epoch 5/8 | Step 190/500] - Loss: 0.0705\n",
      "[Epoch 5/8 | Step 200/500] - Loss: 0.0710\n",
      "[Epoch 5/8 | Step 210/500] - Loss: 0.0705\n",
      "[Epoch 5/8 | Step 220/500] - Loss: 0.0698\n",
      "[Epoch 5/8 | Step 230/500] - Loss: 0.0704\n",
      "[Epoch 5/8 | Step 240/500] - Loss: 0.0712\n",
      "[Epoch 5/8 | Step 250/500] - Loss: 0.0728\n",
      "[Epoch 5/8 | Step 260/500] - Loss: 0.0723\n",
      "[Epoch 5/8 | Step 270/500] - Loss: 0.0707\n",
      "[Epoch 5/8 | Step 280/500] - Loss: 0.0701\n",
      "[Epoch 5/8 | Step 290/500] - Loss: 0.0705\n",
      "[Epoch 5/8 | Step 300/500] - Loss: 0.0727\n",
      "[Epoch 5/8 | Step 310/500] - Loss: 0.0726\n",
      "[Epoch 5/8 | Step 320/500] - Loss: 0.0718\n",
      "[Epoch 5/8 | Step 330/500] - Loss: 0.0720\n",
      "[Epoch 5/8 | Step 340/500] - Loss: 0.0716\n",
      "[Epoch 5/8 | Step 350/500] - Loss: 0.0730\n",
      "[Epoch 5/8 | Step 360/500] - Loss: 0.0721\n",
      "[Epoch 5/8 | Step 370/500] - Loss: 0.0715\n",
      "[Epoch 5/8 | Step 380/500] - Loss: 0.0711\n",
      "[Epoch 5/8 | Step 390/500] - Loss: 0.0709\n",
      "[Epoch 5/8 | Step 400/500] - Loss: 0.0714\n",
      "[Epoch 5/8 | Step 410/500] - Loss: 0.0708\n",
      "[Epoch 5/8 | Step 420/500] - Loss: 0.0712\n",
      "[Epoch 5/8 | Step 430/500] - Loss: 0.0707\n",
      "[Epoch 5/8 | Step 440/500] - Loss: 0.0701\n",
      "[Epoch 5/8 | Step 450/500] - Loss: 0.0706\n",
      "[Epoch 5/8 | Step 460/500] - Loss: 0.0705\n",
      "[Epoch 5/8 | Step 470/500] - Loss: 0.0710\n",
      "[Epoch 5/8 | Step 480/500] - Loss: 0.0703\n",
      "[Epoch 5/8 | Step 490/500] - Loss: 0.0705\n",
      "Epoch 5/8 - Avg Train Loss: 0.0701, Val Loss: 0.0686\n",
      "[Epoch 6/8 | Step 0/500] - Loss: 0.0001\n",
      "[Epoch 6/8 | Step 10/500] - Loss: 0.1277\n",
      "[Epoch 6/8 | Step 20/500] - Loss: 0.1005\n",
      "[Epoch 6/8 | Step 30/500] - Loss: 0.0950\n",
      "[Epoch 6/8 | Step 40/500] - Loss: 0.0961\n",
      "[Epoch 6/8 | Step 50/500] - Loss: 0.0942\n",
      "[Epoch 6/8 | Step 60/500] - Loss: 0.0879\n",
      "[Epoch 6/8 | Step 70/500] - Loss: 0.0810\n",
      "[Epoch 6/8 | Step 80/500] - Loss: 0.0818\n",
      "[Epoch 6/8 | Step 90/500] - Loss: 0.0779\n",
      "[Epoch 6/8 | Step 100/500] - Loss: 0.0806\n",
      "[Epoch 6/8 | Step 110/500] - Loss: 0.0794\n",
      "[Epoch 6/8 | Step 120/500] - Loss: 0.0800\n",
      "[Epoch 6/8 | Step 130/500] - Loss: 0.0789\n",
      "[Epoch 6/8 | Step 140/500] - Loss: 0.0795\n",
      "[Epoch 6/8 | Step 150/500] - Loss: 0.0787\n",
      "[Epoch 6/8 | Step 160/500] - Loss: 0.0788\n",
      "[Epoch 6/8 | Step 170/500] - Loss: 0.0804\n",
      "[Epoch 6/8 | Step 180/500] - Loss: 0.0786\n",
      "[Epoch 6/8 | Step 190/500] - Loss: 0.0794\n",
      "[Epoch 6/8 | Step 200/500] - Loss: 0.0799\n",
      "[Epoch 6/8 | Step 210/500] - Loss: 0.0796\n",
      "[Epoch 6/8 | Step 220/500] - Loss: 0.0789\n",
      "[Epoch 6/8 | Step 230/500] - Loss: 0.0776\n",
      "[Epoch 6/8 | Step 240/500] - Loss: 0.0774\n",
      "[Epoch 6/8 | Step 250/500] - Loss: 0.0768\n",
      "[Epoch 6/8 | Step 260/500] - Loss: 0.0772\n",
      "[Epoch 6/8 | Step 270/500] - Loss: 0.0765\n",
      "[Epoch 6/8 | Step 280/500] - Loss: 0.0760\n",
      "[Epoch 6/8 | Step 290/500] - Loss: 0.0756\n",
      "[Epoch 6/8 | Step 300/500] - Loss: 0.0757\n",
      "[Epoch 6/8 | Step 310/500] - Loss: 0.0770\n",
      "[Epoch 6/8 | Step 320/500] - Loss: 0.0776\n",
      "[Epoch 6/8 | Step 330/500] - Loss: 0.0771\n",
      "[Epoch 6/8 | Step 340/500] - Loss: 0.0771\n",
      "[Epoch 6/8 | Step 350/500] - Loss: 0.0770\n",
      "[Epoch 6/8 | Step 360/500] - Loss: 0.0769\n",
      "[Epoch 6/8 | Step 370/500] - Loss: 0.0766\n",
      "[Epoch 6/8 | Step 380/500] - Loss: 0.0766\n",
      "[Epoch 6/8 | Step 390/500] - Loss: 0.0762\n",
      "[Epoch 6/8 | Step 400/500] - Loss: 0.0767\n",
      "[Epoch 6/8 | Step 410/500] - Loss: 0.0761\n",
      "[Epoch 6/8 | Step 420/500] - Loss: 0.0748\n",
      "[Epoch 6/8 | Step 430/500] - Loss: 0.0744\n",
      "[Epoch 6/8 | Step 440/500] - Loss: 0.0744\n",
      "[Epoch 6/8 | Step 450/500] - Loss: 0.0748\n",
      "[Epoch 6/8 | Step 460/500] - Loss: 0.0739\n",
      "[Epoch 6/8 | Step 470/500] - Loss: 0.0733\n",
      "[Epoch 6/8 | Step 480/500] - Loss: 0.0736\n",
      "[Epoch 6/8 | Step 490/500] - Loss: 0.0728\n",
      "Epoch 6/8 - Avg Train Loss: 0.0720, Val Loss: 0.0686\n",
      "[Epoch 7/8 | Step 0/500] - Loss: 0.0889\n",
      "[Epoch 7/8 | Step 10/500] - Loss: 0.0862\n",
      "[Epoch 7/8 | Step 20/500] - Loss: 0.0628\n",
      "[Epoch 7/8 | Step 30/500] - Loss: 0.0718\n",
      "[Epoch 7/8 | Step 40/500] - Loss: 0.0643\n",
      "[Epoch 7/8 | Step 50/500] - Loss: 0.0617\n",
      "[Epoch 7/8 | Step 60/500] - Loss: 0.0664\n",
      "[Epoch 7/8 | Step 70/500] - Loss: 0.0699\n",
      "[Epoch 7/8 | Step 80/500] - Loss: 0.0684\n",
      "[Epoch 7/8 | Step 90/500] - Loss: 0.0697\n",
      "[Epoch 7/8 | Step 100/500] - Loss: 0.0667\n",
      "[Epoch 7/8 | Step 110/500] - Loss: 0.0656\n",
      "[Epoch 7/8 | Step 120/500] - Loss: 0.0649\n",
      "[Epoch 7/8 | Step 130/500] - Loss: 0.0651\n",
      "[Epoch 7/8 | Step 140/500] - Loss: 0.0692\n",
      "[Epoch 7/8 | Step 150/500] - Loss: 0.0671\n",
      "[Epoch 7/8 | Step 160/500] - Loss: 0.0682\n",
      "[Epoch 7/8 | Step 170/500] - Loss: 0.0683\n",
      "[Epoch 7/8 | Step 180/500] - Loss: 0.0707\n",
      "[Epoch 7/8 | Step 190/500] - Loss: 0.0710\n",
      "[Epoch 7/8 | Step 200/500] - Loss: 0.0712\n",
      "[Epoch 7/8 | Step 210/500] - Loss: 0.0721\n",
      "[Epoch 7/8 | Step 220/500] - Loss: 0.0713\n",
      "[Epoch 7/8 | Step 230/500] - Loss: 0.0718\n",
      "[Epoch 7/8 | Step 240/500] - Loss: 0.0721\n",
      "[Epoch 7/8 | Step 250/500] - Loss: 0.0717\n",
      "[Epoch 7/8 | Step 260/500] - Loss: 0.0723\n",
      "[Epoch 7/8 | Step 270/500] - Loss: 0.0718\n",
      "[Epoch 7/8 | Step 280/500] - Loss: 0.0723\n",
      "[Epoch 7/8 | Step 290/500] - Loss: 0.0721\n",
      "[Epoch 7/8 | Step 300/500] - Loss: 0.0725\n",
      "[Epoch 7/8 | Step 310/500] - Loss: 0.0724\n",
      "[Epoch 7/8 | Step 320/500] - Loss: 0.0722\n",
      "[Epoch 7/8 | Step 330/500] - Loss: 0.0714\n",
      "[Epoch 7/8 | Step 340/500] - Loss: 0.0699\n",
      "[Epoch 7/8 | Step 350/500] - Loss: 0.0692\n",
      "[Epoch 7/8 | Step 360/500] - Loss: 0.0695\n",
      "[Epoch 7/8 | Step 370/500] - Loss: 0.0712\n",
      "[Epoch 7/8 | Step 380/500] - Loss: 0.0708\n",
      "[Epoch 7/8 | Step 390/500] - Loss: 0.0707\n",
      "[Epoch 7/8 | Step 400/500] - Loss: 0.0701\n",
      "[Epoch 7/8 | Step 410/500] - Loss: 0.0708\n",
      "[Epoch 7/8 | Step 420/500] - Loss: 0.0707\n",
      "[Epoch 7/8 | Step 430/500] - Loss: 0.0705\n",
      "[Epoch 7/8 | Step 440/500] - Loss: 0.0700\n",
      "[Epoch 7/8 | Step 450/500] - Loss: 0.0701\n",
      "[Epoch 7/8 | Step 460/500] - Loss: 0.0696\n",
      "[Epoch 7/8 | Step 470/500] - Loss: 0.0704\n",
      "[Epoch 7/8 | Step 480/500] - Loss: 0.0709\n",
      "[Epoch 7/8 | Step 490/500] - Loss: 0.0706\n",
      "Epoch 7/8 - Avg Train Loss: 0.0701, Val Loss: 0.0686\n",
      "[Epoch 8/8 | Step 0/500] - Loss: 0.0000\n",
      "[Epoch 8/8 | Step 10/500] - Loss: 0.0544\n",
      "[Epoch 8/8 | Step 20/500] - Loss: 0.0751\n",
      "[Epoch 8/8 | Step 30/500] - Loss: 0.0661\n",
      "[Epoch 8/8 | Step 40/500] - Loss: 0.0667\n",
      "[Epoch 8/8 | Step 50/500] - Loss: 0.0642\n",
      "[Epoch 8/8 | Step 60/500] - Loss: 0.0726\n",
      "[Epoch 8/8 | Step 70/500] - Loss: 0.0715\n",
      "[Epoch 8/8 | Step 80/500] - Loss: 0.0711\n",
      "[Epoch 8/8 | Step 90/500] - Loss: 0.0687\n",
      "[Epoch 8/8 | Step 100/500] - Loss: 0.0640\n",
      "[Epoch 8/8 | Step 110/500] - Loss: 0.0650\n",
      "[Epoch 8/8 | Step 120/500] - Loss: 0.0645\n",
      "[Epoch 8/8 | Step 130/500] - Loss: 0.0646\n",
      "[Epoch 8/8 | Step 140/500] - Loss: 0.0670\n",
      "[Epoch 8/8 | Step 150/500] - Loss: 0.0660\n",
      "[Epoch 8/8 | Step 160/500] - Loss: 0.0696\n",
      "[Epoch 8/8 | Step 170/500] - Loss: 0.0704\n",
      "[Epoch 8/8 | Step 180/500] - Loss: 0.0721\n",
      "[Epoch 8/8 | Step 190/500] - Loss: 0.0709\n",
      "[Epoch 8/8 | Step 200/500] - Loss: 0.0721\n",
      "[Epoch 8/8 | Step 210/500] - Loss: 0.0723\n",
      "[Epoch 8/8 | Step 220/500] - Loss: 0.0715\n",
      "[Epoch 8/8 | Step 230/500] - Loss: 0.0726\n",
      "[Epoch 8/8 | Step 240/500] - Loss: 0.0729\n",
      "[Epoch 8/8 | Step 250/500] - Loss: 0.0725\n",
      "[Epoch 8/8 | Step 260/500] - Loss: 0.0713\n",
      "[Epoch 8/8 | Step 270/500] - Loss: 0.0703\n",
      "[Epoch 8/8 | Step 280/500] - Loss: 0.0701\n",
      "[Epoch 8/8 | Step 290/500] - Loss: 0.0687\n",
      "[Epoch 8/8 | Step 300/500] - Loss: 0.0669\n",
      "[Epoch 8/8 | Step 310/500] - Loss: 0.0668\n",
      "[Epoch 8/8 | Step 320/500] - Loss: 0.0668\n",
      "[Epoch 8/8 | Step 330/500] - Loss: 0.0673\n",
      "[Epoch 8/8 | Step 340/500] - Loss: 0.0668\n",
      "[Epoch 8/8 | Step 350/500] - Loss: 0.0676\n",
      "[Epoch 8/8 | Step 360/500] - Loss: 0.0680\n",
      "[Epoch 8/8 | Step 370/500] - Loss: 0.0677\n",
      "[Epoch 8/8 | Step 380/500] - Loss: 0.0677\n",
      "[Epoch 8/8 | Step 390/500] - Loss: 0.0687\n",
      "[Epoch 8/8 | Step 400/500] - Loss: 0.0690\n",
      "[Epoch 8/8 | Step 410/500] - Loss: 0.0692\n",
      "[Epoch 8/8 | Step 420/500] - Loss: 0.0701\n",
      "[Epoch 8/8 | Step 430/500] - Loss: 0.0702\n",
      "[Epoch 8/8 | Step 440/500] - Loss: 0.0696\n",
      "[Epoch 8/8 | Step 450/500] - Loss: 0.0702\n",
      "[Epoch 8/8 | Step 460/500] - Loss: 0.0707\n",
      "[Epoch 8/8 | Step 470/500] - Loss: 0.0701\n",
      "[Epoch 8/8 | Step 480/500] - Loss: 0.0697\n",
      "[Epoch 8/8 | Step 490/500] - Loss: 0.0693\n",
      "Epoch 8/8 - Avg Train Loss: 0.0693, Val Loss: 0.0686\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    ")\n",
    "# Keep/Add these imports\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Set up the GPU environment and return the appropriate device.\"\"\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "  \n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    return device\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    \"\"\"Setup BERT model and tokenizer.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "class PreferenceEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Dataset to create pairs of message, preferred response, and rejected response for DPO training.\n",
    "        \"\"\"\n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pairs = self._create_preference_pairs()\n",
    "\n",
    "    def _create_preference_pairs(self):\n",
    "        \"\"\"\n",
    "        Create pairs using emails from the dataset based on their labels.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for _, selected_email in self.emails_df.iterrows():\n",
    "            selected_label = selected_email['label']\n",
    "            ham_emails = self.emails_df[self.emails_df['label'] == 0]\n",
    "            phish_emails = self.emails_df[self.emails_df['label'] == 1]\n",
    "\n",
    "            if selected_label == 1:  # Phishing email\n",
    "                preferred_email = phish_emails[phish_emails.index != selected_email.name].sample(n=1).iloc[0]\n",
    "                rejected_email = ham_emails.sample(n=1).iloc[0]\n",
    "                pairs.append({\n",
    "                    'message': selected_email,\n",
    "                    'preferred': preferred_email,\n",
    "                    'rejected': rejected_email\n",
    "                })\n",
    "            elif selected_label == 0:  # Ham email\n",
    "                preferred_email = ham_emails[ham_emails.index != selected_email.name].sample(n=1).iloc[0]\n",
    "                rejected_email = phish_emails.sample(n=1).iloc[0]\n",
    "                pairs.append({\n",
    "                    'message': selected_email,\n",
    "                    'preferred': preferred_email,\n",
    "                    'rejected': rejected_email\n",
    "                })\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def _prepare_email_input(self, message, response):\n",
    "        \"\"\"\n",
    "        Prepare the input text with formatted message and response for tokenization.\n",
    "        \"\"\"\n",
    "        formatted_input = f\"<s>[INST] {message} [/INST] {response}</s>\"\n",
    "        return self.tokenizer(\n",
    "            formatted_input,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        # Construct the prompt for phishing or ham detection\n",
    "        if pair['message']['label'] == 1:\n",
    "            message_text = (\n",
    "                \"This email is flagged as a phishing email. \"\n",
    "                \"Carefully examine the sender's address, subject line, and content of the email. \"\n",
    "                f\"Sender: {pair['message']['sender']} [SEP] \"\n",
    "                f\"Subject: {pair['message']['subject']} [SEP] \"\n",
    "                f\"Body: {pair['message']['body']}\"\n",
    "            )\n",
    "        else:\n",
    "            message_text = (\n",
    "                \"This email is flagged as a legitimate email. \"\n",
    "                \"Look for consistent and clear sender details, subject relevance, and authentic body content. \"\n",
    "                f\"Sender: {pair['message']['sender']} [SEP] \"\n",
    "                f\"Subject: {pair['message']['subject']} [SEP] \"\n",
    "                f\"Body: {pair['message']['body']}\"\n",
    "            )\n",
    "        \n",
    "        \n",
    "        preferred_response = (\n",
    "            \"This is a similar email example to the one above. \"\n",
    "            f\"Sender: {pair['preferred']['sender']} [SEP] \"\n",
    "            f\"Subject: {pair['preferred']['subject']} [SEP] \"\n",
    "            f\"Body: {pair['preferred']['body']}\"\n",
    "        )\n",
    "        rejected_response = (\n",
    "            \"This email is different in intent. Notice the sender's address, subject, and content mismatch. \"\n",
    "            f\"Sender: {pair['rejected']['sender']} [SEP] \"\n",
    "            f\"Subject: {pair['rejected']['subject']} [SEP] \"\n",
    "            f\"Body: {pair['rejected']['body']}\"\n",
    "        )\n",
    "        \n",
    "        message_inputs = self._prepare_email_input(message_text, \"\")\n",
    "        preferred_inputs = self._prepare_email_input(message_text, preferred_response)\n",
    "        rejected_inputs = self._prepare_email_input(message_text, rejected_response)\n",
    "\n",
    "        return {\n",
    "            'message_input_ids': message_inputs['input_ids'].squeeze(),\n",
    "            'message_attention_mask': message_inputs['attention_mask'].squeeze(),\n",
    "            'preferred_input_ids': preferred_inputs['input_ids'].squeeze(),\n",
    "            'preferred_attention_mask': preferred_inputs['attention_mask'].squeeze(),\n",
    "            'rejected_input_ids': rejected_inputs['input_ids'].squeeze(),\n",
    "            'rejected_attention_mask': rejected_inputs['attention_mask'].squeeze(),\n",
    "        }\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "   \n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    #text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    #text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def compute_dpo_loss(policy_chosen_logits, policy_rejected_logits, \n",
    "                    reference_chosen_logits, reference_rejected_logits, \n",
    "                    beta=0.2):\n",
    "   \n",
    "    epsilon = 1e-8\n",
    "    \n",
    "   \n",
    "    policy_chosen_probs = F.softmax(policy_chosen_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    policy_rejected_probs = F.softmax(policy_rejected_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    ref_chosen_probs = F.softmax(reference_chosen_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    ref_rejected_probs = F.softmax(reference_rejected_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    \n",
    "  \n",
    "    chosen_rewards = (torch.log(policy_chosen_probs + epsilon) - \n",
    "                     torch.log(ref_chosen_probs + epsilon))\n",
    "    rejected_rewards = (torch.log(policy_rejected_probs + epsilon) - \n",
    "                       torch.log(ref_rejected_probs + epsilon))\n",
    "    \n",
    "    \n",
    "    max_reward = 50.0\n",
    "    chosen_rewards = torch.clamp(chosen_rewards, -max_reward, max_reward)\n",
    "    rejected_rewards = torch.clamp(rejected_rewards, -max_reward, max_reward)\n",
    "    \n",
    "    \n",
    "    logits_diff = (chosen_rewards - rejected_rewards) / beta\n",
    "    \n",
    "    valid_mask = ~torch.isnan(logits_diff)\n",
    "    if valid_mask.any():\n",
    "        loss = -F.logsigmoid(logits_diff[valid_mask]).mean()\n",
    "    else:\n",
    "        loss = torch.tensor(0.0, device=logits_diff.device)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_model_dpo(policy_model, reference_model, train_loader, val_loader, \n",
    "                   optimizer, scheduler, device, num_epochs=5, beta=0.2, gradient_accumulation_steps=2):\n",
    "    \"\"\"\n",
    "    Train the model using DPO (Direct Preference Optimization)\n",
    "    \n",
    "    Args:\n",
    "        policy_model: The model being trained\n",
    "        reference_model: The reference model for DPO\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        optimizer: Optimizer for training\n",
    "        scheduler: Learning rate scheduler\n",
    "        device: Device to train on (cuda/cpu)\n",
    "        num_epochs: Number of training epochs\n",
    "        beta: Temperature parameter for DPO\n",
    "        gradient_accumulation_steps: Number of steps to accumulate gradients\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    policy_model = policy_model.to(device)\n",
    "    reference_model = reference_model.to(device)\n",
    "    reference_model.eval()\n",
    "    \n",
    "    # Initialize GradScaler for mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        policy_model.train()\n",
    "        total_loss = 0\n",
    "        valid_steps = 0\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            try:\n",
    "                if scaler is not None:\n",
    "                    # Mixed precision training path\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        # Forward pass\n",
    "                        policy_chosen_outputs = policy_model(\n",
    "                            input_ids=batch['preferred_input_ids'],\n",
    "                            attention_mask=batch['preferred_attention_mask']\n",
    "                        )\n",
    "                        policy_rejected_outputs = policy_model(\n",
    "                            input_ids=batch['rejected_input_ids'],\n",
    "                            attention_mask=batch['rejected_attention_mask']\n",
    "                        )\n",
    "                        \n",
    "                        with torch.no_grad():\n",
    "                            ref_chosen_outputs = reference_model(\n",
    "                                input_ids=batch['preferred_input_ids'],\n",
    "                                attention_mask=batch['preferred_attention_mask']\n",
    "                            )\n",
    "                            ref_rejected_outputs = reference_model(\n",
    "                                input_ids=batch['rejected_input_ids'],\n",
    "                                attention_mask=batch['rejected_attention_mask']\n",
    "                            )\n",
    "                        \n",
    "                        # Compute loss\n",
    "                        loss = compute_dpo_loss(\n",
    "                            policy_chosen_outputs.logits,\n",
    "                            policy_rejected_outputs.logits,\n",
    "                            ref_chosen_outputs.logits,\n",
    "                            ref_rejected_outputs.logits,\n",
    "                            beta=beta\n",
    "                        )\n",
    "                        \n",
    "                        if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                            loss = loss / gradient_accumulation_steps\n",
    "                            # Mixed precision backward pass\n",
    "                            scaler.scale(loss).backward()\n",
    "                            \n",
    "                            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                                scaler.unscale_(optimizer)\n",
    "                                torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)\n",
    "                                scaler.step(optimizer)\n",
    "                                scaler.update()\n",
    "                                scheduler.step()\n",
    "                                optimizer.zero_grad()\n",
    "                            \n",
    "                            total_loss += loss.item() * gradient_accumulation_steps\n",
    "                            valid_steps += 1\n",
    "                else:\n",
    "                    # Standard precision training path\n",
    "                    policy_chosen_outputs = policy_model(\n",
    "                        input_ids=batch['preferred_input_ids'],\n",
    "                        attention_mask=batch['preferred_attention_mask']\n",
    "                    )\n",
    "                    policy_rejected_outputs = policy_model(\n",
    "                        input_ids=batch['rejected_input_ids'],\n",
    "                        attention_mask=batch['rejected_attention_mask']\n",
    "                    )\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        ref_chosen_outputs = reference_model(\n",
    "                            input_ids=batch['preferred_input_ids'],\n",
    "                            attention_mask=batch['preferred_attention_mask']\n",
    "                        )\n",
    "                        ref_rejected_outputs = reference_model(\n",
    "                            input_ids=batch['rejected_input_ids'],\n",
    "                            attention_mask=batch['rejected_attention_mask']\n",
    "                        )\n",
    "                    \n",
    "                    loss = compute_dpo_loss(\n",
    "                        policy_chosen_outputs.logits,\n",
    "                        policy_rejected_outputs.logits,\n",
    "                        ref_chosen_outputs.logits,\n",
    "                        ref_rejected_outputs.logits,\n",
    "                        beta=beta\n",
    "                    )\n",
    "                    \n",
    "                    if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                        loss = loss / gradient_accumulation_steps\n",
    "                        loss.backward()\n",
    "                        \n",
    "                        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)\n",
    "                            optimizer.step()\n",
    "                            scheduler.step()\n",
    "                            optimizer.zero_grad()\n",
    "                        \n",
    "                        total_loss += loss.item() * gradient_accumulation_steps\n",
    "                        valid_steps += 1\n",
    "                \n",
    "                if step % 10 == 0:\n",
    "                    avg_loss = total_loss / max(valid_steps, 1)\n",
    "                    print(f\"[Epoch {epoch+1}/{num_epochs} | Step {step}/{len(train_loader)}] - Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {step}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "            \n",
    "            # Memory management\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        if valid_steps > 0:\n",
    "            avg_train_loss = total_loss / valid_steps\n",
    "            val_loss = evaluate_model_dpo(policy_model, reference_model, val_loader, device, beta)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Avg Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = {k: v.cpu() for k, v in policy_model.state_dict().items()}\n",
    "    \n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model_dpo(policy_model, reference_model, val_loader, device, beta):\n",
    "    \"\"\"\n",
    "    Evaluate the model using DPO loss\n",
    "    \n",
    "    Args:\n",
    "        policy_model: The model being evaluated\n",
    "        reference_model: The reference model for DPO\n",
    "        val_loader: DataLoader for validation data\n",
    "        device: Device to evaluate on (cuda/cpu)\n",
    "        beta: Temperature parameter for DPO\n",
    "    \"\"\"\n",
    "    policy_model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    policy_chosen_outputs = policy_model(\n",
    "                        input_ids=batch['preferred_input_ids'],\n",
    "                        attention_mask=batch['preferred_attention_mask']\n",
    "                    )\n",
    "                    policy_rejected_outputs = policy_model(\n",
    "                        input_ids=batch['rejected_input_ids'],\n",
    "                        attention_mask=batch['rejected_attention_mask']\n",
    "                    )\n",
    "                    \n",
    "                    ref_chosen_outputs = reference_model(\n",
    "                        input_ids=batch['preferred_input_ids'],\n",
    "                        attention_mask=batch['preferred_attention_mask']\n",
    "                    )\n",
    "                    ref_rejected_outputs = reference_model(\n",
    "                        input_ids=batch['rejected_input_ids'],\n",
    "                        attention_mask=batch['rejected_attention_mask']\n",
    "                    )\n",
    "            else:\n",
    "                policy_chosen_outputs = policy_model(\n",
    "                    input_ids=batch['preferred_input_ids'],\n",
    "                    attention_mask=batch['preferred_attention_mask']\n",
    "                )\n",
    "                policy_rejected_outputs = policy_model(\n",
    "                    input_ids=batch['rejected_input_ids'],\n",
    "                    attention_mask=batch['rejected_attention_mask']\n",
    "                )\n",
    "                \n",
    "                ref_chosen_outputs = reference_model(\n",
    "                    input_ids=batch['preferred_input_ids'],\n",
    "                    attention_mask=batch['preferred_attention_mask']\n",
    "                )\n",
    "                ref_rejected_outputs = reference_model(\n",
    "                    input_ids=batch['rejected_input_ids'],\n",
    "                    attention_mask=batch['rejected_attention_mask']\n",
    "                )\n",
    "            \n",
    "            loss = compute_dpo_loss(\n",
    "                policy_chosen_outputs.logits,\n",
    "                policy_rejected_outputs.logits,\n",
    "                ref_chosen_outputs.logits,\n",
    "                ref_rejected_outputs.logits,\n",
    "                beta=beta\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# Helper function for computing DPO loss\n",
    "def compute_dpo_loss(policy_chosen_logits, policy_rejected_logits, \n",
    "                    reference_chosen_logits, reference_rejected_logits, \n",
    "                    beta=0.2):\n",
    "    \"\"\"\n",
    "    Compute the DPO loss between policy and reference models\n",
    "    \n",
    "    Args:\n",
    "        policy_chosen_logits: Logits from policy model for chosen examples\n",
    "        policy_rejected_logits: Logits from policy model for rejected examples\n",
    "        reference_chosen_logits: Logits from reference model for chosen examples\n",
    "        reference_rejected_logits: Logits from reference model for rejected examples\n",
    "        beta: Temperature parameter\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    # Get probabilities\n",
    "    policy_chosen_probs = F.softmax(policy_chosen_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    policy_rejected_probs = F.softmax(policy_rejected_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    ref_chosen_probs = F.softmax(reference_chosen_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    ref_rejected_probs = F.softmax(reference_rejected_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    \n",
    "    # Compute rewards\n",
    "    chosen_rewards = (torch.log(policy_chosen_probs + epsilon) - \n",
    "                     torch.log(ref_chosen_probs + epsilon))\n",
    "    rejected_rewards = (torch.log(policy_rejected_probs + epsilon) - \n",
    "                       torch.log(ref_rejected_probs + epsilon))\n",
    "    \n",
    "    # Clamp rewards\n",
    "    max_reward = 50.0\n",
    "    chosen_rewards = torch.clamp(chosen_rewards, -max_reward, max_reward)\n",
    "    rejected_rewards = torch.clamp(rejected_rewards, -max_reward, max_reward)\n",
    "    \n",
    "    # Compute loss\n",
    "    logits_diff = (chosen_rewards - rejected_rewards) / beta\n",
    "    \n",
    "    valid_mask = ~torch.isnan(logits_diff)\n",
    "    if valid_mask.any():\n",
    "        loss = -F.logsigmoid(logits_diff[valid_mask]).mean()\n",
    "    else:\n",
    "        loss = torch.tensor(0.0, device=logits_diff.device)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def main():\n",
    "\n",
    "    \n",
    "    device = setup_environment()\n",
    "    model_name = 'bert-base-uncased'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/final_data.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    policy_model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    reference_model, _ = setup_model_and_tokenizer(model_name, device)\n",
    "    \n",
    "\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "  \n",
    "    train_dataset = PreferenceEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = PreferenceEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # Setup optimization\n",
    "    optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "    num_epochs = 8\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    best_model_state = train_model_dpo(\n",
    "        policy_model,\n",
    "        reference_model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        num_epochs=num_epochs,\n",
    "        beta=0.2\n",
    "    )\n",
    "\n",
    "   \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/bert_dpo123_classification_model\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    policy_model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"beta\": 0.2\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fd34613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/8 | Step 0/500] - Loss: 0.8203\n",
      "[Epoch 1/8 | Step 10/500] - Loss: 0.7087\n",
      "[Epoch 1/8 | Step 20/500] - Loss: 0.7226\n",
      "[Epoch 1/8 | Step 30/500] - Loss: 0.7019\n",
      "[Epoch 1/8 | Step 40/500] - Loss: 0.7047\n",
      "[Epoch 1/8 | Step 50/500] - Loss: 0.6942\n",
      "[Epoch 1/8 | Step 60/500] - Loss: 0.6977\n",
      "[Epoch 1/8 | Step 70/500] - Loss: 0.6917\n",
      "[Epoch 1/8 | Step 80/500] - Loss: 0.6937\n",
      "[Epoch 1/8 | Step 90/500] - Loss: 0.6947\n",
      "[Epoch 1/8 | Step 100/500] - Loss: 0.6892\n",
      "[Epoch 1/8 | Step 110/500] - Loss: 0.6869\n",
      "[Epoch 1/8 | Step 120/500] - Loss: 0.6846\n",
      "[Epoch 1/8 | Step 130/500] - Loss: 0.6834\n",
      "[Epoch 1/8 | Step 140/500] - Loss: 0.6790\n",
      "[Epoch 1/8 | Step 150/500] - Loss: 0.6726\n",
      "[Epoch 1/8 | Step 160/500] - Loss: 0.6572\n",
      "[Epoch 1/8 | Step 170/500] - Loss: 0.6463\n",
      "[Epoch 1/8 | Step 180/500] - Loss: 0.6308\n",
      "[Epoch 1/8 | Step 190/500] - Loss: 0.6082\n",
      "[Epoch 1/8 | Step 200/500] - Loss: 0.5822\n",
      "[Epoch 1/8 | Step 210/500] - Loss: 0.5622\n",
      "[Epoch 1/8 | Step 220/500] - Loss: 0.5386\n",
      "[Epoch 1/8 | Step 230/500] - Loss: 0.5172\n",
      "[Epoch 1/8 | Step 240/500] - Loss: 0.4995\n",
      "[Epoch 1/8 | Step 250/500] - Loss: 0.4834\n",
      "[Epoch 1/8 | Step 260/500] - Loss: 0.4685\n",
      "[Epoch 1/8 | Step 270/500] - Loss: 0.4538\n",
      "[Epoch 1/8 | Step 280/500] - Loss: 0.4387\n",
      "[Epoch 1/8 | Step 290/500] - Loss: 0.4241\n",
      "[Epoch 1/8 | Step 300/500] - Loss: 0.4131\n",
      "[Epoch 1/8 | Step 310/500] - Loss: 0.4026\n",
      "[Epoch 1/8 | Step 320/500] - Loss: 0.3906\n",
      "[Epoch 1/8 | Step 330/500] - Loss: 0.3810\n",
      "[Epoch 1/8 | Step 340/500] - Loss: 0.3740\n",
      "[Epoch 1/8 | Step 350/500] - Loss: 0.3644\n",
      "[Epoch 1/8 | Step 360/500] - Loss: 0.3574\n",
      "[Epoch 1/8 | Step 370/500] - Loss: 0.3499\n",
      "[Epoch 1/8 | Step 380/500] - Loss: 0.3414\n",
      "[Epoch 1/8 | Step 390/500] - Loss: 0.3361\n",
      "[Epoch 1/8 | Step 400/500] - Loss: 0.3286\n",
      "[Epoch 1/8 | Step 410/500] - Loss: 0.3217\n",
      "[Epoch 1/8 | Step 420/500] - Loss: 0.3166\n",
      "[Epoch 1/8 | Step 430/500] - Loss: 0.3113\n",
      "[Epoch 1/8 | Step 440/500] - Loss: 0.3064\n",
      "[Epoch 1/8 | Step 450/500] - Loss: 0.3014\n",
      "[Epoch 1/8 | Step 460/500] - Loss: 0.2973\n",
      "[Epoch 1/8 | Step 470/500] - Loss: 0.2932\n",
      "[Epoch 1/8 | Step 480/500] - Loss: 0.2885\n",
      "[Epoch 1/8 | Step 490/500] - Loss: 0.2850\n",
      "Epoch 1/8 - Avg Train Loss: 0.2818, Val Loss: 0.0687\n",
      "[Epoch 2/8 | Step 0/500] - Loss: 0.1699\n",
      "[Epoch 2/8 | Step 10/500] - Loss: 0.1234\n",
      "[Epoch 2/8 | Step 20/500] - Loss: 0.1085\n",
      "[Epoch 2/8 | Step 30/500] - Loss: 0.1063\n",
      "[Epoch 2/8 | Step 40/500] - Loss: 0.1094\n",
      "[Epoch 2/8 | Step 50/500] - Loss: 0.0978\n",
      "[Epoch 2/8 | Step 60/500] - Loss: 0.0897\n",
      "[Epoch 2/8 | Step 70/500] - Loss: 0.0898\n",
      "[Epoch 2/8 | Step 80/500] - Loss: 0.0819\n",
      "[Epoch 2/8 | Step 90/500] - Loss: 0.0877\n",
      "[Epoch 2/8 | Step 100/500] - Loss: 0.0897\n",
      "[Epoch 2/8 | Step 110/500] - Loss: 0.0898\n",
      "[Epoch 2/8 | Step 120/500] - Loss: 0.0900\n",
      "[Epoch 2/8 | Step 130/500] - Loss: 0.0887\n",
      "[Epoch 2/8 | Step 140/500] - Loss: 0.0896\n",
      "[Epoch 2/8 | Step 150/500] - Loss: 0.0883\n",
      "[Epoch 2/8 | Step 160/500] - Loss: 0.0895\n",
      "[Epoch 2/8 | Step 170/500] - Loss: 0.0901\n",
      "[Epoch 2/8 | Step 180/500] - Loss: 0.0880\n",
      "[Epoch 2/8 | Step 190/500] - Loss: 0.0856\n",
      "[Epoch 2/8 | Step 200/500] - Loss: 0.0857\n",
      "[Epoch 2/8 | Step 210/500] - Loss: 0.0846\n",
      "[Epoch 2/8 | Step 220/500] - Loss: 0.0851\n",
      "[Epoch 2/8 | Step 230/500] - Loss: 0.0862\n",
      "[Epoch 2/8 | Step 240/500] - Loss: 0.0856\n",
      "[Epoch 2/8 | Step 250/500] - Loss: 0.0854\n",
      "[Epoch 2/8 | Step 260/500] - Loss: 0.0848\n",
      "[Epoch 2/8 | Step 270/500] - Loss: 0.0834\n",
      "[Epoch 2/8 | Step 280/500] - Loss: 0.0834\n",
      "[Epoch 2/8 | Step 290/500] - Loss: 0.0824\n",
      "[Epoch 2/8 | Step 300/500] - Loss: 0.0834\n",
      "[Epoch 2/8 | Step 310/500] - Loss: 0.0832\n",
      "[Epoch 2/8 | Step 320/500] - Loss: 0.0814\n",
      "[Epoch 2/8 | Step 330/500] - Loss: 0.0807\n",
      "[Epoch 2/8 | Step 340/500] - Loss: 0.0792\n",
      "[Epoch 2/8 | Step 350/500] - Loss: 0.0775\n",
      "[Epoch 2/8 | Step 360/500] - Loss: 0.0800\n",
      "[Epoch 2/8 | Step 370/500] - Loss: 0.0804\n",
      "[Epoch 2/8 | Step 380/500] - Loss: 0.0810\n",
      "[Epoch 2/8 | Step 390/500] - Loss: 0.0808\n",
      "[Epoch 2/8 | Step 400/500] - Loss: 0.0809\n",
      "[Epoch 2/8 | Step 410/500] - Loss: 0.0820\n",
      "[Epoch 2/8 | Step 420/500] - Loss: 0.0827\n",
      "[Epoch 2/8 | Step 430/500] - Loss: 0.0821\n",
      "[Epoch 2/8 | Step 440/500] - Loss: 0.0819\n",
      "[Epoch 2/8 | Step 450/500] - Loss: 0.0825\n",
      "[Epoch 2/8 | Step 460/500] - Loss: 0.0824\n",
      "[Epoch 2/8 | Step 470/500] - Loss: 0.0811\n",
      "[Epoch 2/8 | Step 480/500] - Loss: 0.0823\n",
      "[Epoch 2/8 | Step 490/500] - Loss: 0.0833\n",
      "Epoch 2/8 - Avg Train Loss: 0.0831, Val Loss: 0.0690\n",
      "[Epoch 3/8 | Step 0/500] - Loss: 0.3398\n",
      "[Epoch 3/8 | Step 10/500] - Loss: 0.1487\n",
      "[Epoch 3/8 | Step 20/500] - Loss: 0.1137\n",
      "[Epoch 3/8 | Step 30/500] - Loss: 0.1135\n",
      "[Epoch 3/8 | Step 40/500] - Loss: 0.1050\n",
      "[Epoch 3/8 | Step 50/500] - Loss: 0.1030\n",
      "[Epoch 3/8 | Step 60/500] - Loss: 0.1014\n",
      "[Epoch 3/8 | Step 70/500] - Loss: 0.0950\n",
      "[Epoch 3/8 | Step 80/500] - Loss: 0.0924\n",
      "[Epoch 3/8 | Step 90/500] - Loss: 0.0888\n",
      "[Epoch 3/8 | Step 100/500] - Loss: 0.0953\n",
      "[Epoch 3/8 | Step 110/500] - Loss: 0.0924\n",
      "[Epoch 3/8 | Step 120/500] - Loss: 0.0907\n",
      "[Epoch 3/8 | Step 130/500] - Loss: 0.0895\n",
      "[Epoch 3/8 | Step 140/500] - Loss: 0.0864\n",
      "[Epoch 3/8 | Step 150/500] - Loss: 0.0844\n",
      "[Epoch 3/8 | Step 160/500] - Loss: 0.0852\n",
      "[Epoch 3/8 | Step 170/500] - Loss: 0.0835\n",
      "[Epoch 3/8 | Step 180/500] - Loss: 0.0852\n",
      "[Epoch 3/8 | Step 190/500] - Loss: 0.0842\n",
      "[Epoch 3/8 | Step 200/500] - Loss: 0.0855\n",
      "[Epoch 3/8 | Step 210/500] - Loss: 0.0832\n",
      "[Epoch 3/8 | Step 220/500] - Loss: 0.0836\n",
      "[Epoch 3/8 | Step 230/500] - Loss: 0.0837\n",
      "[Epoch 3/8 | Step 240/500] - Loss: 0.0842\n",
      "[Epoch 3/8 | Step 250/500] - Loss: 0.0826\n",
      "[Epoch 3/8 | Step 260/500] - Loss: 0.0819\n",
      "[Epoch 3/8 | Step 270/500] - Loss: 0.0811\n",
      "[Epoch 3/8 | Step 280/500] - Loss: 0.0806\n",
      "[Epoch 3/8 | Step 290/500] - Loss: 0.0801\n",
      "[Epoch 3/8 | Step 300/500] - Loss: 0.0797\n",
      "[Epoch 3/8 | Step 310/500] - Loss: 0.0802\n",
      "[Epoch 3/8 | Step 320/500] - Loss: 0.0795\n",
      "[Epoch 3/8 | Step 330/500] - Loss: 0.0783\n",
      "[Epoch 3/8 | Step 340/500] - Loss: 0.0775\n",
      "[Epoch 3/8 | Step 350/500] - Loss: 0.0770\n",
      "[Epoch 3/8 | Step 360/500] - Loss: 0.0777\n",
      "[Epoch 3/8 | Step 370/500] - Loss: 0.0771\n",
      "[Epoch 3/8 | Step 380/500] - Loss: 0.0788\n",
      "[Epoch 3/8 | Step 390/500] - Loss: 0.0779\n",
      "[Epoch 3/8 | Step 400/500] - Loss: 0.0771\n",
      "[Epoch 3/8 | Step 410/500] - Loss: 0.0763\n",
      "[Epoch 3/8 | Step 420/500] - Loss: 0.0782\n",
      "[Epoch 3/8 | Step 430/500] - Loss: 0.0778\n",
      "[Epoch 3/8 | Step 440/500] - Loss: 0.0777\n",
      "[Epoch 3/8 | Step 450/500] - Loss: 0.0770\n",
      "[Epoch 3/8 | Step 460/500] - Loss: 0.0785\n",
      "[Epoch 3/8 | Step 470/500] - Loss: 0.0783\n",
      "[Epoch 3/8 | Step 480/500] - Loss: 0.0780\n",
      "[Epoch 3/8 | Step 490/500] - Loss: 0.0778\n",
      "Epoch 3/8 - Avg Train Loss: 0.0779, Val Loss: 0.0688\n",
      "[Epoch 4/8 | Step 0/500] - Loss: 0.0013\n",
      "[Epoch 4/8 | Step 10/500] - Loss: 0.0420\n",
      "[Epoch 4/8 | Step 20/500] - Loss: 0.0697\n",
      "[Epoch 4/8 | Step 30/500] - Loss: 0.0635\n",
      "[Epoch 4/8 | Step 40/500] - Loss: 0.0575\n",
      "[Epoch 4/8 | Step 50/500] - Loss: 0.0545\n",
      "[Epoch 4/8 | Step 60/500] - Loss: 0.0648\n",
      "[Epoch 4/8 | Step 70/500] - Loss: 0.0657\n",
      "[Epoch 4/8 | Step 80/500] - Loss: 0.0599\n",
      "[Epoch 4/8 | Step 90/500] - Loss: 0.0596\n",
      "[Epoch 4/8 | Step 100/500] - Loss: 0.0604\n",
      "[Epoch 4/8 | Step 110/500] - Loss: 0.0624\n",
      "[Epoch 4/8 | Step 120/500] - Loss: 0.0644\n",
      "[Epoch 4/8 | Step 130/500] - Loss: 0.0651\n",
      "[Epoch 4/8 | Step 140/500] - Loss: 0.0698\n",
      "[Epoch 4/8 | Step 150/500] - Loss: 0.0677\n",
      "[Epoch 4/8 | Step 160/500] - Loss: 0.0663\n",
      "[Epoch 4/8 | Step 170/500] - Loss: 0.0651\n",
      "[Epoch 4/8 | Step 180/500] - Loss: 0.0675\n",
      "[Epoch 4/8 | Step 190/500] - Loss: 0.0684\n",
      "[Epoch 4/8 | Step 200/500] - Loss: 0.0689\n",
      "[Epoch 4/8 | Step 210/500] - Loss: 0.0695\n",
      "[Epoch 4/8 | Step 220/500] - Loss: 0.0711\n",
      "[Epoch 4/8 | Step 230/500] - Loss: 0.0727\n",
      "[Epoch 4/8 | Step 240/500] - Loss: 0.0744\n",
      "[Epoch 4/8 | Step 250/500] - Loss: 0.0751\n",
      "[Epoch 4/8 | Step 260/500] - Loss: 0.0747\n",
      "[Epoch 4/8 | Step 270/500] - Loss: 0.0742\n",
      "[Epoch 4/8 | Step 280/500] - Loss: 0.0765\n",
      "[Epoch 4/8 | Step 290/500] - Loss: 0.0748\n",
      "[Epoch 4/8 | Step 300/500] - Loss: 0.0750\n",
      "[Epoch 4/8 | Step 310/500] - Loss: 0.0770\n",
      "[Epoch 4/8 | Step 320/500] - Loss: 0.0769\n",
      "[Epoch 4/8 | Step 330/500] - Loss: 0.0777\n",
      "[Epoch 4/8 | Step 340/500] - Loss: 0.0780\n",
      "[Epoch 4/8 | Step 350/500] - Loss: 0.0777\n",
      "[Epoch 4/8 | Step 360/500] - Loss: 0.0763\n",
      "[Epoch 4/8 | Step 370/500] - Loss: 0.0756\n",
      "[Epoch 4/8 | Step 380/500] - Loss: 0.0762\n",
      "[Epoch 4/8 | Step 390/500] - Loss: 0.0771\n",
      "[Epoch 4/8 | Step 400/500] - Loss: 0.0763\n",
      "[Epoch 4/8 | Step 410/500] - Loss: 0.0750\n",
      "[Epoch 4/8 | Step 420/500] - Loss: 0.0753\n",
      "[Epoch 4/8 | Step 430/500] - Loss: 0.0769\n",
      "[Epoch 4/8 | Step 440/500] - Loss: 0.0765\n",
      "[Epoch 4/8 | Step 450/500] - Loss: 0.0761\n",
      "[Epoch 4/8 | Step 460/500] - Loss: 0.0760\n",
      "[Epoch 4/8 | Step 470/500] - Loss: 0.0759\n",
      "[Epoch 4/8 | Step 480/500] - Loss: 0.0771\n",
      "[Epoch 4/8 | Step 490/500] - Loss: 0.0769\n",
      "Epoch 4/8 - Avg Train Loss: 0.0773, Val Loss: 0.0687\n",
      "[Epoch 5/8 | Step 0/500] - Loss: 0.0637\n",
      "[Epoch 5/8 | Step 10/500] - Loss: 0.0763\n",
      "[Epoch 5/8 | Step 20/500] - Loss: 0.0963\n",
      "[Epoch 5/8 | Step 30/500] - Loss: 0.0854\n",
      "[Epoch 5/8 | Step 40/500] - Loss: 0.0694\n",
      "[Epoch 5/8 | Step 50/500] - Loss: 0.0712\n",
      "[Epoch 5/8 | Step 60/500] - Loss: 0.0763\n",
      "[Epoch 5/8 | Step 70/500] - Loss: 0.0678\n",
      "[Epoch 5/8 | Step 80/500] - Loss: 0.0705\n",
      "[Epoch 5/8 | Step 90/500] - Loss: 0.0712\n",
      "[Epoch 5/8 | Step 100/500] - Loss: 0.0746\n",
      "[Epoch 5/8 | Step 110/500] - Loss: 0.0768\n",
      "[Epoch 5/8 | Step 120/500] - Loss: 0.0745\n",
      "[Epoch 5/8 | Step 130/500] - Loss: 0.0753\n",
      "[Epoch 5/8 | Step 140/500] - Loss: 0.0744\n",
      "[Epoch 5/8 | Step 150/500] - Loss: 0.0747\n",
      "[Epoch 5/8 | Step 160/500] - Loss: 0.0725\n",
      "[Epoch 5/8 | Step 170/500] - Loss: 0.0710\n",
      "[Epoch 5/8 | Step 180/500] - Loss: 0.0691\n",
      "[Epoch 5/8 | Step 190/500] - Loss: 0.0705\n",
      "[Epoch 5/8 | Step 200/500] - Loss: 0.0710\n",
      "[Epoch 5/8 | Step 210/500] - Loss: 0.0705\n",
      "[Epoch 5/8 | Step 220/500] - Loss: 0.0698\n",
      "[Epoch 5/8 | Step 230/500] - Loss: 0.0704\n",
      "[Epoch 5/8 | Step 240/500] - Loss: 0.0712\n",
      "[Epoch 5/8 | Step 250/500] - Loss: 0.0728\n",
      "[Epoch 5/8 | Step 260/500] - Loss: 0.0723\n",
      "[Epoch 5/8 | Step 270/500] - Loss: 0.0707\n",
      "[Epoch 5/8 | Step 280/500] - Loss: 0.0701\n",
      "[Epoch 5/8 | Step 290/500] - Loss: 0.0705\n",
      "[Epoch 5/8 | Step 300/500] - Loss: 0.0727\n",
      "[Epoch 5/8 | Step 310/500] - Loss: 0.0726\n",
      "[Epoch 5/8 | Step 320/500] - Loss: 0.0718\n",
      "[Epoch 5/8 | Step 330/500] - Loss: 0.0720\n",
      "[Epoch 5/8 | Step 340/500] - Loss: 0.0716\n",
      "[Epoch 5/8 | Step 350/500] - Loss: 0.0730\n",
      "[Epoch 5/8 | Step 360/500] - Loss: 0.0721\n",
      "[Epoch 5/8 | Step 370/500] - Loss: 0.0715\n",
      "[Epoch 5/8 | Step 380/500] - Loss: 0.0711\n",
      "[Epoch 5/8 | Step 390/500] - Loss: 0.0709\n",
      "[Epoch 5/8 | Step 400/500] - Loss: 0.0714\n",
      "[Epoch 5/8 | Step 410/500] - Loss: 0.0708\n",
      "[Epoch 5/8 | Step 420/500] - Loss: 0.0712\n",
      "[Epoch 5/8 | Step 430/500] - Loss: 0.0707\n",
      "[Epoch 5/8 | Step 440/500] - Loss: 0.0701\n",
      "[Epoch 5/8 | Step 450/500] - Loss: 0.0706\n",
      "[Epoch 5/8 | Step 460/500] - Loss: 0.0705\n",
      "[Epoch 5/8 | Step 470/500] - Loss: 0.0710\n",
      "[Epoch 5/8 | Step 480/500] - Loss: 0.0703\n",
      "[Epoch 5/8 | Step 490/500] - Loss: 0.0705\n",
      "Epoch 5/8 - Avg Train Loss: 0.0701, Val Loss: 0.0686\n",
      "[Epoch 6/8 | Step 0/500] - Loss: 0.0001\n",
      "[Epoch 6/8 | Step 10/500] - Loss: 0.1277\n",
      "[Epoch 6/8 | Step 20/500] - Loss: 0.1005\n",
      "[Epoch 6/8 | Step 30/500] - Loss: 0.0950\n",
      "[Epoch 6/8 | Step 40/500] - Loss: 0.0961\n",
      "[Epoch 6/8 | Step 50/500] - Loss: 0.0942\n",
      "[Epoch 6/8 | Step 60/500] - Loss: 0.0879\n",
      "[Epoch 6/8 | Step 70/500] - Loss: 0.0810\n",
      "[Epoch 6/8 | Step 80/500] - Loss: 0.0818\n",
      "[Epoch 6/8 | Step 90/500] - Loss: 0.0779\n",
      "[Epoch 6/8 | Step 100/500] - Loss: 0.0806\n",
      "[Epoch 6/8 | Step 110/500] - Loss: 0.0794\n",
      "[Epoch 6/8 | Step 120/500] - Loss: 0.0800\n",
      "[Epoch 6/8 | Step 130/500] - Loss: 0.0789\n",
      "[Epoch 6/8 | Step 140/500] - Loss: 0.0795\n",
      "[Epoch 6/8 | Step 150/500] - Loss: 0.0787\n",
      "[Epoch 6/8 | Step 160/500] - Loss: 0.0788\n",
      "[Epoch 6/8 | Step 170/500] - Loss: 0.0804\n",
      "[Epoch 6/8 | Step 180/500] - Loss: 0.0786\n",
      "[Epoch 6/8 | Step 190/500] - Loss: 0.0794\n",
      "[Epoch 6/8 | Step 200/500] - Loss: 0.0799\n",
      "[Epoch 6/8 | Step 210/500] - Loss: 0.0796\n",
      "[Epoch 6/8 | Step 220/500] - Loss: 0.0789\n",
      "[Epoch 6/8 | Step 230/500] - Loss: 0.0776\n",
      "[Epoch 6/8 | Step 240/500] - Loss: 0.0774\n",
      "[Epoch 6/8 | Step 250/500] - Loss: 0.0768\n",
      "[Epoch 6/8 | Step 260/500] - Loss: 0.0772\n",
      "[Epoch 6/8 | Step 270/500] - Loss: 0.0765\n",
      "[Epoch 6/8 | Step 280/500] - Loss: 0.0760\n",
      "[Epoch 6/8 | Step 290/500] - Loss: 0.0756\n",
      "[Epoch 6/8 | Step 300/500] - Loss: 0.0757\n",
      "[Epoch 6/8 | Step 310/500] - Loss: 0.0770\n",
      "[Epoch 6/8 | Step 320/500] - Loss: 0.0776\n",
      "[Epoch 6/8 | Step 330/500] - Loss: 0.0771\n",
      "[Epoch 6/8 | Step 340/500] - Loss: 0.0771\n",
      "[Epoch 6/8 | Step 350/500] - Loss: 0.0770\n",
      "[Epoch 6/8 | Step 360/500] - Loss: 0.0769\n",
      "[Epoch 6/8 | Step 370/500] - Loss: 0.0766\n",
      "[Epoch 6/8 | Step 380/500] - Loss: 0.0766\n",
      "[Epoch 6/8 | Step 390/500] - Loss: 0.0762\n",
      "[Epoch 6/8 | Step 400/500] - Loss: 0.0767\n",
      "[Epoch 6/8 | Step 410/500] - Loss: 0.0761\n",
      "[Epoch 6/8 | Step 420/500] - Loss: 0.0748\n",
      "[Epoch 6/8 | Step 430/500] - Loss: 0.0744\n",
      "[Epoch 6/8 | Step 440/500] - Loss: 0.0744\n",
      "[Epoch 6/8 | Step 450/500] - Loss: 0.0748\n",
      "[Epoch 6/8 | Step 460/500] - Loss: 0.0739\n",
      "[Epoch 6/8 | Step 470/500] - Loss: 0.0733\n",
      "[Epoch 6/8 | Step 480/500] - Loss: 0.0736\n",
      "[Epoch 6/8 | Step 490/500] - Loss: 0.0728\n",
      "Epoch 6/8 - Avg Train Loss: 0.0720, Val Loss: 0.0686\n",
      "[Epoch 7/8 | Step 0/500] - Loss: 0.0889\n",
      "[Epoch 7/8 | Step 10/500] - Loss: 0.0862\n",
      "[Epoch 7/8 | Step 20/500] - Loss: 0.0628\n",
      "[Epoch 7/8 | Step 30/500] - Loss: 0.0718\n",
      "[Epoch 7/8 | Step 40/500] - Loss: 0.0643\n",
      "[Epoch 7/8 | Step 50/500] - Loss: 0.0617\n",
      "[Epoch 7/8 | Step 60/500] - Loss: 0.0664\n",
      "[Epoch 7/8 | Step 70/500] - Loss: 0.0699\n",
      "[Epoch 7/8 | Step 80/500] - Loss: 0.0684\n",
      "[Epoch 7/8 | Step 90/500] - Loss: 0.0697\n",
      "[Epoch 7/8 | Step 100/500] - Loss: 0.0667\n",
      "[Epoch 7/8 | Step 110/500] - Loss: 0.0656\n",
      "[Epoch 7/8 | Step 120/500] - Loss: 0.0649\n",
      "[Epoch 7/8 | Step 130/500] - Loss: 0.0651\n",
      "[Epoch 7/8 | Step 140/500] - Loss: 0.0692\n",
      "[Epoch 7/8 | Step 150/500] - Loss: 0.0671\n",
      "[Epoch 7/8 | Step 160/500] - Loss: 0.0682\n",
      "[Epoch 7/8 | Step 170/500] - Loss: 0.0683\n",
      "[Epoch 7/8 | Step 180/500] - Loss: 0.0707\n",
      "[Epoch 7/8 | Step 190/500] - Loss: 0.0710\n",
      "[Epoch 7/8 | Step 200/500] - Loss: 0.0712\n",
      "[Epoch 7/8 | Step 210/500] - Loss: 0.0721\n",
      "[Epoch 7/8 | Step 220/500] - Loss: 0.0713\n",
      "[Epoch 7/8 | Step 230/500] - Loss: 0.0718\n",
      "[Epoch 7/8 | Step 240/500] - Loss: 0.0721\n",
      "[Epoch 7/8 | Step 250/500] - Loss: 0.0717\n",
      "[Epoch 7/8 | Step 260/500] - Loss: 0.0723\n",
      "[Epoch 7/8 | Step 270/500] - Loss: 0.0718\n",
      "[Epoch 7/8 | Step 280/500] - Loss: 0.0723\n",
      "[Epoch 7/8 | Step 290/500] - Loss: 0.0721\n",
      "[Epoch 7/8 | Step 300/500] - Loss: 0.0725\n",
      "[Epoch 7/8 | Step 310/500] - Loss: 0.0724\n",
      "[Epoch 7/8 | Step 320/500] - Loss: 0.0722\n",
      "[Epoch 7/8 | Step 330/500] - Loss: 0.0714\n",
      "[Epoch 7/8 | Step 340/500] - Loss: 0.0699\n",
      "[Epoch 7/8 | Step 350/500] - Loss: 0.0692\n",
      "[Epoch 7/8 | Step 360/500] - Loss: 0.0695\n",
      "[Epoch 7/8 | Step 370/500] - Loss: 0.0712\n",
      "[Epoch 7/8 | Step 380/500] - Loss: 0.0708\n",
      "[Epoch 7/8 | Step 390/500] - Loss: 0.0707\n",
      "[Epoch 7/8 | Step 400/500] - Loss: 0.0701\n",
      "[Epoch 7/8 | Step 410/500] - Loss: 0.0708\n",
      "[Epoch 7/8 | Step 420/500] - Loss: 0.0707\n",
      "[Epoch 7/8 | Step 430/500] - Loss: 0.0705\n",
      "[Epoch 7/8 | Step 440/500] - Loss: 0.0700\n",
      "[Epoch 7/8 | Step 450/500] - Loss: 0.0701\n",
      "[Epoch 7/8 | Step 460/500] - Loss: 0.0696\n",
      "[Epoch 7/8 | Step 470/500] - Loss: 0.0704\n",
      "[Epoch 7/8 | Step 480/500] - Loss: 0.0709\n",
      "[Epoch 7/8 | Step 490/500] - Loss: 0.0706\n",
      "Epoch 7/8 - Avg Train Loss: 0.0701, Val Loss: 0.0686\n",
      "[Epoch 8/8 | Step 0/500] - Loss: 0.0000\n",
      "[Epoch 8/8 | Step 10/500] - Loss: 0.0544\n",
      "[Epoch 8/8 | Step 20/500] - Loss: 0.0751\n",
      "[Epoch 8/8 | Step 30/500] - Loss: 0.0661\n",
      "[Epoch 8/8 | Step 40/500] - Loss: 0.0667\n",
      "[Epoch 8/8 | Step 50/500] - Loss: 0.0642\n",
      "[Epoch 8/8 | Step 60/500] - Loss: 0.0726\n",
      "[Epoch 8/8 | Step 70/500] - Loss: 0.0715\n",
      "[Epoch 8/8 | Step 80/500] - Loss: 0.0711\n",
      "[Epoch 8/8 | Step 90/500] - Loss: 0.0687\n",
      "[Epoch 8/8 | Step 100/500] - Loss: 0.0640\n",
      "[Epoch 8/8 | Step 110/500] - Loss: 0.0650\n",
      "[Epoch 8/8 | Step 120/500] - Loss: 0.0645\n",
      "[Epoch 8/8 | Step 130/500] - Loss: 0.0646\n",
      "[Epoch 8/8 | Step 140/500] - Loss: 0.0670\n",
      "[Epoch 8/8 | Step 150/500] - Loss: 0.0660\n",
      "[Epoch 8/8 | Step 160/500] - Loss: 0.0696\n",
      "[Epoch 8/8 | Step 170/500] - Loss: 0.0704\n",
      "[Epoch 8/8 | Step 180/500] - Loss: 0.0721\n",
      "[Epoch 8/8 | Step 190/500] - Loss: 0.0709\n",
      "[Epoch 8/8 | Step 200/500] - Loss: 0.0721\n",
      "[Epoch 8/8 | Step 210/500] - Loss: 0.0723\n",
      "[Epoch 8/8 | Step 220/500] - Loss: 0.0715\n",
      "[Epoch 8/8 | Step 230/500] - Loss: 0.0726\n",
      "[Epoch 8/8 | Step 240/500] - Loss: 0.0729\n",
      "[Epoch 8/8 | Step 250/500] - Loss: 0.0725\n",
      "[Epoch 8/8 | Step 260/500] - Loss: 0.0713\n",
      "[Epoch 8/8 | Step 270/500] - Loss: 0.0703\n",
      "[Epoch 8/8 | Step 280/500] - Loss: 0.0701\n",
      "[Epoch 8/8 | Step 290/500] - Loss: 0.0687\n",
      "[Epoch 8/8 | Step 300/500] - Loss: 0.0669\n",
      "[Epoch 8/8 | Step 310/500] - Loss: 0.0668\n",
      "[Epoch 8/8 | Step 320/500] - Loss: 0.0668\n",
      "[Epoch 8/8 | Step 330/500] - Loss: 0.0673\n",
      "[Epoch 8/8 | Step 340/500] - Loss: 0.0668\n",
      "[Epoch 8/8 | Step 350/500] - Loss: 0.0676\n",
      "[Epoch 8/8 | Step 360/500] - Loss: 0.0680\n",
      "[Epoch 8/8 | Step 370/500] - Loss: 0.0677\n",
      "[Epoch 8/8 | Step 380/500] - Loss: 0.0677\n",
      "[Epoch 8/8 | Step 390/500] - Loss: 0.0687\n",
      "[Epoch 8/8 | Step 400/500] - Loss: 0.0690\n",
      "[Epoch 8/8 | Step 410/500] - Loss: 0.0692\n",
      "[Epoch 8/8 | Step 420/500] - Loss: 0.0701\n",
      "[Epoch 8/8 | Step 430/500] - Loss: 0.0702\n",
      "[Epoch 8/8 | Step 440/500] - Loss: 0.0696\n",
      "[Epoch 8/8 | Step 450/500] - Loss: 0.0702\n",
      "[Epoch 8/8 | Step 460/500] - Loss: 0.0707\n",
      "[Epoch 8/8 | Step 470/500] - Loss: 0.0701\n",
      "[Epoch 8/8 | Step 480/500] - Loss: 0.0697\n",
      "[Epoch 8/8 | Step 490/500] - Loss: 0.0693\n",
      "Epoch 8/8 - Avg Train Loss: 0.0693, Val Loss: 0.0686\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    ")\n",
    "# Keep/Add these imports\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Set up the GPU environment and return the appropriate device.\"\"\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "  \n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    return device\n",
    "\n",
    "def setup_model_and_tokenizer(model_name, device):\n",
    "    \"\"\"Setup BERT model and tokenizer.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(model_name)\n",
    "    model_config.num_labels = 2\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=model_config\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "class PreferenceEmailDataset(Dataset):\n",
    "    def __init__(self, emails_df, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Dataset to create pairs of message, preferred response, and rejected response for DPO training.\n",
    "        \"\"\"\n",
    "        self.emails_df = emails_df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pairs = self._create_preference_pairs()\n",
    "\n",
    "    def _create_preference_pairs(self):\n",
    "        \"\"\"\n",
    "        Create pairs using emails from the dataset based on their labels.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for _, selected_email in self.emails_df.iterrows():\n",
    "            selected_label = selected_email['label']\n",
    "            ham_emails = self.emails_df[self.emails_df['label'] == 0]\n",
    "            phish_emails = self.emails_df[self.emails_df['label'] == 1]\n",
    "\n",
    "            if selected_label == 1:  # Phishing email\n",
    "                preferred_email = phish_emails[phish_emails.index != selected_email.name].sample(n=1).iloc[0]\n",
    "                rejected_email = ham_emails.sample(n=1).iloc[0]\n",
    "                pairs.append({\n",
    "                    'message': selected_email,\n",
    "                    'preferred': preferred_email,\n",
    "                    'rejected': rejected_email\n",
    "                })\n",
    "            elif selected_label == 0:  # Ham email\n",
    "                preferred_email = ham_emails[ham_emails.index != selected_email.name].sample(n=1).iloc[0]\n",
    "                rejected_email = phish_emails.sample(n=1).iloc[0]\n",
    "                pairs.append({\n",
    "                    'message': selected_email,\n",
    "                    'preferred': preferred_email,\n",
    "                    'rejected': rejected_email\n",
    "                })\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def _prepare_email_input(self, message, response):\n",
    "        \"\"\"\n",
    "        Prepare the input text with formatted message and response for tokenization.\n",
    "        \"\"\"\n",
    "        formatted_input = f\"<s>[INST] {message} [/INST] {response}</s>\"\n",
    "        return self.tokenizer(\n",
    "            formatted_input,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        # Construct the prompt for phishing or ham detection\n",
    "        if pair['message']['label'] == 1:\n",
    "            message_text = (\n",
    "                \"This email is flagged as a phishing email. \"\n",
    "                \"Carefully examine the sender's address, subject line, and content of the email. \"\n",
    "                f\"Sender: {pair['message']['sender']} [SEP] \"\n",
    "                f\"Subject: {pair['message']['subject']} [SEP] \"\n",
    "                f\"Body: {pair['message']['body']}\"\n",
    "            )\n",
    "        else:\n",
    "            message_text = (\n",
    "                \"This email is flagged as a legitimate email. \"\n",
    "                \"Look for consistent and clear sender details, subject relevance, and authentic body content. \"\n",
    "                f\"Sender: {pair['message']['sender']} [SEP] \"\n",
    "                f\"Subject: {pair['message']['subject']} [SEP] \"\n",
    "                f\"Body: {pair['message']['body']}\"\n",
    "            )\n",
    "        \n",
    "        \n",
    "        preferred_response = (\n",
    "            \"This is a similar email example to the one above. \"\n",
    "            f\"Sender: {pair['preferred']['sender']} [SEP] \"\n",
    "            f\"Subject: {pair['preferred']['subject']} [SEP] \"\n",
    "            f\"Body: {pair['preferred']['body']}\"\n",
    "        )\n",
    "        rejected_response = (\n",
    "            \"This email is different in intent. Notice the sender's address, subject, and content mismatch. \"\n",
    "            f\"Sender: {pair['rejected']['sender']} [SEP] \"\n",
    "            f\"Subject: {pair['rejected']['subject']} [SEP] \"\n",
    "            f\"Body: {pair['rejected']['body']}\"\n",
    "        )\n",
    "        \n",
    "        message_inputs = self._prepare_email_input(message_text, \"\")\n",
    "        preferred_inputs = self._prepare_email_input(message_text, preferred_response)\n",
    "        rejected_inputs = self._prepare_email_input(message_text, rejected_response)\n",
    "\n",
    "        return {\n",
    "            'message_input_ids': message_inputs['input_ids'].squeeze(),\n",
    "            'message_attention_mask': message_inputs['attention_mask'].squeeze(),\n",
    "            'preferred_input_ids': preferred_inputs['input_ids'].squeeze(),\n",
    "            'preferred_attention_mask': preferred_inputs['attention_mask'].squeeze(),\n",
    "            'rejected_input_ids': rejected_inputs['input_ids'].squeeze(),\n",
    "            'rejected_attention_mask': rejected_inputs['attention_mask'].squeeze(),\n",
    "        }\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "   \n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    #text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    #text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def compute_dpo_loss(policy_chosen_logits, policy_rejected_logits, \n",
    "                    reference_chosen_logits, reference_rejected_logits, \n",
    "                    beta=0.2):\n",
    "   \n",
    "    epsilon = 1e-8\n",
    "    \n",
    "   \n",
    "    policy_chosen_probs = F.softmax(policy_chosen_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    policy_rejected_probs = F.softmax(policy_rejected_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    ref_chosen_probs = F.softmax(reference_chosen_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    ref_rejected_probs = F.softmax(reference_rejected_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    \n",
    "  \n",
    "    chosen_rewards = (torch.log(policy_chosen_probs + epsilon) - \n",
    "                     torch.log(ref_chosen_probs + epsilon))\n",
    "    rejected_rewards = (torch.log(policy_rejected_probs + epsilon) - \n",
    "                       torch.log(ref_rejected_probs + epsilon))\n",
    "    \n",
    "    \n",
    "    max_reward = 50.0\n",
    "    chosen_rewards = torch.clamp(chosen_rewards, -max_reward, max_reward)\n",
    "    rejected_rewards = torch.clamp(rejected_rewards, -max_reward, max_reward)\n",
    "    \n",
    "    \n",
    "    logits_diff = (chosen_rewards - rejected_rewards) / beta\n",
    "    \n",
    "    valid_mask = ~torch.isnan(logits_diff)\n",
    "    if valid_mask.any():\n",
    "        loss = -F.logsigmoid(logits_diff[valid_mask]).mean()\n",
    "    else:\n",
    "        loss = torch.tensor(0.0, device=logits_diff.device)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_model_dpo(policy_model, reference_model, train_loader, val_loader, \n",
    "                   optimizer, scheduler, device, num_epochs=5, beta=0.2, gradient_accumulation_steps=2):\n",
    "    \"\"\"\n",
    "    Train the model using DPO (Direct Preference Optimization)\n",
    "    \n",
    "    Args:\n",
    "        policy_model: The model being trained\n",
    "        reference_model: The reference model for DPO\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        optimizer: Optimizer for training\n",
    "        scheduler: Learning rate scheduler\n",
    "        device: Device to train on (cuda/cpu)\n",
    "        num_epochs: Number of training epochs\n",
    "        beta: Temperature parameter for DPO\n",
    "        gradient_accumulation_steps: Number of steps to accumulate gradients\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    policy_model = policy_model.to(device)\n",
    "    reference_model = reference_model.to(device)\n",
    "    reference_model.eval()\n",
    "    \n",
    "    # Initialize GradScaler for mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        policy_model.train()\n",
    "        total_loss = 0\n",
    "        valid_steps = 0\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            try:\n",
    "                if scaler is not None:\n",
    "                    # Mixed precision training path\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        # Forward pass\n",
    "                        policy_chosen_outputs = policy_model(\n",
    "                            input_ids=batch['preferred_input_ids'],\n",
    "                            attention_mask=batch['preferred_attention_mask']\n",
    "                        )\n",
    "                        policy_rejected_outputs = policy_model(\n",
    "                            input_ids=batch['rejected_input_ids'],\n",
    "                            attention_mask=batch['rejected_attention_mask']\n",
    "                        )\n",
    "                        \n",
    "                        with torch.no_grad():\n",
    "                            ref_chosen_outputs = reference_model(\n",
    "                                input_ids=batch['preferred_input_ids'],\n",
    "                                attention_mask=batch['preferred_attention_mask']\n",
    "                            )\n",
    "                            ref_rejected_outputs = reference_model(\n",
    "                                input_ids=batch['rejected_input_ids'],\n",
    "                                attention_mask=batch['rejected_attention_mask']\n",
    "                            )\n",
    "                        \n",
    "                        # Compute loss\n",
    "                        loss = compute_dpo_loss(\n",
    "                            policy_chosen_outputs.logits,\n",
    "                            policy_rejected_outputs.logits,\n",
    "                            ref_chosen_outputs.logits,\n",
    "                            ref_rejected_outputs.logits,\n",
    "                            beta=beta\n",
    "                        )\n",
    "                        \n",
    "                        if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                            loss = loss / gradient_accumulation_steps\n",
    "                            # Mixed precision backward pass\n",
    "                            scaler.scale(loss).backward()\n",
    "                            \n",
    "                            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                                scaler.unscale_(optimizer)\n",
    "                                torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)\n",
    "                                scaler.step(optimizer)\n",
    "                                scaler.update()\n",
    "                                scheduler.step()\n",
    "                                optimizer.zero_grad()\n",
    "                            \n",
    "                            total_loss += loss.item() * gradient_accumulation_steps\n",
    "                            valid_steps += 1\n",
    "                else:\n",
    "                    # Standard precision training path\n",
    "                    policy_chosen_outputs = policy_model(\n",
    "                        input_ids=batch['preferred_input_ids'],\n",
    "                        attention_mask=batch['preferred_attention_mask']\n",
    "                    )\n",
    "                    policy_rejected_outputs = policy_model(\n",
    "                        input_ids=batch['rejected_input_ids'],\n",
    "                        attention_mask=batch['rejected_attention_mask']\n",
    "                    )\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        ref_chosen_outputs = reference_model(\n",
    "                            input_ids=batch['preferred_input_ids'],\n",
    "                            attention_mask=batch['preferred_attention_mask']\n",
    "                        )\n",
    "                        ref_rejected_outputs = reference_model(\n",
    "                            input_ids=batch['rejected_input_ids'],\n",
    "                            attention_mask=batch['rejected_attention_mask']\n",
    "                        )\n",
    "                    \n",
    "                    loss = compute_dpo_loss(\n",
    "                        policy_chosen_outputs.logits,\n",
    "                        policy_rejected_outputs.logits,\n",
    "                        ref_chosen_outputs.logits,\n",
    "                        ref_rejected_outputs.logits,\n",
    "                        beta=beta\n",
    "                    )\n",
    "                    \n",
    "                    if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                        loss = loss / gradient_accumulation_steps\n",
    "                        loss.backward()\n",
    "                        \n",
    "                        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)\n",
    "                            optimizer.step()\n",
    "                            scheduler.step()\n",
    "                            optimizer.zero_grad()\n",
    "                        \n",
    "                        total_loss += loss.item() * gradient_accumulation_steps\n",
    "                        valid_steps += 1\n",
    "                \n",
    "                if step % 10 == 0:\n",
    "                    avg_loss = total_loss / max(valid_steps, 1)\n",
    "                    print(f\"[Epoch {epoch+1}/{num_epochs} | Step {step}/{len(train_loader)}] - Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {step}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "            \n",
    "            # Memory management\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        if valid_steps > 0:\n",
    "            avg_train_loss = total_loss / valid_steps\n",
    "            val_loss = evaluate_model_dpo(policy_model, reference_model, val_loader, device, beta)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Avg Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = {k: v.cpu() for k, v in policy_model.state_dict().items()}\n",
    "    \n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model_dpo(policy_model, reference_model, val_loader, device, beta):\n",
    "    \n",
    "    policy_model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    policy_chosen_outputs = policy_model(\n",
    "                        input_ids=batch['preferred_input_ids'],\n",
    "                        attention_mask=batch['preferred_attention_mask']\n",
    "                    )\n",
    "                    policy_rejected_outputs = policy_model(\n",
    "                        input_ids=batch['rejected_input_ids'],\n",
    "                        attention_mask=batch['rejected_attention_mask']\n",
    "                    )\n",
    "                    \n",
    "                    ref_chosen_outputs = reference_model(\n",
    "                        input_ids=batch['preferred_input_ids'],\n",
    "                        attention_mask=batch['preferred_attention_mask']\n",
    "                    )\n",
    "                    ref_rejected_outputs = reference_model(\n",
    "                        input_ids=batch['rejected_input_ids'],\n",
    "                        attention_mask=batch['rejected_attention_mask']\n",
    "                    )\n",
    "            else:\n",
    "                policy_chosen_outputs = policy_model(\n",
    "                    input_ids=batch['preferred_input_ids'],\n",
    "                    attention_mask=batch['preferred_attention_mask']\n",
    "                )\n",
    "                policy_rejected_outputs = policy_model(\n",
    "                    input_ids=batch['rejected_input_ids'],\n",
    "                    attention_mask=batch['rejected_attention_mask']\n",
    "                )\n",
    "                \n",
    "                ref_chosen_outputs = reference_model(\n",
    "                    input_ids=batch['preferred_input_ids'],\n",
    "                    attention_mask=batch['preferred_attention_mask']\n",
    "                )\n",
    "                ref_rejected_outputs = reference_model(\n",
    "                    input_ids=batch['rejected_input_ids'],\n",
    "                    attention_mask=batch['rejected_attention_mask']\n",
    "                )\n",
    "            \n",
    "            loss = compute_dpo_loss(\n",
    "                policy_chosen_outputs.logits,\n",
    "                policy_rejected_outputs.logits,\n",
    "                ref_chosen_outputs.logits,\n",
    "                ref_rejected_outputs.logits,\n",
    "                beta=beta\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# Helper function for computing DPO loss\n",
    "def compute_dpo_loss(policy_chosen_logits, policy_rejected_logits, \n",
    "                    reference_chosen_logits, reference_rejected_logits, \n",
    "                    beta=0.2):\n",
    "    \"\"\"\n",
    "    Compute the DPO loss between policy and reference models\n",
    "    \n",
    "    Args:\n",
    "        policy_chosen_logits: Logits from policy model for chosen examples\n",
    "        policy_rejected_logits: Logits from policy model for rejected examples\n",
    "        reference_chosen_logits: Logits from reference model for chosen examples\n",
    "        reference_rejected_logits: Logits from reference model for rejected examples\n",
    "        beta: Temperature parameter\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    # Get probabilities\n",
    "    policy_chosen_probs = F.softmax(policy_chosen_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    policy_rejected_probs = F.softmax(policy_rejected_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    ref_chosen_probs = F.softmax(reference_chosen_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    ref_rejected_probs = F.softmax(reference_rejected_logits, dim=-1)[:, 0].clamp(epsilon, 1-epsilon)\n",
    "    \n",
    "    # Compute rewards\n",
    "    chosen_rewards = (torch.log(policy_chosen_probs + epsilon) - \n",
    "                     torch.log(ref_chosen_probs + epsilon))\n",
    "    rejected_rewards = (torch.log(policy_rejected_probs + epsilon) - \n",
    "                       torch.log(ref_rejected_probs + epsilon))\n",
    "    \n",
    "    # Clamp rewards\n",
    "    max_reward = 50.0\n",
    "    chosen_rewards = torch.clamp(chosen_rewards, -max_reward, max_reward)\n",
    "    rejected_rewards = torch.clamp(rejected_rewards, -max_reward, max_reward)\n",
    "    \n",
    "    # Compute loss\n",
    "    logits_diff = (chosen_rewards - rejected_rewards) / beta\n",
    "    \n",
    "    valid_mask = ~torch.isnan(logits_diff)\n",
    "    if valid_mask.any():\n",
    "        loss = -F.logsigmoid(logits_diff[valid_mask]).mean()\n",
    "    else:\n",
    "        loss = torch.tensor(0.0, device=logits_diff.device)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def main():\n",
    "\n",
    "   \n",
    "    device = setup_environment()\n",
    "    model_name = 'bert-base-uncased'\n",
    "    data_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/final_data.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"File not found: {data_path}\")\n",
    "\n",
    "    policy_model, tokenizer = setup_model_and_tokenizer(model_name, device)\n",
    "    reference_model, _ = setup_model_and_tokenizer(model_name, device)\n",
    "    \n",
    "\n",
    "    emails_df = pd.read_csv(data_path)\n",
    "    emails_df['sender'] = emails_df['sender'].astype(str).apply(clean_text)\n",
    "    emails_df['subject'] = emails_df['subject'].astype(str).apply(clean_text)\n",
    "    emails_df['body'] = emails_df['body'].astype(str).apply(clean_text)\n",
    "\n",
    "    train_df, val_df = train_test_split(emails_df, test_size=0.2, stratify=emails_df['label'], random_state=42)\n",
    "\n",
    "  \n",
    "    train_dataset = PreferenceEmailDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = PreferenceEmailDataset(val_df, tokenizer, max_length=512)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # Setup optimization\n",
    "    optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "    num_epochs = 8\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 20\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    best_model_state = train_model_dpo(\n",
    "        policy_model,\n",
    "        reference_model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        num_epochs=num_epochs,\n",
    "        beta=0.2\n",
    "    )\n",
    "\n",
    "   \n",
    "    output_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/bert_dpo123_classification_model\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    policy_model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"warmup_steps\": num_warmup_steps,\n",
    "        \"total_steps\": num_training_steps,\n",
    "        \"device\": str(device),\n",
    "        \"beta\": 0.2\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399d8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
