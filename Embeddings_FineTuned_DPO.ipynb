{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b45fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f03d0ce36f645959e562c7f2e9c9b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved similarity matrix to sender_similarity_matrix_llama7b_ft_dpo.csv\n",
      "Saved similarity matrix to subject_similarity_matrix_llama7b_ft_dpo.csv\n",
      "Saved similarity matrix to body_similarity_matrix_llama7b_ft_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,LlamaForSequenceClassification\n",
    "import gc\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "def load_fine_tuned_model(model_dir, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_dir, output_hidden_states=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_embeddings(model, tokenizer, text, device, max_length=512):\n",
    "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  \n",
    "        embedding = hidden_states.mean(dim=1).squeeze().cpu().numpy()  \n",
    "    return embedding\n",
    "\n",
    "def extract_embeddings_from_csv(model, tokenizer, csv_path, device, max_length=512):\n",
    "    data = pd.read_excel(csv_path)\n",
    "    sender_embeddings, subject_embeddings, body_embeddings = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, row in data.iterrows():\n",
    "            sender = str(row['Sender'])\n",
    "            subject = str(row['Subject'])\n",
    "            body = str(row['Email'])\n",
    "            \n",
    "           \n",
    "            sender_embeddings.append(get_embeddings(model, tokenizer, sender, device, max_length))\n",
    "            subject_embeddings.append(get_embeddings(model, tokenizer, subject, device, max_length))\n",
    "            body_embeddings.append(get_embeddings(model, tokenizer, body, device, max_length))\n",
    "    \n",
    "    return sender_embeddings, subject_embeddings, body_embeddings\n",
    "\n",
    "\n",
    "def calculate_similarity_matrix(embeddings):\n",
    "    return cosine_similarity(embeddings)\n",
    "\n",
    "\n",
    "def save_matrix_to_csv(matrix, filename):\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved similarity matrix to {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = setup_environment()\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/dpo_7B\")\n",
    "    csv_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "\n",
    "    model, tokenizer = load_fine_tuned_model(model_dir, device)\n",
    "\n",
    "    sender_embeddings, subject_embeddings, body_embeddings = extract_embeddings_from_csv(model, tokenizer, csv_path, device)\n",
    "\n",
    "    sender_similarity_matrix = calculate_similarity_matrix(sender_embeddings)\n",
    "    subject_similarity_matrix = calculate_similarity_matrix(subject_embeddings)\n",
    "    body_similarity_matrix = calculate_similarity_matrix(body_embeddings)\n",
    "    \n",
    "\n",
    "    save_matrix_to_csv(sender_similarity_matrix, \"sender_similarity_matrix_llama7b_ft_dpo.csv\")\n",
    "    save_matrix_to_csv(subject_similarity_matrix, \"subject_similarity_matrix_llama7b_ft_dpo.csv\")\n",
    "    save_matrix_to_csv(body_similarity_matrix, \"body_similarity_matrix_llama7b_ft_dpo.csv\")\n",
    "\n",
    "    # Clean up memory using cache \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dff043b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Using CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afafe99c1dfa45018ec4570ccacccb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved similarity matrix to sender_similarity_matrix_llama8b_ft_dpo.csv\n",
      "Saved similarity matrix to subject_similarity_matrix_llama8b_ft_dpo.csv\n",
      "Saved similarity matrix to body_similarity_matrix_llama8b_ft_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer, AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "def setup_environment(preferred_gpu=None):\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "\n",
    "        if preferred_gpu is not None and preferred_gpu < gpu_count:\n",
    "            os.environ['CUDA_VISIBLE_DEVICES'] = str(preferred_gpu)\n",
    "            device = torch.device(\"cuda:0\")  # first visible GPU\n",
    "            print(f\"Using preferred GPU {preferred_gpu}: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            if preferred_gpu is not None:\n",
    "                print(f\"Preferred GPU {preferred_gpu} not found. Using GPU 0 instead.\")\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            print(f\"Using GPU 0: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "        # CUDA optimizations\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"No GPU found. Using CPU.\")\n",
    "\n",
    "    # Seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_fine_tuned_model(model_dir, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_dir, output_hidden_states=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokenizer, text, device, max_length=512):\n",
    "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.hidden_states[-1] \n",
    "        embedding = hidden_states.mean(dim=1).squeeze().cpu().numpy()  \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def extract_embeddings_from_csv(model, tokenizer, csv_path, device, max_length=512):\n",
    "    data = pd.read_excel(csv_path)\n",
    "    sender_embeddings, subject_embeddings, body_embeddings = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, row in data.iterrows():\n",
    "            sender = str(row['Sender'])\n",
    "            subject = str(row['Subject'])\n",
    "            body = str(row['Email'])\n",
    "            \n",
    "            \n",
    "            sender_embeddings.append(get_embeddings(model, tokenizer, sender, device, max_length))\n",
    "            subject_embeddings.append(get_embeddings(model, tokenizer, subject, device, max_length))\n",
    "            body_embeddings.append(get_embeddings(model, tokenizer, body, device, max_length))\n",
    "    \n",
    "    return sender_embeddings, subject_embeddings, body_embeddings\n",
    "\n",
    "\n",
    "def calculate_similarity_matrix(embeddings):\n",
    "    return cosine_similarity(embeddings)\n",
    "\n",
    "\n",
    "def save_matrix_to_csv(matrix, filename):\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved similarity matrix to {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = setup_environment()\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/dpo_8B\")\n",
    "    csv_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    \n",
    " \n",
    "    model, tokenizer = load_fine_tuned_model(model_dir, device)\n",
    "    \n",
    "   \n",
    "    sender_embeddings, subject_embeddings, body_embeddings = extract_embeddings_from_csv(model, tokenizer, csv_path, device)\n",
    "    \n",
    "  \n",
    "    sender_similarity_matrix = calculate_similarity_matrix(sender_embeddings)\n",
    "    subject_similarity_matrix = calculate_similarity_matrix(subject_embeddings)\n",
    "    body_similarity_matrix = calculate_similarity_matrix(body_embeddings)\n",
    "    \n",
    "   \n",
    "    save_matrix_to_csv(sender_similarity_matrix, \"sender_similarity_matrix_llama8b_ft_dpo.csv\")\n",
    "    save_matrix_to_csv(subject_similarity_matrix, \"subject_similarity_matrix_llama8b_ft_dpo.csv\")\n",
    "    save_matrix_to_csv(body_similarity_matrix, \"body_similarity_matrix_llama8b_ft_dpo.csv\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f42408a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Using CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290a544714fc4f9f826f66acf5c2baf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved similarity matrix to sender_similarity_matrix_mistral_ft_dpo.csv\n",
      "Saved similarity matrix to subject_similarity_matrix_mistral_ft_dpo.csv\n",
      "Saved similarity matrix to body_similarity_matrix_mistral_ft_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer, AutoConfig, AutoTokenizer,AutoModelForSequenceClassification\n",
    "import gc\n",
    "\n",
    "\n",
    "from transformers import LlamaForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def load_fine_tuned_model(model_dir, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_dir, output_hidden_states=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokenizer, text, device, max_length=512):\n",
    "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.hidden_states[-1] \n",
    "        embedding = hidden_states.mean(dim=1).squeeze().cpu().numpy()  \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def extract_embeddings_from_csv(model, tokenizer, csv_path, device, max_length=512):\n",
    "    data = pd.read_excel(csv_path)\n",
    "    sender_embeddings, subject_embeddings, body_embeddings = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, row in data.iterrows():\n",
    "            sender = str(row['Sender'])\n",
    "            subject = str(row['Subject'])\n",
    "            body = str(row['Email'])\n",
    "            \n",
    "            \n",
    "            sender_embeddings.append(get_embeddings(model, tokenizer, sender, device, max_length))\n",
    "            subject_embeddings.append(get_embeddings(model, tokenizer, subject, device, max_length))\n",
    "            body_embeddings.append(get_embeddings(model, tokenizer, body, device, max_length))\n",
    "    \n",
    "    return sender_embeddings, subject_embeddings, body_embeddings\n",
    "\n",
    "\n",
    "def calculate_similarity_matrix(embeddings):\n",
    "    return cosine_similarity(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def save_matrix_to_csv(matrix, filename):\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved similarity matrix to {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = setup_environment()\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/dpo_Mistral\")\n",
    "    csv_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    \n",
    " \n",
    "    model, tokenizer = load_fine_tuned_model(model_dir, device)\n",
    "    \n",
    "   \n",
    "    sender_embeddings, subject_embeddings, body_embeddings = extract_embeddings_from_csv(model, tokenizer, csv_path, device)\n",
    "    \n",
    "  \n",
    "    sender_similarity_matrix = calculate_similarity_matrix(sender_embeddings)\n",
    "    subject_similarity_matrix = calculate_similarity_matrix(subject_embeddings)\n",
    "    body_similarity_matrix = calculate_similarity_matrix(body_embeddings)\n",
    "    \n",
    "   \n",
    "    save_matrix_to_csv(sender_similarity_matrix, \"sender_similarity_matrix_mistral_ft_dpo.csv\")\n",
    "    save_matrix_to_csv(subject_similarity_matrix, \"subject_similarity_matrix_mistral_ft_dpo.csv\")\n",
    "    save_matrix_to_csv(body_similarity_matrix, \"body_similarity_matrix_mistral_ft_dpo.csv\")\n",
    "\n",
    " #  \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb091d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Using CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f71100ce1b747b49b9da55e4d03fca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at dreamgen/WizardLM-2-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved similarity matrix to sender_similarity_matrix_wizard_ft_dpo.csv\n",
      "Saved similarity matrix to subject_similarity_matrix_wizard_ft_dpo.csv\n",
      "Saved similarity matrix to body_similarity_matrix_wizard_ft_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer, AutoConfig, AutoTokenizer,AutoModelForSequenceClassification\n",
    "import gc\n",
    "\n",
    "\n",
    "from transformers import LlamaForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def load_fine_tuned_model(model_dir, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model =AutoModelForSequenceClassification .from_pretrained(model_dir, output_hidden_states=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokenizer, text, device, max_length=512):\n",
    "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.hidden_states[-1] \n",
    "        embedding = hidden_states.mean(dim=1).squeeze().cpu().numpy()  \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def extract_embeddings_from_csv(model, tokenizer, csv_path, device, max_length=512):\n",
    "    data = pd.read_excel(csv_path)\n",
    "    sender_embeddings, subject_embeddings, body_embeddings = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, row in data.iterrows():\n",
    "            sender = str(row['Sender'])\n",
    "            subject = str(row['Subject'])\n",
    "            body = str(row['Email'])\n",
    "            \n",
    "            \n",
    "            sender_embeddings.append(get_embeddings(model, tokenizer, sender, device, max_length))\n",
    "            subject_embeddings.append(get_embeddings(model, tokenizer, subject, device, max_length))\n",
    "            body_embeddings.append(get_embeddings(model, tokenizer, body, device, max_length))\n",
    "    \n",
    "    return sender_embeddings, subject_embeddings, body_embeddings\n",
    "\n",
    "\n",
    "def calculate_similarity_matrix(embeddings):\n",
    "    return cosine_similarity(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def save_matrix_to_csv(matrix, filename):\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved similarity matrix to {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = setup_environment()\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/dpo_7B_Wizard\")\n",
    "    csv_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    \n",
    " \n",
    "    model, tokenizer = load_fine_tuned_model(model_dir, device)\n",
    "    \n",
    "   \n",
    "    sender_embeddings, subject_embeddings, body_embeddings = extract_embeddings_from_csv(model, tokenizer, csv_path, device)\n",
    "    \n",
    "  \n",
    "    sender_similarity_matrix = calculate_similarity_matrix(sender_embeddings)\n",
    "    subject_similarity_matrix = calculate_similarity_matrix(subject_embeddings)\n",
    "    body_similarity_matrix = calculate_similarity_matrix(body_embeddings)\n",
    "    \n",
    "   \n",
    "    save_matrix_to_csv(sender_similarity_matrix, \"sender_similarity_matrix_wizard_ft_dpo.csv\")\n",
    "    save_matrix_to_csv(subject_similarity_matrix, \"subject_similarity_matrix_wizard_ft_dpo.csv\")\n",
    "    save_matrix_to_csv(body_similarity_matrix, \"body_similarity_matrix_wizard_ft_dpo.csv\")\n",
    "\n",
    " #  \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b05fe83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Using CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d2aa0541af4a51b22017d72a4f8e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved similarity matrix to sender_similarity_matrix_qwen_ft_dpo.csv\n",
      "Saved similarity matrix to subject_similarity_matrix_qwen_ft_dpo.csv\n",
      "Saved similarity matrix to body_similarity_matrix_qwen_ft_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer, AutoConfig, AutoTokenizer,AutoModelForSequenceClassification\n",
    "import gc\n",
    "\n",
    "\n",
    "from transformers import LlamaForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def load_fine_tuned_model(model_dir, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model =AutoModelForSequenceClassification .from_pretrained(model_dir, output_hidden_states=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokenizer, text, device, max_length=512):\n",
    "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.hidden_states[-1] \n",
    "        embedding = hidden_states.mean(dim=1).squeeze().cpu().numpy()  \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def extract_embeddings_from_csv(model, tokenizer, csv_path, device, max_length=512):\n",
    "    data = pd.read_excel(csv_path)\n",
    "    sender_embeddings, subject_embeddings, body_embeddings = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, row in data.iterrows():\n",
    "            sender = str(row['Sender'])\n",
    "            subject = str(row['Subject'])\n",
    "            body = str(row['Email'])\n",
    "            \n",
    "            \n",
    "            sender_embeddings.append(get_embeddings(model, tokenizer, sender, device, max_length))\n",
    "            subject_embeddings.append(get_embeddings(model, tokenizer, subject, device, max_length))\n",
    "            body_embeddings.append(get_embeddings(model, tokenizer, body, device, max_length))\n",
    "    \n",
    "    return sender_embeddings, subject_embeddings, body_embeddings\n",
    "\n",
    "\n",
    "def calculate_similarity_matrix(embeddings):\n",
    "    return cosine_similarity(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def save_matrix_to_csv(matrix, filename):\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved similarity matrix to {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = setup_environment()\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/dpo_Qwen\")\n",
    "    csv_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    \n",
    " \n",
    "    model, tokenizer = load_fine_tuned_model(model_dir, device)\n",
    "    \n",
    "   \n",
    "    sender_embeddings, subject_embeddings, body_embeddings = extract_embeddings_from_csv(model, tokenizer, csv_path, device)\n",
    "    \n",
    "  \n",
    "    sender_similarity_matrix = calculate_similarity_matrix(sender_embeddings)\n",
    "    subject_similarity_matrix = calculate_similarity_matrix(subject_embeddings)\n",
    "    body_similarity_matrix = calculate_similarity_matrix(body_embeddings)\n",
    "    \n",
    "   \n",
    "    save_matrix_to_csv(sender_similarity_matrix, \"sender_similarity_matrix_qwen_ft_dpo.csv\")\n",
    "    save_matrix_to_csv(subject_similarity_matrix, \"subject_similarity_matrix_qwen_ft_dpo.csv\")\n",
    "    save_matrix_to_csv(body_similarity_matrix, \"body_similarity_matrix_qwen_ft_dpo.csv\")\n",
    "\n",
    " #  \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b88a39d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Using CPU.\n",
      "Saved similarity matrix to sender_similarity_matrix_bert_ft_dpo.csv\n",
      "Saved similarity matrix to subject_similarity_matrix_bert_ft_dpo.csv\n",
      "Saved similarity matrix to body_similarity_matrix_bert_ft_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer, AutoConfig, AutoTokenizer,AutoModelForSequenceClassification\n",
    "import gc\n",
    "\n",
    "\n",
    "from transformers import LlamaForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def load_fine_tuned_model(model_dir, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model =AutoModelForSequenceClassification .from_pretrained(model_dir, output_hidden_states=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokenizer, text, device, max_length=512):\n",
    "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.hidden_states[-1] \n",
    "        embedding = hidden_states.mean(dim=1).squeeze().cpu().numpy()  \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def extract_embeddings_from_csv(model, tokenizer, csv_path, device, max_length=512):\n",
    "    data = pd.read_excel(csv_path)\n",
    "    sender_embeddings, subject_embeddings, body_embeddings = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, row in data.iterrows():\n",
    "            sender = str(row['Sender'])\n",
    "            subject = str(row['Subject'])\n",
    "            body = str(row['Email'])\n",
    "            \n",
    "            \n",
    "            sender_embeddings.append(get_embeddings(model, tokenizer, sender, device, max_length))\n",
    "            subject_embeddings.append(get_embeddings(model, tokenizer, subject, device, max_length))\n",
    "            body_embeddings.append(get_embeddings(model, tokenizer, body, device, max_length))\n",
    "    \n",
    "    return sender_embeddings, subject_embeddings, body_embeddings\n",
    "\n",
    "\n",
    "def calculate_similarity_matrix(embeddings):\n",
    "    return cosine_similarity(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def save_matrix_to_csv(matrix, filename):\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved similarity matrix to {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = setup_environment()\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/dpo_bert_uncased\")\n",
    "    csv_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    \n",
    " \n",
    "    model, tokenizer = load_fine_tuned_model(model_dir, device)\n",
    "    \n",
    "   \n",
    "    sender_embeddings, subject_embeddings, body_embeddings = extract_embeddings_from_csv(model, tokenizer, csv_path, device)\n",
    "    \n",
    "  \n",
    "    sender_similarity_matrix = calculate_similarity_matrix(sender_embeddings)\n",
    "    subject_similarity_matrix = calculate_similarity_matrix(subject_embeddings)\n",
    "    body_similarity_matrix = calculate_similarity_matrix(body_embeddings)\n",
    "    \n",
    "   \n",
    "    save_matrix_to_csv(sender_similarity_matrix, \"sender_similarity_matrix_bert_ft_dpo.csv\")\n",
    "    save_matrix_to_csv(subject_similarity_matrix, \"subject_similarity_matrix_bert_ft_dpo.csv\")\n",
    "    save_matrix_to_csv(body_similarity_matrix, \"body_similarity_matrix_bert_ft_dpo.csv\")\n",
    "\n",
    " #  \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bd96f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA device available. Falling back to CPU.\n",
      "Detected adapter in: /home/users/skuikel/Downloads/Tune/FineTune/dpo_7B\n",
      "Loading base from:   meta-llama/Llama-2-7b-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e13a882d25345fba3ef71234f58dc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base + adapter.\n",
      "Embedding Sender…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sender:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Subject…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Body…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Body:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA reducing…\n",
      "Computing cosine similarity & saving…\n",
      "Saved: sender_similarity_matrix_llama7b_ft_pca_dpo.csv\n",
      "Saved: subject_similarity_matrix_llama7b_ft_pca_dpo.csv\n",
      "Saved: body_similarity_matrix_llama7b_ft_pca_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "# pip install -U \"transformers>=4.41\" \"peft>=0.11.1\" pandas numpy scikit-learn torch tqdm\n",
    "\n",
    "import os, gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "MODEL_DIR        = os.path.expanduser(\"~/Downloads/Tune/FineTune/dpo_7B\")  # full model OR adapter folder\n",
    "BASE_MODEL_ID    = os.environ.get(\"BASE_MODEL_ID\", \"meta-llama/Llama-2-7b-hf\")     # used only if MODEL_DIR is an adapter\n",
    "BASE_MODEL_LOCAL = os.environ.get(\"BASE_MODEL_LOCAL\")                               # local base path (preferred if you have it)\n",
    "XLSX_PATH        = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "\n",
    "BATCH_SIZE_GPU   = 32\n",
    "BATCH_SIZE_CPU   = 4\n",
    "\n",
    "MAXLEN_SENDER    = 48\n",
    "MAXLEN_SUBJECT   = 64\n",
    "MAXLEN_BODY      = 384\n",
    "TARGET_DIM       = 200\n",
    "\n",
    "OUT_SENDER       = \"sender_similarity_matrix_llama7b_ft_pca_dpo.csv\"\n",
    "OUT_SUBJECT      = \"subject_similarity_matrix_llama7b_ft_pca_dpo.csv\"\n",
    "OUT_BODY         = \"body_similarity_matrix_llama7b_ft_pca_dpo.csv\"\n",
    "\n",
    "# -------------- SETUP --------------\n",
    "def setup_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(f\"Using device: {torch.cuda.get_device_name(None)}\")\n",
    "        print(f\"Total CUDA devices: {torch.cuda.device_count()}\")\n",
    "        return device\n",
    "    print(\"No CUDA device available. Falling back to CPU.\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def _is_adapter(path: str) -> bool:\n",
    "    return os.path.isfile(os.path.join(path, \"adapter_config.json\"))\n",
    "\n",
    "def _load_tokenizer(src: str):\n",
    "    tok = AutoTokenizer.from_pretrained(src, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok\n",
    "\n",
    "def load_model_and_tokenizer(device):\n",
    "   \n",
    "    if _is_adapter(MODEL_DIR):\n",
    "        base_src = BASE_MODEL_LOCAL if BASE_MODEL_LOCAL else BASE_MODEL_ID\n",
    "        print(f\"Detected adapter in: {MODEL_DIR}\")\n",
    "        print(f\"Loading base from:   {base_src}\")\n",
    "        tok = _load_tokenizer(base_src)\n",
    "        base = AutoModelForCausalLM.from_pretrained(\n",
    "            base_src,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        ).to(device).eval()\n",
    "\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(base, MODEL_DIR).eval()\n",
    "        print(\"Loaded base + adapter.\")\n",
    "        return model, tok\n",
    "    else:\n",
    "        print(f\"Loading full model from: {MODEL_DIR}\")\n",
    "        tok = _load_tokenizer(MODEL_DIR)\n",
    "        model = AutoModel.from_pretrained(\n",
    "            MODEL_DIR,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        ).to(device).eval()\n",
    "        print(\"Loaded full model.\")\n",
    "        return model, tok\n",
    "\n",
    "@torch.inference_mode()\n",
    "def embed_batch(model, tok, texts, device, max_length):\n",
    "    enc = tok(\n",
    "        [\"\" if t is None else str(t) for t in texts],\n",
    "        padding=True,                 # pad to longest in batch\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc = {k: v.to(device, non_blocking=True) for k, v in enc.items()}\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            out = model(**enc, output_hidden_states=True, use_cache=False)\n",
    "    else:\n",
    "        out = model(**enc, output_hidden_states=True, use_cache=False)\n",
    "\n",
    "    # Robustly get the final hidden states:\n",
    "    if hasattr(out, \"last_hidden_state\") and out.last_hidden_state is not None:\n",
    "        last = out.last_hidden_state\n",
    "    else:\n",
    "        # CausalLM returns CausalLMOutputWithPast (no last_hidden_state). Use hidden_states[-1].\n",
    "        if getattr(out, \"hidden_states\", None) is None:\n",
    "            raise RuntimeError(\"Model did not return hidden states. Ensure output_hidden_states=True.\")\n",
    "        last = out.hidden_states[-1]  # [B, T, H]\n",
    "\n",
    "    mask = enc[\"attention_mask\"].unsqueeze(-1)    # [B, T, 1]\n",
    "    pooled = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "    return pooled.detach().cpu().numpy()          # [B, H]\n",
    "\n",
    "def embed_series(model, tok, series: pd.Series, device, max_length, batch_size, desc=\"Embedding\"):\n",
    "    vals = series.astype(str).fillna(\"\").tolist()\n",
    "    # De-dup for speed\n",
    "    uniq = list(dict.fromkeys(vals))\n",
    "    cache = {}\n",
    "\n",
    "    for i in tqdm(range(0, len(uniq), batch_size), desc=desc, leave=False):\n",
    "        batch = uniq[i:i+batch_size]\n",
    "        vecs  = embed_batch(model, tok, batch, device, max_length)\n",
    "        for t, v in zip(batch, vecs):\n",
    "            cache[t] = v\n",
    "\n",
    "    # Map back to original order\n",
    "    return np.stack([cache[v] for v in vals], axis=0)\n",
    "\n",
    "# -------------- PCA + SIM --------------\n",
    "def pca_reduce(X: np.ndarray, target_dim: int) -> np.ndarray:\n",
    "    n_samples, n_features = X.shape\n",
    "    n_components = min(target_dim, n_samples, n_features)\n",
    "    if n_components < target_dim:\n",
    "        print(f\"PCA components clipped to {n_components} (samples={n_samples}, features={n_features}).\")\n",
    "    return PCA(n_components=n_components, random_state=42).fit_transform(X)\n",
    "\n",
    "def save_csv(matrix: np.ndarray, path: str):\n",
    "    pd.DataFrame(matrix).to_csv(path, index=False)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "# -------------- MAIN --------------\n",
    "def main():\n",
    "    device = setup_device()\n",
    "\n",
    "    # free mem\n",
    "    if device.type == \"cuda\":\n",
    "        batch_size = BATCH_SIZE_GPU\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        batch_size = BATCH_SIZE_CPU\n",
    "    gc.collect()\n",
    "\n",
    "    model, tok = load_model_and_tokenizer(device)\n",
    "\n",
    "    # Read data\n",
    "    try:\n",
    "        df = pd.read_excel(XLSX_PATH)\n",
    "    except Exception:\n",
    "        df = pd.read_excel(XLSX_PATH, engine=\"openpyxl\")\n",
    "\n",
    "    for col in [\"Sender\", \"Subject\", \"Email\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Required column '{col}' not found. Got: {list(df.columns)}\")\n",
    "\n",
    "    print(\"Embedding Sender…\")\n",
    "    sender_emb  = embed_series(model, tok, df[\"Sender\"],  device, MAXLEN_SENDER,  batch_size, desc=\"Sender\")\n",
    "    print(\"Embedding Subject…\")\n",
    "    subject_emb = embed_series(model, tok, df[\"Subject\"], device, MAXLEN_SUBJECT, batch_size, desc=\"Subject\")\n",
    "    print(\"Embedding Body…\")\n",
    "    body_emb    = embed_series(model, tok, df[\"Email\"],   device, MAXLEN_BODY,    batch_size, desc=\"Body\")\n",
    "\n",
    "    print(\"PCA reducing…\")\n",
    "    sender_r  = pca_reduce(sender_emb,  TARGET_DIM)\n",
    "    subject_r = pca_reduce(subject_emb, TARGET_DIM)\n",
    "    body_r    = pca_reduce(body_emb,    TARGET_DIM)\n",
    "\n",
    "    print(\"Computing cosine similarity & saving…\")\n",
    "    save_csv(cosine_similarity(sender_r),  OUT_SENDER)\n",
    "    save_csv(cosine_similarity(subject_r), OUT_SUBJECT)\n",
    "    save_csv(cosine_similarity(body_r),    OUT_BODY)\n",
    "\n",
    "    gc.collect()\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b24ece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected adapter in: /home/users/skuikel/Downloads/Tune/FineTune/dpo_8B\n",
      "Loading base from:   meta-llama/Meta-Llama-3-8B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4c426d2afd432baa7765144a7a5725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base + adapter.\n",
      "Embedding Sender…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sender:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Subject…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Body…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Body:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA reducing…\n",
      "Computing cosine similarity & saving…\n",
      "Saved: sender_similarity_matrix_llama8b_ft_pca_dpo.csv\n",
      "Saved: subject_similarity_matrix_llama8b_ft_pca_dpo.csv\n",
      "Saved: body_similarity_matrix_llama8b_ft_pca_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "MODEL_DIR        = os.path.expanduser(\"~/Downloads/Tune/FineTune/dpo_8B\")  # full model OR adapter folder\n",
    "BASE_MODEL_ID    = os.environ.get(\"BASE_MODEL_ID\", \"meta-llama/Meta-Llama-3-8B\")     # used only if MODEL_DIR is an adapter\n",
    "BASE_MODEL_LOCAL = os.environ.get(\"BASE_MODEL_LOCAL\")                               # local base path (preferred if you have it)\n",
    "XLSX_PATH        = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "\n",
    "BATCH_SIZE_GPU   = 32\n",
    "BATCH_SIZE_CPU   = 4\n",
    "\n",
    "MAXLEN_SENDER    = 48\n",
    "MAXLEN_SUBJECT   = 64\n",
    "MAXLEN_BODY      = 384\n",
    "TARGET_DIM       = 200\n",
    "\n",
    "OUT_SENDER       = \"sender_similarity_matrix_llama8b_ft_pca_dpo.csv\"\n",
    "OUT_SUBJECT      = \"subject_similarity_matrix_llama8b_ft_pca_dpo.csv\"\n",
    "OUT_BODY         = \"body_similarity_matrix_llama8b_ft_pca_dpo.csv\"\n",
    "\n",
    "def setup_device():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "def _is_adapter(path: str) -> bool:\n",
    "    return os.path.isfile(os.path.join(path, \"adapter_config.json\"))\n",
    "\n",
    "def _load_tokenizer(src: str):\n",
    "    tok = AutoTokenizer.from_pretrained(src, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok\n",
    "\n",
    "def load_model_and_tokenizer(device):\n",
    "   \n",
    "    if _is_adapter(MODEL_DIR):\n",
    "        base_src = BASE_MODEL_LOCAL if BASE_MODEL_LOCAL else BASE_MODEL_ID\n",
    "        print(f\"Detected adapter in: {MODEL_DIR}\")\n",
    "        print(f\"Loading base from:   {base_src}\")\n",
    "        tok = _load_tokenizer(base_src)\n",
    "        base = AutoModelForCausalLM.from_pretrained(\n",
    "            base_src,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        ).to(device).eval()\n",
    "\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(base, MODEL_DIR).eval()\n",
    "        print(\"Loaded base + adapter.\")\n",
    "        return model, tok\n",
    "    else:\n",
    "        print(f\"Loading full model from: {MODEL_DIR}\")\n",
    "        tok = _load_tokenizer(MODEL_DIR)\n",
    "        model = AutoModel.from_pretrained(\n",
    "            MODEL_DIR,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        ).to(device).eval()\n",
    "        print(\"Loaded full model.\")\n",
    "        return model, tok\n",
    "\n",
    "@torch.inference_mode()\n",
    "def embed_batch(model, tok, texts, device, max_length):\n",
    "    enc = tok(\n",
    "        [\"\" if t is None else str(t) for t in texts],\n",
    "        padding=True,                 # pad to longest in batch\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc = {k: v.to(device, non_blocking=True) for k, v in enc.items()}\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            out = model(**enc, output_hidden_states=True, use_cache=False)\n",
    "    else:\n",
    "        out = model(**enc, output_hidden_states=True, use_cache=False)\n",
    "\n",
    "    # Robustly get the final hidden states:\n",
    "    if hasattr(out, \"last_hidden_state\") and out.last_hidden_state is not None:\n",
    "        last = out.last_hidden_state\n",
    "    else:\n",
    "        # CausalLM returns CausalLMOutputWithPast (no last_hidden_state). Use hidden_states[-1].\n",
    "        if getattr(out, \"hidden_states\", None) is None:\n",
    "            raise RuntimeError(\"Model did not return hidden states. Ensure output_hidden_states=True.\")\n",
    "        last = out.hidden_states[-1]  # [B, T, H]\n",
    "\n",
    "    mask = enc[\"attention_mask\"].unsqueeze(-1)    # [B, T, 1]\n",
    "    pooled = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "    return pooled.detach().cpu().numpy()          # [B, H]\n",
    "\n",
    "def embed_series(model, tok, series: pd.Series, device, max_length, batch_size, desc=\"Embedding\"):\n",
    "    vals = series.astype(str).fillna(\"\").tolist()\n",
    "    # De-dup for speed\n",
    "    uniq = list(dict.fromkeys(vals))\n",
    "    cache = {}\n",
    "\n",
    "    for i in tqdm(range(0, len(uniq), batch_size), desc=desc, leave=False):\n",
    "        batch = uniq[i:i+batch_size]\n",
    "        vecs  = embed_batch(model, tok, batch, device, max_length)\n",
    "        for t, v in zip(batch, vecs):\n",
    "            cache[t] = v\n",
    "\n",
    "    \n",
    "    return np.stack([cache[v] for v in vals], axis=0)\n",
    "\n",
    "\n",
    "def pca_reduce(X: np.ndarray, target_dim: int) -> np.ndarray:\n",
    "    n_samples, n_features = X.shape\n",
    "    n_components = min(target_dim, n_samples, n_features)\n",
    "    if n_components < target_dim:\n",
    "        print(f\"PCA components clipped to {n_components} (samples={n_samples}, features={n_features}).\")\n",
    "    return PCA(n_components=n_components, random_state=42).fit_transform(X)\n",
    "\n",
    "def save_csv(matrix: np.ndarray, path: str):\n",
    "    pd.DataFrame(matrix).to_csv(path, index=False)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "def main():\n",
    "    device = setup_device()\n",
    "\n",
    "    # free mem\n",
    "    if device.type == \"cuda\":\n",
    "        batch_size = BATCH_SIZE_GPU\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        batch_size = BATCH_SIZE_CPU\n",
    "    gc.collect()\n",
    "\n",
    "    model, tok = load_model_and_tokenizer(device)\n",
    "\n",
    "    # Read data\n",
    "    try:\n",
    "        df = pd.read_excel(XLSX_PATH)\n",
    "    except Exception:\n",
    "        df = pd.read_excel(XLSX_PATH, engine=\"openpyxl\")\n",
    "\n",
    "    for col in [\"Sender\", \"Subject\", \"Email\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Required column '{col}' not found. Got: {list(df.columns)}\")\n",
    "\n",
    "    print(\"Embedding Sender…\")\n",
    "    sender_emb  = embed_series(model, tok, df[\"Sender\"],  device, MAXLEN_SENDER,  batch_size, desc=\"Sender\")\n",
    "    print(\"Embedding Subject…\")\n",
    "    subject_emb = embed_series(model, tok, df[\"Subject\"], device, MAXLEN_SUBJECT, batch_size, desc=\"Subject\")\n",
    "    print(\"Embedding Body…\")\n",
    "    body_emb    = embed_series(model, tok, df[\"Email\"],   device, MAXLEN_BODY,    batch_size, desc=\"Body\")\n",
    "\n",
    "    print(\"PCA reducing…\")\n",
    "    sender_r  = pca_reduce(sender_emb,  TARGET_DIM)\n",
    "    subject_r = pca_reduce(subject_emb, TARGET_DIM)\n",
    "    body_r    = pca_reduce(body_emb,    TARGET_DIM)\n",
    "\n",
    "    print(\"Computing cosine similarity & saving…\")\n",
    "    save_csv(cosine_similarity(sender_r),  OUT_SENDER)\n",
    "    save_csv(cosine_similarity(subject_r), OUT_SUBJECT)\n",
    "    save_csv(cosine_similarity(body_r),    OUT_BODY)\n",
    "\n",
    "    gc.collect()\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519fbbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected adapter in: /home/users/skuikel/Downloads/Tune/FineTune/dpo_Mistral\n",
      "Loading base from:   mistralai/Mistral-7B-v0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68675e41461e42f09351724c928da5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base + adapter.\n",
      "Embedding Sender…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sender:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Subject…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Body…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Body:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA reducing…\n",
      "Computing cosine similarity & saving…\n",
      "Saved: sender_similarity_matrix_mistral_ft_pca_dpo.csv\n",
      "Saved: subject_similarity_matrix_mistral_ft_pca_dpo.csv\n",
      "Saved: body_similarity_matrix_mistral_ft_pca_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "MODEL_DIR        = os.path.expanduser(\"~/Downloads/Tune/FineTune/dpo_Mistral\")  # full model OR adapter folder\n",
    "BASE_MODEL_ID    = os.environ.get(\"BASE_MODEL_ID\", \"mistralai/Mistral-7B-v0.1\")     # used only if MODEL_DIR is an adapter\n",
    "BASE_MODEL_LOCAL = os.environ.get(\"BASE_MODEL_LOCAL\")                               # local base path (preferred if you have it)\n",
    "XLSX_PATH        = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "\n",
    "BATCH_SIZE_GPU   = 32\n",
    "BATCH_SIZE_CPU   = 4\n",
    "\n",
    "MAXLEN_SENDER    = 48\n",
    "MAXLEN_SUBJECT   = 64\n",
    "MAXLEN_BODY      = 384\n",
    "TARGET_DIM       = 200\n",
    "\n",
    "OUT_SENDER       = \"sender_similarity_matrix_mistral_ft_pca_dpo.csv\"\n",
    "OUT_SUBJECT      = \"subject_similarity_matrix_mistral_ft_pca_dpo.csv\"\n",
    "OUT_BODY         = \"body_similarity_matrix_mistral_ft_pca_dpo.csv\"\n",
    "\n",
    "def setup_device():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "def _is_adapter(path: str) -> bool:\n",
    "    return os.path.isfile(os.path.join(path, \"adapter_config.json\"))\n",
    "\n",
    "def _load_tokenizer(src: str):\n",
    "    tok = AutoTokenizer.from_pretrained(src, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok\n",
    "\n",
    "def load_model_and_tokenizer(device):\n",
    "   \n",
    "    if _is_adapter(MODEL_DIR):\n",
    "        base_src = BASE_MODEL_LOCAL if BASE_MODEL_LOCAL else BASE_MODEL_ID\n",
    "        print(f\"Detected adapter in: {MODEL_DIR}\")\n",
    "        print(f\"Loading base from:   {base_src}\")\n",
    "        tok = _load_tokenizer(base_src)\n",
    "        base = AutoModelForCausalLM.from_pretrained(\n",
    "            base_src,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        ).to(device).eval()\n",
    "\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(base, MODEL_DIR).eval()\n",
    "        print(\"Loaded base + adapter.\")\n",
    "        return model, tok\n",
    "    else:\n",
    "        print(f\"Loading full model from: {MODEL_DIR}\")\n",
    "        tok = _load_tokenizer(MODEL_DIR)\n",
    "        model = AutoModel.from_pretrained(\n",
    "            MODEL_DIR,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        ).to(device).eval()\n",
    "        print(\"Loaded full model.\")\n",
    "        return model, tok\n",
    "\n",
    "@torch.inference_mode()\n",
    "def embed_batch(model, tok, texts, device, max_length):\n",
    "    enc = tok(\n",
    "        [\"\" if t is None else str(t) for t in texts],\n",
    "        padding=True,                 # pad to longest in batch\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc = {k: v.to(device, non_blocking=True) for k, v in enc.items()}\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            out = model(**enc, output_hidden_states=True, use_cache=False)\n",
    "    else:\n",
    "        out = model(**enc, output_hidden_states=True, use_cache=False)\n",
    "\n",
    "    # Robustly get the final hidden states:\n",
    "    if hasattr(out, \"last_hidden_state\") and out.last_hidden_state is not None:\n",
    "        last = out.last_hidden_state\n",
    "    else:\n",
    "        # CausalLM returns CausalLMOutputWithPast (no last_hidden_state). Use hidden_states[-1].\n",
    "        if getattr(out, \"hidden_states\", None) is None:\n",
    "            raise RuntimeError(\"Model did not return hidden states. Ensure output_hidden_states=True.\")\n",
    "        last = out.hidden_states[-1]  # [B, T, H]\n",
    "\n",
    "    mask = enc[\"attention_mask\"].unsqueeze(-1)    # [B, T, 1]\n",
    "    pooled = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "    return pooled.detach().cpu().numpy()          # [B, H]\n",
    "\n",
    "def embed_series(model, tok, series: pd.Series, device, max_length, batch_size, desc=\"Embedding\"):\n",
    "    vals = series.astype(str).fillna(\"\").tolist()\n",
    "    # De-dup for speed\n",
    "    uniq = list(dict.fromkeys(vals))\n",
    "    cache = {}\n",
    "\n",
    "    for i in tqdm(range(0, len(uniq), batch_size), desc=desc, leave=False):\n",
    "        batch = uniq[i:i+batch_size]\n",
    "        vecs  = embed_batch(model, tok, batch, device, max_length)\n",
    "        for t, v in zip(batch, vecs):\n",
    "            cache[t] = v\n",
    "\n",
    "    \n",
    "    return np.stack([cache[v] for v in vals], axis=0)\n",
    "\n",
    "\n",
    "def pca_reduce(X: np.ndarray, target_dim: int) -> np.ndarray:\n",
    "    n_samples, n_features = X.shape\n",
    "    n_components = min(target_dim, n_samples, n_features)\n",
    "    if n_components < target_dim:\n",
    "        print(f\"PCA components clipped to {n_components} (samples={n_samples}, features={n_features}).\")\n",
    "    return PCA(n_components=n_components, random_state=42).fit_transform(X)\n",
    "\n",
    "def save_csv(matrix: np.ndarray, path: str):\n",
    "    pd.DataFrame(matrix).to_csv(path, index=False)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "def main():\n",
    "    device = setup_device()\n",
    "\n",
    "    # free mem\n",
    "    if device.type == \"cuda\":\n",
    "        batch_size = BATCH_SIZE_GPU\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        batch_size = BATCH_SIZE_CPU\n",
    "    gc.collect()\n",
    "\n",
    "    model, tok = load_model_and_tokenizer(device)\n",
    "\n",
    "    # Read data\n",
    "    try:\n",
    "        df = pd.read_excel(XLSX_PATH)\n",
    "    except Exception:\n",
    "        df = pd.read_excel(XLSX_PATH, engine=\"openpyxl\")\n",
    "\n",
    "    for col in [\"Sender\", \"Subject\", \"Email\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Required column '{col}' not found. Got: {list(df.columns)}\")\n",
    "\n",
    "    print(\"Embedding Sender…\")\n",
    "    sender_emb  = embed_series(model, tok, df[\"Sender\"],  device, MAXLEN_SENDER,  batch_size, desc=\"Sender\")\n",
    "    print(\"Embedding Subject…\")\n",
    "    subject_emb = embed_series(model, tok, df[\"Subject\"], device, MAXLEN_SUBJECT, batch_size, desc=\"Subject\")\n",
    "    print(\"Embedding Body…\")\n",
    "    body_emb    = embed_series(model, tok, df[\"Email\"],   device, MAXLEN_BODY,    batch_size, desc=\"Body\")\n",
    "\n",
    "    print(\"PCA reducing…\")\n",
    "    sender_r  = pca_reduce(sender_emb,  TARGET_DIM)\n",
    "    subject_r = pca_reduce(subject_emb, TARGET_DIM)\n",
    "    body_r    = pca_reduce(body_emb,    TARGET_DIM)\n",
    "\n",
    "    print(\"Computing cosine similarity & saving…\")\n",
    "    save_csv(cosine_similarity(sender_r),  OUT_SENDER)\n",
    "    save_csv(cosine_similarity(subject_r), OUT_SUBJECT)\n",
    "    save_csv(cosine_similarity(body_r),    OUT_BODY)\n",
    "\n",
    "    gc.collect()\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62942c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected adapter in: /home/users/skuikel/Downloads/Tune/FineTune/dpo_7B_Wizard\n",
      "Loading base from:   dreamgen/WizardLM-2-7B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ce23662e954605b1d8225377bc81fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base + adapter.\n",
      "Embedding Sender…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sender:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Subject…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Body…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Body:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA reducing…\n",
      "Computing cosine similarity & saving…\n",
      "Saved: sender_similarity_matrix_wizard_ft_pca_dpo.csv\n",
      "Saved: subject_similarity_matrix_wizard_ft_pca_dpo.csv\n",
      "Saved: body_similarity_matrix_wizard_ft_pca_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "MODEL_DIR        = os.path.expanduser(\"~/Downloads/Tune/FineTune/dpo_7B_Wizard\")  # full model OR adapter folder\n",
    "BASE_MODEL_ID    = os.environ.get(\"BASE_MODEL_ID\", \"dreamgen/WizardLM-2-7B\")     # used only if MODEL_DIR is an adapter\n",
    "BASE_MODEL_LOCAL = os.environ.get(\"BASE_MODEL_LOCAL\")                               # local base path (preferred if you have it)\n",
    "XLSX_PATH        = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "\n",
    "BATCH_SIZE_CPU   = 4\n",
    "\n",
    "MAXLEN_SENDER    = 48\n",
    "MAXLEN_SUBJECT   = 64\n",
    "MAXLEN_BODY      = 384\n",
    "TARGET_DIM       = 200\n",
    "\n",
    "OUT_SENDER       = \"sender_similarity_matrix_wizard_ft_pca_dpo.csv\"\n",
    "OUT_SUBJECT      = \"subject_similarity_matrix_wizard_ft_pca_dpo.csv\"\n",
    "OUT_BODY         = \"body_similarity_matrix_wizard_ft_pca_dpo.csv\"\n",
    "\n",
    "def setup_device():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "def _is_adapter(path: str) -> bool:\n",
    "    return os.path.isfile(os.path.join(path, \"adapter_config.json\"))\n",
    "\n",
    "def _load_tokenizer(src: str):\n",
    "    tok = AutoTokenizer.from_pretrained(src, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok\n",
    "\n",
    "def load_model_and_tokenizer(device):\n",
    "   \n",
    "    if _is_adapter(MODEL_DIR):\n",
    "        base_src = BASE_MODEL_LOCAL if BASE_MODEL_LOCAL else BASE_MODEL_ID\n",
    "        print(f\"Detected adapter in: {MODEL_DIR}\")\n",
    "        print(f\"Loading base from:   {base_src}\")\n",
    "        tok = _load_tokenizer(base_src)\n",
    "        base = AutoModelForCausalLM.from_pretrained(\n",
    "            base_src,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        ).to(device).eval()\n",
    "\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(base, MODEL_DIR).eval()\n",
    "        print(\"Loaded base + adapter.\")\n",
    "        return model, tok\n",
    "    else:\n",
    "        print(f\"Loading full model from: {MODEL_DIR}\")\n",
    "        tok = _load_tokenizer(MODEL_DIR)\n",
    "        model = AutoModel.from_pretrained(\n",
    "            MODEL_DIR,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        ).to(device).eval()\n",
    "        print(\"Loaded full model.\")\n",
    "        return model, tok\n",
    "\n",
    "@torch.inference_mode()\n",
    "def embed_batch(model, tok, texts, device, max_length):\n",
    "    enc = tok(\n",
    "        [\"\" if t is None else str(t) for t in texts],\n",
    "        padding=True,                 # pad to longest in batch\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc = {k: v.to(device, non_blocking=True) for k, v in enc.items()}\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            out = model(**enc, output_hidden_states=True, use_cache=False)\n",
    "    else:\n",
    "        out = model(**enc, output_hidden_states=True, use_cache=False)\n",
    "\n",
    "    # Robustly get the final hidden states:\n",
    "    if hasattr(out, \"last_hidden_state\") and out.last_hidden_state is not None:\n",
    "        last = out.last_hidden_state\n",
    "    else:\n",
    "        # CausalLM returns CausalLMOutputWithPast (no last_hidden_state). Use hidden_states[-1].\n",
    "        if getattr(out, \"hidden_states\", None) is None:\n",
    "            raise RuntimeError(\"Model did not return hidden states. Ensure output_hidden_states=True.\")\n",
    "        last = out.hidden_states[-1]  # [B, T, H]\n",
    "\n",
    "    mask = enc[\"attention_mask\"].unsqueeze(-1)    # [B, T, 1]\n",
    "    pooled = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "    return pooled.detach().cpu().numpy()          # [B, H]\n",
    "\n",
    "def embed_series(model, tok, series: pd.Series, device, max_length, batch_size, desc=\"Embedding\"):\n",
    "    vals = series.astype(str).fillna(\"\").tolist()\n",
    "    # De-dup for speed\n",
    "    uniq = list(dict.fromkeys(vals))\n",
    "    cache = {}\n",
    "\n",
    "    for i in tqdm(range(0, len(uniq), batch_size), desc=desc, leave=False):\n",
    "        batch = uniq[i:i+batch_size]\n",
    "        vecs  = embed_batch(model, tok, batch, device, max_length)\n",
    "        for t, v in zip(batch, vecs):\n",
    "            cache[t] = v\n",
    "\n",
    "    \n",
    "    return np.stack([cache[v] for v in vals], axis=0)\n",
    "\n",
    "\n",
    "def pca_reduce(X: np.ndarray, target_dim: int) -> np.ndarray:\n",
    "    n_samples, n_features = X.shape\n",
    "    n_components = min(target_dim, n_samples, n_features)\n",
    "    if n_components < target_dim:\n",
    "        print(f\"PCA components clipped to {n_components} (samples={n_samples}, features={n_features}).\")\n",
    "    return PCA(n_components=n_components, random_state=42).fit_transform(X)\n",
    "\n",
    "def save_csv(matrix: np.ndarray, path: str):\n",
    "    pd.DataFrame(matrix).to_csv(path, index=False)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "def main():\n",
    "    device = setup_device()\n",
    "\n",
    "    # free mem\n",
    "    if device.type == \"cuda\":\n",
    "        batch_size = BATCH_SIZE_GPU\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        batch_size = BATCH_SIZE_CPU\n",
    "    gc.collect()\n",
    "\n",
    "    model, tok = load_model_and_tokenizer(device)\n",
    "\n",
    "    # Read data\n",
    "    try:\n",
    "        df = pd.read_excel(XLSX_PATH)\n",
    "    except Exception:\n",
    "        df = pd.read_excel(XLSX_PATH, engine=\"openpyxl\")\n",
    "\n",
    "    for col in [\"Sender\", \"Subject\", \"Email\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Required column '{col}' not found. Got: {list(df.columns)}\")\n",
    "\n",
    "    print(\"Embedding Sender…\")\n",
    "    sender_emb  = embed_series(model, tok, df[\"Sender\"],  device, MAXLEN_SENDER,  batch_size, desc=\"Sender\")\n",
    "    print(\"Embedding Subject…\")\n",
    "    subject_emb = embed_series(model, tok, df[\"Subject\"], device, MAXLEN_SUBJECT, batch_size, desc=\"Subject\")\n",
    "    print(\"Embedding Body…\")\n",
    "    body_emb    = embed_series(model, tok, df[\"Email\"],   device, MAXLEN_BODY,    batch_size, desc=\"Body\")\n",
    "\n",
    "    print(\"PCA reducing…\")\n",
    "    sender_r  = pca_reduce(sender_emb,  TARGET_DIM)\n",
    "    subject_r = pca_reduce(subject_emb, TARGET_DIM)\n",
    "    body_r    = pca_reduce(body_emb,    TARGET_DIM)\n",
    "\n",
    "    print(\"Computing cosine similarity & saving…\")\n",
    "    save_csv(cosine_similarity(sender_r),  OUT_SENDER)\n",
    "    save_csv(cosine_similarity(subject_r), OUT_SUBJECT)\n",
    "    save_csv(cosine_similarity(body_r),    OUT_BODY)\n",
    "\n",
    "    gc.collect()\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28970a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected adapter in: /home/users/skuikel/Downloads/Tune/FineTune/dpo_Qwen\n",
      "Loading base from:   Qwen/Qwen3-8B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604daffc08bf49fda8de54c077f4650a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base + adapter.\n",
      "Embedding Sender…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sender:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Subject…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Body…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Body:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA reducing…\n",
      "Computing cosine similarity & saving…\n",
      "Saved: sender_similarity_matrix_qwen_ft_pca_dpo.csv\n",
      "Saved: subject_similarity_matrix_qwen_ft_pca_dpo.csv\n",
      "Saved: body_similarity_matrix_qwen_ft_pca_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "MODEL_DIR        = os.path.expanduser(\"~/Downloads/Tune/FineTune/dpo_Qwen\")  # full model OR adapter folder\n",
    "BASE_MODEL_ID    = os.environ.get(\"BASE_MODEL_ID\", \"Qwen/Qwen3-8B\")     # used only if MODEL_DIR is an adapter\n",
    "BASE_MODEL_LOCAL = os.environ.get(\"BASE_MODEL_LOCAL\")                               # local base path (preferred if you have it)\n",
    "XLSX_PATH        = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "\n",
    "BATCH_SIZE_GPU   = 32\n",
    "BATCH_SIZE_CPU   = 4\n",
    "\n",
    "MAXLEN_SENDER    = 48\n",
    "MAXLEN_SUBJECT   = 64\n",
    "MAXLEN_BODY      = 384\n",
    "TARGET_DIM       = 200\n",
    "\n",
    "OUT_SENDER       = \"sender_similarity_matrix_qwen_ft_pca_dpo.csv\"\n",
    "OUT_SUBJECT      = \"subject_similarity_matrix_qwen_ft_pca_dpo.csv\"\n",
    "OUT_BODY         = \"body_similarity_matrix_qwen_ft_pca_dpo.csv\"\n",
    "\n",
    "def setup_device():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "def _is_adapter(path: str) -> bool:\n",
    "    return os.path.isfile(os.path.join(path, \"adapter_config.json\"))\n",
    "\n",
    "def _load_tokenizer(src: str):\n",
    "    tok = AutoTokenizer.from_pretrained(src, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok\n",
    "\n",
    "def load_model_and_tokenizer(device):\n",
    "   \n",
    "    if _is_adapter(MODEL_DIR):\n",
    "        base_src = BASE_MODEL_LOCAL if BASE_MODEL_LOCAL else BASE_MODEL_ID\n",
    "        print(f\"Detected adapter in: {MODEL_DIR}\")\n",
    "        print(f\"Loading base from:   {base_src}\")\n",
    "        tok = _load_tokenizer(base_src)\n",
    "        base = AutoModelForCausalLM.from_pretrained(\n",
    "            base_src,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        ).to(device).eval()\n",
    "\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(base, MODEL_DIR).eval()\n",
    "        print(\"Loaded base + adapter.\")\n",
    "        return model, tok\n",
    "    else:\n",
    "        print(f\"Loading full model from: {MODEL_DIR}\")\n",
    "        tok = _load_tokenizer(MODEL_DIR)\n",
    "        model = AutoModel.from_pretrained(\n",
    "            MODEL_DIR,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        ).to(device).eval()\n",
    "        print(\"Loaded full model.\")\n",
    "        return model, tok\n",
    "\n",
    "@torch.inference_mode()\n",
    "def embed_batch(model, tok, texts, device, max_length):\n",
    "    enc = tok(\n",
    "        [\"\" if t is None else str(t) for t in texts],\n",
    "        padding=True,                 # pad to longest in batch\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc = {k: v.to(device, non_blocking=True) for k, v in enc.items()}\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            out = model(**enc, output_hidden_states=True, use_cache=False)\n",
    "    else:\n",
    "        out = model(**enc, output_hidden_states=True, use_cache=False)\n",
    "\n",
    "    # Robustly get the final hidden states:\n",
    "    if hasattr(out, \"last_hidden_state\") and out.last_hidden_state is not None:\n",
    "        last = out.last_hidden_state\n",
    "    else:\n",
    "        # CausalLM returns CausalLMOutputWithPast (no last_hidden_state). Use hidden_states[-1].\n",
    "        if getattr(out, \"hidden_states\", None) is None:\n",
    "            raise RuntimeError(\"Model did not return hidden states. Ensure output_hidden_states=True.\")\n",
    "        last = out.hidden_states[-1]  # [B, T, H]\n",
    "\n",
    "    mask = enc[\"attention_mask\"].unsqueeze(-1)    # [B, T, 1]\n",
    "    pooled = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "    return pooled.detach().cpu().numpy()          # [B, H]\n",
    "\n",
    "def embed_series(model, tok, series: pd.Series, device, max_length, batch_size, desc=\"Embedding\"):\n",
    "    vals = series.astype(str).fillna(\"\").tolist()\n",
    "    # De-dup for speed\n",
    "    uniq = list(dict.fromkeys(vals))\n",
    "    cache = {}\n",
    "\n",
    "    for i in tqdm(range(0, len(uniq), batch_size), desc=desc, leave=False):\n",
    "        batch = uniq[i:i+batch_size]\n",
    "        vecs  = embed_batch(model, tok, batch, device, max_length)\n",
    "        for t, v in zip(batch, vecs):\n",
    "            cache[t] = v\n",
    "\n",
    "    \n",
    "    return np.stack([cache[v] for v in vals], axis=0)\n",
    "\n",
    "\n",
    "def pca_reduce(X: np.ndarray, target_dim: int) -> np.ndarray:\n",
    "    n_samples, n_features = X.shape\n",
    "    n_components = min(target_dim, n_samples, n_features)\n",
    "    if n_components < target_dim:\n",
    "        print(f\"PCA components clipped to {n_components} (samples={n_samples}, features={n_features}).\")\n",
    "    return PCA(n_components=n_components, random_state=42).fit_transform(X)\n",
    "\n",
    "def save_csv(matrix: np.ndarray, path: str):\n",
    "    pd.DataFrame(matrix).to_csv(path, index=False)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "def main():\n",
    "    device = setup_device()\n",
    "\n",
    "    # free mem\n",
    "    if device.type == \"cuda\":\n",
    "        batch_size = BATCH_SIZE_GPU\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        batch_size = BATCH_SIZE_CPU\n",
    "    gc.collect()\n",
    "\n",
    "    model, tok = load_model_and_tokenizer(device)\n",
    "\n",
    "    # Read data\n",
    "    try:\n",
    "        df = pd.read_excel(XLSX_PATH)\n",
    "    except Exception:\n",
    "        df = pd.read_excel(XLSX_PATH, engine=\"openpyxl\")\n",
    "\n",
    "    for col in [\"Sender\", \"Subject\", \"Email\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Required column '{col}' not found. Got: {list(df.columns)}\")\n",
    "\n",
    "    print(\"Embedding Sender…\")\n",
    "    sender_emb  = embed_series(model, tok, df[\"Sender\"],  device, MAXLEN_SENDER,  batch_size, desc=\"Sender\")\n",
    "    print(\"Embedding Subject…\")\n",
    "    subject_emb = embed_series(model, tok, df[\"Subject\"], device, MAXLEN_SUBJECT, batch_size, desc=\"Subject\")\n",
    "    print(\"Embedding Body…\")\n",
    "    body_emb    = embed_series(model, tok, df[\"Email\"],   device, MAXLEN_BODY,    batch_size, desc=\"Body\")\n",
    "\n",
    "    print(\"PCA reducing…\")\n",
    "    sender_r  = pca_reduce(sender_emb,  TARGET_DIM)\n",
    "    subject_r = pca_reduce(subject_emb, TARGET_DIM)\n",
    "    body_r    = pca_reduce(body_emb,    TARGET_DIM)\n",
    "\n",
    "    print(\"Computing cosine similarity & saving…\")\n",
    "    save_csv(cosine_similarity(sender_r),  OUT_SENDER)\n",
    "    save_csv(cosine_similarity(subject_r), OUT_SUBJECT)\n",
    "    save_csv(cosine_similarity(body_r),    OUT_BODY)\n",
    "\n",
    "    gc.collect()\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf31017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA RTX A5000\n",
      "Total CUDA devices: 3\n",
      "Loading full model from: /home/users/skuikel/Downloads/Tune/FineTune/dpo_bert_uncased\n",
      "Loaded full model.\n",
      "Embedding Sender…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sender:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Subject…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2681785/2658747337.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2681785/2658747337.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Body…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Body:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2681785/2658747337.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA reducing…\n",
      "Computing cosine similarity & saving…\n",
      "Saved: sender_similarity_matrix_bert_ft_pca_dpo.csv\n",
      "Saved: subject_similarity_matrix_bert_ft_pca_dpo.csv\n",
      "Saved: body_similarity_matrix_bert_ft_pca_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "MODEL_DIR        = os.path.expanduser(\"~/Downloads/Tune/FineTune/dpo_bert_uncased\")  # full model OR adapter folder\n",
    "BASE_MODEL_ID    = os.environ.get(\"BASE_MODEL_ID\", \"bert-base-uncased\")     # used only if MODEL_DIR is an adapter\n",
    "BASE_MODEL_LOCAL = os.environ.get(\"BASE_MODEL_LOCAL\")                               # local base path (preferred if you have it)\n",
    "XLSX_PATH        = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "\n",
    "BATCH_SIZE_GPU   = 32\n",
    "BATCH_SIZE_CPU   = 4\n",
    "\n",
    "MAXLEN_SENDER    = 48\n",
    "MAXLEN_SUBJECT   = 64\n",
    "MAXLEN_BODY      = 384\n",
    "TARGET_DIM       = 200\n",
    "\n",
    "OUT_SENDER       = \"sender_similarity_matrix_bert_ft_pca_dpo.csv\"\n",
    "OUT_SUBJECT      = \"subject_similarity_matrix_bert_ft_pca_dpo.csv\"\n",
    "OUT_BODY         = \"body_similarity_matrix_bert_ft_pca_dpo.csv\"\n",
    "\n",
    "def setup_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(f\"Using device: {torch.cuda.get_device_name(None)}\")\n",
    "        print(f\"Total CUDA devices: {torch.cuda.device_count()}\")\n",
    "        return device\n",
    "    print(\"No CUDA device available. Falling back to CPU.\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def _is_adapter(path: str) -> bool:\n",
    "    return os.path.isfile(os.path.join(path, \"adapter_config.json\"))\n",
    "\n",
    "def _load_tokenizer(src: str):\n",
    "    tok = AutoTokenizer.from_pretrained(src, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok\n",
    "\n",
    "def load_model_and_tokenizer(device):\n",
    "   \n",
    "    if _is_adapter(MODEL_DIR):\n",
    "        base_src = BASE_MODEL_LOCAL if BASE_MODEL_LOCAL else BASE_MODEL_ID\n",
    "        print(f\"Detected adapter in: {MODEL_DIR}\")\n",
    "        print(f\"Loading base from:   {base_src}\")\n",
    "        tok = _load_tokenizer(base_src)\n",
    "        base = AutoModelForCausalLM.from_pretrained(\n",
    "            base_src,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        ).to(device).eval()\n",
    "\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(base, MODEL_DIR).eval()\n",
    "        print(\"Loaded base + adapter.\")\n",
    "        return model, tok\n",
    "    else:\n",
    "        print(f\"Loading full model from: {MODEL_DIR}\")\n",
    "        tok = _load_tokenizer(MODEL_DIR)\n",
    "        model = AutoModel.from_pretrained(\n",
    "            MODEL_DIR,\n",
    "            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        ).to(device).eval()\n",
    "        print(\"Loaded full model.\")\n",
    "        return model, tok\n",
    "\n",
    "@torch.inference_mode()\n",
    "def embed_batch(model, tok, texts, device, max_length):\n",
    "    enc = tok(\n",
    "        [\"\" if t is None else str(t) for t in texts],\n",
    "        padding=True,                 # pad to longest in batch\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc = {k: v.to(device, non_blocking=True) for k, v in enc.items()}\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            out = model(**enc, output_hidden_states=True, use_cache=False)\n",
    "    else:\n",
    "        out = model(**enc, output_hidden_states=True, use_cache=False)\n",
    "\n",
    "    # Robustly get the final hidden states:\n",
    "    if hasattr(out, \"last_hidden_state\") and out.last_hidden_state is not None:\n",
    "        last = out.last_hidden_state\n",
    "    else:\n",
    "        # CausalLM returns CausalLMOutputWithPast (no last_hidden_state). Use hidden_states[-1].\n",
    "        if getattr(out, \"hidden_states\", None) is None:\n",
    "            raise RuntimeError(\"Model did not return hidden states. Ensure output_hidden_states=True.\")\n",
    "        last = out.hidden_states[-1]  # [B, T, H]\n",
    "\n",
    "    mask = enc[\"attention_mask\"].unsqueeze(-1)    # [B, T, 1]\n",
    "    pooled = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "    return pooled.detach().cpu().numpy()          # [B, H]\n",
    "\n",
    "def embed_series(model, tok, series: pd.Series, device, max_length, batch_size, desc=\"Embedding\"):\n",
    "    vals = series.astype(str).fillna(\"\").tolist()\n",
    "    # De-dup for speed\n",
    "    uniq = list(dict.fromkeys(vals))\n",
    "    cache = {}\n",
    "\n",
    "    for i in tqdm(range(0, len(uniq), batch_size), desc=desc, leave=False):\n",
    "        batch = uniq[i:i+batch_size]\n",
    "        vecs  = embed_batch(model, tok, batch, device, max_length)\n",
    "        for t, v in zip(batch, vecs):\n",
    "            cache[t] = v\n",
    "\n",
    "    \n",
    "    return np.stack([cache[v] for v in vals], axis=0)\n",
    "\n",
    "\n",
    "def pca_reduce(X: np.ndarray, target_dim: int) -> np.ndarray:\n",
    "    n_samples, n_features = X.shape\n",
    "    n_components = min(target_dim, n_samples, n_features)\n",
    "    if n_components < target_dim:\n",
    "        print(f\"PCA components clipped to {n_components} (samples={n_samples}, features={n_features}).\")\n",
    "    return PCA(n_components=n_components, random_state=42).fit_transform(X)\n",
    "\n",
    "def save_csv(matrix: np.ndarray, path: str):\n",
    "    pd.DataFrame(matrix).to_csv(path, index=False)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "def main():\n",
    "    device = setup_device()\n",
    "\n",
    "    # free mem\n",
    "    if device.type == \"cuda\":\n",
    "        batch_size = BATCH_SIZE_GPU\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        batch_size = BATCH_SIZE_CPU\n",
    "    gc.collect()\n",
    "\n",
    "    model, tok = load_model_and_tokenizer(device)\n",
    "\n",
    "    # Read data\n",
    "    try:\n",
    "        df = pd.read_excel(XLSX_PATH)\n",
    "    except Exception:\n",
    "        df = pd.read_excel(XLSX_PATH, engine=\"openpyxl\")\n",
    "\n",
    "    for col in [\"Sender\", \"Subject\", \"Email\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Required column '{col}' not found. Got: {list(df.columns)}\")\n",
    "\n",
    "    print(\"Embedding Sender…\")\n",
    "    sender_emb  = embed_series(model, tok, df[\"Sender\"],  device, MAXLEN_SENDER,  batch_size, desc=\"Sender\")\n",
    "    print(\"Embedding Subject…\")\n",
    "    subject_emb = embed_series(model, tok, df[\"Subject\"], device, MAXLEN_SUBJECT, batch_size, desc=\"Subject\")\n",
    "    print(\"Embedding Body…\")\n",
    "    body_emb    = embed_series(model, tok, df[\"Email\"],   device, MAXLEN_BODY,    batch_size, desc=\"Body\")\n",
    "\n",
    "    print(\"PCA reducing…\")\n",
    "    sender_r  = pca_reduce(sender_emb,  TARGET_DIM)\n",
    "    subject_r = pca_reduce(subject_emb, TARGET_DIM)\n",
    "    body_r    = pca_reduce(body_emb,    TARGET_DIM)\n",
    "\n",
    "    print(\"Computing cosine similarity & saving…\")\n",
    "    save_csv(cosine_similarity(sender_r),  OUT_SENDER)\n",
    "    save_csv(cosine_similarity(subject_r), OUT_SUBJECT)\n",
    "    save_csv(cosine_similarity(body_r),    OUT_BODY)\n",
    "\n",
    "    gc.collect()\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18461422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496259c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
