{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4b45fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/users/skuikel/Downloads/Tune/FineTune/bert_dpo123_classification_model and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'token_type_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~/Downloads/Tune/FineTune/Original_data.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m load_fine_tuned_model(model_dir, device)\n\u001b[0;32m---> 62\u001b[0m sender_embeddings, subject_embeddings, body_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mextract_embeddings_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m sender_similarity_matrix \u001b[38;5;241m=\u001b[39m calculate_similarity_matrix(sender_embeddings)\n\u001b[1;32m     65\u001b[0m subject_similarity_matrix \u001b[38;5;241m=\u001b[39m calculate_similarity_matrix(subject_embeddings)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mextract_embeddings_from_csv\u001b[0;34m(model, tokenizer, csv_path, device, max_length)\u001b[0m\n\u001b[1;32m     34\u001b[0m subject \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubject\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     35\u001b[0m body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmail\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 38\u001b[0m sender_embeddings\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     39\u001b[0m subject_embeddings\u001b[38;5;241m.\u001b[39mappend(get_embeddings(model, tokenizer, subject, device, max_length))\n\u001b[1;32m     40\u001b[0m body_embeddings\u001b[38;5;241m.\u001b[39mappend(get_embeddings(model, tokenizer, body, device, max_length))\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(model, tokenizer, text, device, max_length)\u001b[0m\n\u001b[1;32m     20\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \n\u001b[1;32m     24\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'token_type_ids'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import gc\n",
    "\n",
    "\n",
    "def load_fine_tuned_model(model_dir, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = LlamaForSequenceClassification.from_pretrained(model_dir, output_hidden_states=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_embeddings(model, tokenizer, text, device, max_length=512):\n",
    "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  \n",
    "        embedding = hidden_states.mean(dim=1).squeeze().cpu().numpy()  \n",
    "    return embedding\n",
    "\n",
    "def extract_embeddings_from_csv(model, tokenizer, csv_path, device, max_length=512):\n",
    "    data = pd.read_excel(csv_path)\n",
    "    sender_embeddings, subject_embeddings, body_embeddings = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, row in data.iterrows():\n",
    "            sender = str(row['Sender'])\n",
    "            subject = str(row['Subject'])\n",
    "            body = str(row['Email'])\n",
    "            \n",
    "           \n",
    "            sender_embeddings.append(get_embeddings(model, tokenizer, sender, device, max_length))\n",
    "            subject_embeddings.append(get_embeddings(model, tokenizer, subject, device, max_length))\n",
    "            body_embeddings.append(get_embeddings(model, tokenizer, body, device, max_length))\n",
    "    \n",
    "    return sender_embeddings, subject_embeddings, body_embeddings\n",
    "\n",
    "\n",
    "def calculate_similarity_matrix(embeddings):\n",
    "    return cosine_similarity(embeddings)\n",
    "\n",
    "\n",
    "def save_matrix_to_csv(matrix, filename):\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved similarity matrix to {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = setup_environment()\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/bert_dpo123_classification_model\")\n",
    "    csv_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "\n",
    "    model, tokenizer = load_fine_tuned_model(model_dir, device)\n",
    "\n",
    "    sender_embeddings, subject_embeddings, body_embeddings = extract_embeddings_from_csv(model, tokenizer, csv_path, device)\n",
    "\n",
    "    sender_similarity_matrix = calculate_similarity_matrix(sender_embeddings)\n",
    "    subject_similarity_matrix = calculate_similarity_matrix(subject_embeddings)\n",
    "    body_similarity_matrix = calculate_similarity_matrix(body_embeddings)\n",
    "    \n",
    "\n",
    "    save_matrix_to_csv(sender_similarity_matrix, \"sender_similarity_matrix_bert_dpo1.csv\")\n",
    "    save_matrix_to_csv(subject_similarity_matrix, \"subject_similarity_matrix_bert_dpo1.csv\")\n",
    "    save_matrix_to_csv(body_similarity_matrix, \"body_similarity_matrix_bert_dpo1.csv\")\n",
    "\n",
    "    # Clean up memory using cache \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dff043b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cda352b7e6742f9ab5cbddd0459d2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/users/skuikel/.local/lib/python3.8/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/skuikel/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "Saved similarity matrix to sender_similarity_matrix_llama_7b_dpo.csv\n",
      "Saved similarity matrix to subject_similarity_matrix_llama_7b_dpo.csv\n",
      "Saved similarity matrix to body_similarity_matrix_llama_7b_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer, AutoConfig, AutoTokenizer\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def setup_environment():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "def load_fine_tuned_model(model_dir, device):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
    "    model = LlamaForSequenceClassification.from_pretrained(model_dir, output_hidden_states=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokenizer, text, device, max_length=512):\n",
    "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.hidden_states[-1] \n",
    "        embedding = hidden_states.mean(dim=1).squeeze().cpu().numpy()  \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def extract_embeddings_from_csv(model, tokenizer, csv_path, device, max_length=512):\n",
    "    data = pd.read_excel(csv_path)\n",
    "    sender_embeddings, subject_embeddings, body_embeddings = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, row in data.iterrows():\n",
    "            sender = str(row['Sender'])\n",
    "            subject = str(row['Subject'])\n",
    "            body = str(row['Email'])\n",
    "            \n",
    "            \n",
    "            sender_embeddings.append(get_embeddings(model, tokenizer, sender, device, max_length))\n",
    "            subject_embeddings.append(get_embeddings(model, tokenizer, subject, device, max_length))\n",
    "            body_embeddings.append(get_embeddings(model, tokenizer, body, device, max_length))\n",
    "    \n",
    "    return sender_embeddings, subject_embeddings, body_embeddings\n",
    "\n",
    "\n",
    "def calculate_similarity_matrix(embeddings):\n",
    "    return cosine_similarity(embeddings)\n",
    "\n",
    "\n",
    "def save_matrix_to_csv(matrix, filename):\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved similarity matrix to {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = setup_environment()\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/llama_7b_dpo123_classification_model\")\n",
    "    csv_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    \n",
    " \n",
    "    model, tokenizer = load_fine_tuned_model(model_dir, device)\n",
    "    \n",
    "   \n",
    "    sender_embeddings, subject_embeddings, body_embeddings = extract_embeddings_from_csv(model, tokenizer, csv_path, device)\n",
    "    \n",
    "  \n",
    "    sender_similarity_matrix = calculate_similarity_matrix(sender_embeddings)\n",
    "    subject_similarity_matrix = calculate_similarity_matrix(subject_embeddings)\n",
    "    body_similarity_matrix = calculate_similarity_matrix(body_embeddings)\n",
    "    \n",
    "   \n",
    "    save_matrix_to_csv(sender_similarity_matrix, \"sender_similarity_matrix_llama_7b_dpo.csv\")\n",
    "    save_matrix_to_csv(subject_similarity_matrix, \"subject_similarity_matrix_llama_7b_dpo.csv\")\n",
    "    save_matrix_to_csv(body_similarity_matrix, \"body_similarity_matrix_llama_7b_dpo.csv\")\n",
    "\n",
    "   \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f42408a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab9058696344589ae451a8f10689de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved similarity matrix to sender_similarity_matrix_llama_8b_dpo.csv\n",
      "Saved similarity matrix to subject_similarity_matrix_llama_8b_dpo.csv\n",
      "Saved similarity matrix to body_similarity_matrix_llama_8b_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer, AutoConfig, AutoTokenizer\n",
    "import gc\n",
    "\n",
    "\n",
    "from transformers import LlamaForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def load_fine_tuned_model(model_dir, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = LlamaForSequenceClassification.from_pretrained(model_dir, output_hidden_states=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokenizer, text, device, max_length=512):\n",
    "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.hidden_states[-1] \n",
    "        embedding = hidden_states.mean(dim=1).squeeze().cpu().numpy()  \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def extract_embeddings_from_csv(model, tokenizer, csv_path, device, max_length=512):\n",
    "    data = pd.read_excel(csv_path)\n",
    "    sender_embeddings, subject_embeddings, body_embeddings = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, row in data.iterrows():\n",
    "            sender = str(row['Sender'])\n",
    "            subject = str(row['Subject'])\n",
    "            body = str(row['Email'])\n",
    "            \n",
    "            \n",
    "            sender_embeddings.append(get_embeddings(model, tokenizer, sender, device, max_length))\n",
    "            subject_embeddings.append(get_embeddings(model, tokenizer, subject, device, max_length))\n",
    "            body_embeddings.append(get_embeddings(model, tokenizer, body, device, max_length))\n",
    "    \n",
    "    return sender_embeddings, subject_embeddings, body_embeddings\n",
    "\n",
    "\n",
    "def calculate_similarity_matrix(embeddings):\n",
    "    return cosine_similarity(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def save_matrix_to_csv(matrix, filename):\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved similarity matrix to {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = setup_environment()\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/llama_8b_dpo123_classification_model\")\n",
    "    csv_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    \n",
    " \n",
    "    model, tokenizer = load_fine_tuned_model(model_dir, device)\n",
    "    \n",
    "   \n",
    "    sender_embeddings, subject_embeddings, body_embeddings = extract_embeddings_from_csv(model, tokenizer, csv_path, device)\n",
    "    \n",
    "  \n",
    "    sender_similarity_matrix = calculate_similarity_matrix(sender_embeddings)\n",
    "    subject_similarity_matrix = calculate_similarity_matrix(subject_embeddings)\n",
    "    body_similarity_matrix = calculate_similarity_matrix(body_embeddings)\n",
    "    \n",
    "   \n",
    "    save_matrix_to_csv(sender_similarity_matrix, \"sender_similarity_matrix_llama_8b_dpo.csv\")\n",
    "    save_matrix_to_csv(subject_similarity_matrix, \"subject_similarity_matrix_llama_8b_dpo.csv\")\n",
    "    save_matrix_to_csv(body_similarity_matrix, \"body_similarity_matrix_llama_8b_dpo.csv\")\n",
    "\n",
    " #  \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb091d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294c9c0f04d949c6bbd91433e9f624f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at dreamgen/WizardLM-2-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved similarity matrix to sender_similarity_matrix_wizard_dpo.csv\n",
      "Saved similarity matrix to subject_similarity_matrix_wizard_dpo.csv\n",
      "Saved similarity matrix to body_similarity_matrix_wizard_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import LlamaForSequenceClassification, LlamaTokenizer, AutoConfig, AutoTokenizer\n",
    "import gc\n",
    "\n",
    "\n",
    "from transformers import LlamaForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def load_fine_tuned_model(model_dir, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = LlamaForSequenceClassification.from_pretrained(model_dir, output_hidden_states=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokenizer, text, device, max_length=512):\n",
    "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.hidden_states[-1] \n",
    "        embedding = hidden_states.mean(dim=1).squeeze().cpu().numpy()  \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def extract_embeddings_from_csv(model, tokenizer, csv_path, device, max_length=512):\n",
    "    data = pd.read_excel(csv_path)\n",
    "    sender_embeddings, subject_embeddings, body_embeddings = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, row in data.iterrows():\n",
    "            sender = str(row['Sender'])\n",
    "            subject = str(row['Subject'])\n",
    "            body = str(row['Email'])\n",
    "            \n",
    "            \n",
    "            sender_embeddings.append(get_embeddings(model, tokenizer, sender, device, max_length))\n",
    "            subject_embeddings.append(get_embeddings(model, tokenizer, subject, device, max_length))\n",
    "            body_embeddings.append(get_embeddings(model, tokenizer, body, device, max_length))\n",
    "    \n",
    "    return sender_embeddings, subject_embeddings, body_embeddings\n",
    "\n",
    "\n",
    "def calculate_similarity_matrix(embeddings):\n",
    "    return cosine_similarity(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def save_matrix_to_csv(matrix, filename):\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved similarity matrix to {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = setup_environment()\n",
    "    model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/wizard_7b_dpo_classification_model\")\n",
    "    csv_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "    \n",
    " \n",
    "    model, tokenizer = load_fine_tuned_model(model_dir, device)\n",
    "    \n",
    "   \n",
    "    sender_embeddings, subject_embeddings, body_embeddings = extract_embeddings_from_csv(model, tokenizer, csv_path, device)\n",
    "    \n",
    "  \n",
    "    sender_similarity_matrix = calculate_similarity_matrix(sender_embeddings)\n",
    "    subject_similarity_matrix = calculate_similarity_matrix(subject_embeddings)\n",
    "    body_similarity_matrix = calculate_similarity_matrix(body_embeddings)\n",
    "    \n",
    "   \n",
    "    save_matrix_to_csv(sender_similarity_matrix, \"sender_similarity_matrix_wizard_dpo.csv\")\n",
    "    save_matrix_to_csv(subject_similarity_matrix, \"subject_similarity_matrix_wizard_dpo.csv\")\n",
    "    save_matrix_to_csv(body_similarity_matrix, \"body_similarity_matrix_wizard_dpo.csv\")\n",
    "\n",
    " #  \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab4c29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "Loading model and tokenizer...\n",
      "Extracting embeddings...\n",
      "Processing 241 emails...\n",
      "Processing email 0/241\n",
      "Processing email 10/241\n",
      "Processing email 20/241\n",
      "Processing email 30/241\n",
      "Processing email 40/241\n",
      "Processing email 50/241\n",
      "Processing email 60/241\n",
      "Processing email 70/241\n",
      "Processing email 80/241\n",
      "Processing email 90/241\n",
      "Processing email 100/241\n",
      "Processing email 110/241\n",
      "Processing email 120/241\n",
      "Processing email 130/241\n",
      "Processing email 140/241\n",
      "Processing email 150/241\n",
      "Processing email 160/241\n",
      "Processing email 170/241\n",
      "Processing email 180/241\n",
      "Processing email 190/241\n",
      "Processing email 200/241\n",
      "Processing email 210/241\n",
      "Processing email 220/241\n",
      "Processing email 230/241\n",
      "Processing email 240/241\n",
      "Calculating similarity matrices...\n",
      "Saving matrices...\n",
      "Saved similarity matrix to sender_similarity_matrix_bert_dpo1.csv\n",
      "Saved similarity matrix to subject_similarity_matrix_bert_dpo1.csv\n",
      "Saved similarity matrix to body_similarity_matrix_bert_dpo1.csv\n",
      "Processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import gc\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Set up the GPU environment and return the appropriate device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "def load_fine_tuned_model(model_dir, device):\n",
    "    \"\"\"Load the fine-tuned BERT model and tokenizer.\"\"\"\n",
    "    # Use AutoModelForSequenceClassification instead of LlamaForSequenceClassification\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_dir, \n",
    "        output_hidden_states=True,\n",
    "        num_labels=2\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_embeddings(model, tokenizer, text, device, max_length=512):\n",
    "    \"\"\"Get embeddings for a given text using the model.\"\"\"\n",
    "    # Remove token_type_ids from inputs if using BERT\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Remove token_type_ids if present\n",
    "    if 'token_type_ids' in inputs:\n",
    "        del inputs['token_type_ids']\n",
    "    \n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        # Get the last hidden state\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        # Mean pooling with attention mask\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        mask = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        sum_embeddings = torch.sum(hidden_states * mask, dim=1)\n",
    "        sum_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "        embedding = (sum_embeddings / sum_mask).squeeze().cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "def extract_embeddings_from_csv(model, tokenizer, csv_path, device, max_length=512):\n",
    "    \"\"\"Extract embeddings from CSV file.\"\"\"\n",
    "    try:\n",
    "        data = pd.read_excel(csv_path)\n",
    "        sender_embeddings, subject_embeddings, body_embeddings = [], [], []\n",
    "        \n",
    "        total_rows = len(data)\n",
    "        print(f\"Processing {total_rows} emails...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx, row in data.iterrows():\n",
    "                if idx % 10 == 0:\n",
    "                    print(f\"Processing email {idx}/{total_rows}\")\n",
    "                \n",
    "                try:\n",
    "                    sender = str(row['Sender'])\n",
    "                    subject = str(row['Subject'])\n",
    "                    body = str(row['Email'])\n",
    "                    \n",
    "                    sender_embeddings.append(get_embeddings(model, tokenizer, sender, device, max_length))\n",
    "                    subject_embeddings.append(get_embeddings(model, tokenizer, subject, device, max_length))\n",
    "                    body_embeddings.append(get_embeddings(model, tokenizer, body, device, max_length))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row {idx}: {str(e)}\")\n",
    "                    # Use zero embeddings for failed rows\n",
    "                    zero_emb = np.zeros(model.config.hidden_size)\n",
    "                    sender_embeddings.append(zero_emb)\n",
    "                    subject_embeddings.append(zero_emb)\n",
    "                    body_embeddings.append(zero_emb)\n",
    "        \n",
    "        return sender_embeddings, subject_embeddings, body_embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def calculate_similarity_matrix(embeddings):\n",
    "    \"\"\"Calculate cosine similarity matrix.\"\"\"\n",
    "    return cosine_similarity(embeddings)\n",
    "\n",
    "def save_matrix_to_csv(matrix, filename):\n",
    "    \"\"\"Save similarity matrix to CSV.\"\"\"\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved similarity matrix to {filename}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        device = setup_environment()\n",
    "        model_dir = os.path.expanduser(\"~/Downloads/Tune/FineTune/bert_dpo123_classification_model\")\n",
    "        csv_path = os.path.expanduser(\"~/Downloads/Tune/FineTune/Original_data.xlsx\")\n",
    "\n",
    "        print(\"Loading model and tokenizer...\")\n",
    "        model, tokenizer = load_fine_tuned_model(model_dir, device)\n",
    "\n",
    "        print(\"Extracting embeddings...\")\n",
    "        sender_embeddings, subject_embeddings, body_embeddings = extract_embeddings_from_csv(\n",
    "            model, tokenizer, csv_path, device\n",
    "        )\n",
    "\n",
    "        print(\"Calculating similarity matrices...\")\n",
    "        sender_similarity_matrix = calculate_similarity_matrix(sender_embeddings)\n",
    "        subject_similarity_matrix = calculate_similarity_matrix(subject_embeddings)\n",
    "        body_similarity_matrix = calculate_similarity_matrix(body_embeddings)\n",
    "\n",
    "        print(\"Saving matrices...\")\n",
    "        save_matrix_to_csv(sender_similarity_matrix, \"sender_similarity_matrix_bert_dpo1.csv\")\n",
    "        save_matrix_to_csv(subject_similarity_matrix, \"subject_similarity_matrix_bert_dpo1.csv\")\n",
    "        save_matrix_to_csv(body_similarity_matrix, \"body_similarity_matrix_bert_dpo1.csv\")\n",
    "\n",
    "        # Clean up\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"Processing completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aef5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
